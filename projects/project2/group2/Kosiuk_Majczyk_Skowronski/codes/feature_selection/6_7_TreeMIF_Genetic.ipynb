{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RDJkoBbn2Jx2",
        "outputId": "443be7ab-3de2-4909-ed39-1c35f6896e38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sklearn-genetic-opt\n",
            "  Downloading sklearn_genetic_opt-0.10.1-py3-none-any.whl (33 kB)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sklearn-genetic-opt) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from sklearn-genetic-opt) (1.25.2)\n",
            "Collecting deap>=1.3.3 (from sklearn-genetic-opt)\n",
            "  Downloading deap-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.4/135.4 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.61.1 in /usr/local/lib/python3.10/dist-packages (from sklearn-genetic-opt) (4.66.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.1.0->sklearn-genetic-opt) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.1.0->sklearn-genetic-opt) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.1.0->sklearn-genetic-opt) (3.5.0)\n",
            "Installing collected packages: deap, sklearn-genetic-opt\n",
            "Successfully installed deap-1.4.1 sklearn-genetic-opt-0.10.1\n"
          ]
        }
      ],
      "source": [
        "!pip install sklearn-genetic-opt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "iLSIDGZU2DHt"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn_genetic import GAFeatureSelectionCV\n",
        "from sklearn_genetic.space import Categorical\n",
        "from sklearn_genetic.callbacks import ProgressBar\n",
        "from sklearn_genetic.plots import plot_fitness_evolution\n",
        "from sklearn.metrics import make_scorer, accuracy_score\n",
        "\n",
        "x_train = pd.read_csv('x_train.txt', header=None, sep=' ')\n",
        "y_train = pd.read_csv('y_train.txt', header=None, sep=' ')\n",
        "x_test = pd.read_csv('x_test.txt', header=None, sep=' ')\n",
        "x_train.columns = [f\"x{i+1}\" for i in x_train.columns]\n",
        "x_test.columns = [f\"x{i+1}\" for i in x_test.columns]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Ea5fohs_DEKZ"
      },
      "outputs": [],
      "source": [
        "initial_features = [\n",
        "    \"x1\",\n",
        "    \"x10\",\n",
        "    \"x101\",\n",
        "    \"x102\",\n",
        "    \"x103\",\n",
        "    \"x104\",\n",
        "    \"x105\",\n",
        "    \"x106\",\n",
        "    \"x132\",\n",
        "    \"x140\",\n",
        "    \"x149\",\n",
        "    \"x153\",\n",
        "    \"x156\",\n",
        "    \"x176\",\n",
        "    \"x191\",\n",
        "    \"x2\",\n",
        "    \"x22\",\n",
        "    \"x221\",\n",
        "    \"x229\",\n",
        "    \"x253\",\n",
        "    \"x286\",\n",
        "    \"x3\",\n",
        "    \"x304\",\n",
        "    \"x322\",\n",
        "    \"x323\",\n",
        "    \"x324\",\n",
        "    \"x329\",\n",
        "    \"x336\",\n",
        "    \"x35\",\n",
        "    \"x352\",\n",
        "    \"x36\",\n",
        "    \"x4\",\n",
        "    \"x40\",\n",
        "    \"x404\",\n",
        "    \"x413\",\n",
        "    \"x423\",\n",
        "    \"x459\",\n",
        "    \"x463\",\n",
        "    \"x499\",\n",
        "    \"x5\",\n",
        "    \"x58\",\n",
        "    \"x6\",\n",
        "    \"x65\",\n",
        "    \"x7\",\n",
        "    \"x74\",\n",
        "    \"x8\",\n",
        "    \"x81\",\n",
        "    \"x9\",\n",
        "    \"x99\",\n",
        "]\n",
        "\n",
        "x_train = x_train[initial_features]\n",
        "x_test = x_test[initial_features]\n",
        "X_train, X_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "KUznOull6Szs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def calculate_score(estimator, X, y_true, reward=10, punishment=200):\n",
        "    number_of_features = X.shape[1]\n",
        "    number_1s = sum(y_true==1)\n",
        "\n",
        "    y_pred = estimator.predict_proba(X)\n",
        "\n",
        "    probabilities = y_pred[:,1]\n",
        "    top_indices = np.argpartition(probabilities, -number_1s)[-number_1s:]\n",
        "    labels = np.zeros_like(probabilities, dtype=int)\n",
        "    labels[top_indices] = 1\n",
        "    labels = labels.tolist()\n",
        "    y_pred = labels\n",
        "    correct_class_1 = sum((np.array(y_pred)==1) & (np.array(list(y_true))==1))\n",
        "    score = reward * correct_class_1*(1000/number_1s) - punishment * number_of_features\n",
        "    return score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Genetic Algorithm part 1"
      ],
      "metadata": {
        "id": "Moyq4KdonOYh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyHCoGEGWjMO",
        "outputId": "beb1b715-6149-4156-8ff6-4c94f37dc6ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selecting for params: {'population_size': 20, 'mutation_probability': 0.5, 'crossover_probability': 0.2} and scoring: accuracy\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  pid = os.fork()\n",
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gen\tnevals\tfitness \tfitness_std\tfitness_max\tfitness_min\n",
            "0  \t20    \t0.536161\t0.00595634 \t0.546001   \t0.52325    \n",
            "1  \t27    \t0.538849\t0.00303151 \t0.545501   \t0.532998   \n",
            "2  \t33    \t0.5423  \t0.00370771 \t0.54825    \t0.53675    \n",
            "3  \t27    \t0.543462\t0.00328699 \t0.54825    \t0.536997   \n",
            "4  \t29    \t0.546037\t0.00371835 \t0.552253   \t0.538997   \n",
            "5  \t26    \t0.546537\t0.00357078 \t0.548999   \t0.535499   \n",
            "6  \t33    \t0.548212\t0.00306159 \t0.552751   \t0.535748   \n",
            "7  \t28    \t0.548987\t0.00316812 \t0.552751   \t0.542249   \n",
            "8  \t31    \t0.549062\t0.00372068 \t0.552751   \t0.534246   \n",
            "9  \t33    \t0.549225\t0.00316745 \t0.552749   \t0.538253   \n",
            "10 \t30    \t0.549161\t0.00478134 \t0.552749   \t0.534      \n",
            "11 \t30    \t0.551486\t0.00386331 \t0.5595     \t0.540751   \n",
            "12 \t20    \t0.551149\t0.00481751 \t0.5595     \t0.53175    \n",
            "13 \t30    \t0.551074\t0.00327154 \t0.552749   \t0.539247   \n",
            "14 \t30    \t0.549974\t0.00496327 \t0.552749   \t0.539748   \n",
            "15 \t26    \t0.550874\t0.00435124 \t0.553001   \t0.539748   \n",
            "16 \t27    \t0.550763\t0.00448879 \t0.553001   \t0.5365     \n",
            "17 \t24    \t0.552388\t0.00169956 \t0.553001   \t0.546002   \n",
            "18 \t26    \t0.552938\t0.000953737\t0.553747   \t0.549003   \n",
            "19 \t29    \t0.553025\t0.000552374\t0.553998   \t0.550999   \n",
            "20 \t26    \t0.5532  \t0.000357458\t0.553998   \t0.553001   \n",
            "21 \t27    \t0.552237\t0.00471863 \t0.553998   \t0.531749   \n",
            "22 \t29    \t0.551586\t0.00468693 \t0.553998   \t0.537499   \n",
            "23 \t28    \t0.553511\t0.00159836 \t0.554502   \t0.546745   \n",
            "24 \t26    \t0.553612\t0.0017333  \t0.554502   \t0.546248   \n",
            "25 \t32    \t0.554424\t0.00309791 \t0.559499   \t0.544747   \n",
            "26 \t30    \t0.552736\t0.00415629 \t0.559499   \t0.54075    \n",
            "27 \t30    \t0.552924\t0.00396118 \t0.559499   \t0.54475    \n",
            "28 \t27    \t0.553999\t0.00299393 \t0.559499   \t0.546499   \n",
            "29 \t31    \t0.554474\t0.00438246 \t0.559499   \t0.54       \n",
            "30 \t29    \t0.554449\t0.00589451 \t0.559499   \t0.537997   \n",
            "31 \t28    \t0.556449\t0.00625711 \t0.559499   \t0.530498   \n",
            "32 \t29    \t0.556462\t0.00574349 \t0.559499   \t0.540747   \n",
            "33 \t32    \t0.558812\t0.00179858 \t0.559499   \t0.552251   \n",
            "34 \t31    \t0.558124\t0.00468253 \t0.562751   \t0.538747   \n",
            "35 \t30    \t0.55955 \t0.00180581 \t0.562751   \t0.554498   \n",
            "36 \t26    \t0.558837\t0.00459945 \t0.562751   \t0.540251   \n",
            "37 \t31    \t0.55855 \t0.00627071 \t0.562751   \t0.54025    \n",
            "38 \t25    \t0.560925\t0.00445155 \t0.562751   \t0.5425     \n",
            "39 \t28    \t0.561588\t0.00437943 \t0.562751   \t0.542749   \n",
            "40 \t29    \t0.560388\t0.0066419  \t0.562751   \t0.537997   \n",
            "41 \t29    \t0.562751\t1.11022e-16\t0.562751   \t0.562751   \n",
            "42 \t30    \t0.561463\t0.00561222 \t0.562751   \t0.537      \n",
            "43 \t27    \t0.561713\t0.00452214 \t0.562751   \t0.542002   \n",
            "44 \t36    \t0.561526\t0.00367468 \t0.562751   \t0.550502   \n",
            "45 \t28    \t0.559563\t0.0079836  \t0.562751   \t0.536006   \n",
            "46 \t29    \t0.559251\t0.00839732 \t0.562751   \t0.536496   \n",
            "47 \t25    \t0.558813\t0.00942162 \t0.562751   \t0.5335     \n",
            "48 \t25    \t0.562751\t1.11022e-16\t0.562751   \t0.562751   \n",
            "49 \t17    \t0.561538\t0.00528469 \t0.562751   \t0.538503   \n",
            "50 \t31    \t0.560613\t0.00659563 \t0.562751   \t0.536499   \n",
            "51 \t31    \t0.55895 \t0.00794867 \t0.562751   \t0.534747   \n",
            "52 \t32    \t0.5594  \t0.00798266 \t0.562751   \t0.539247   \n",
            "53 \t29    \t0.562751\t1.11022e-16\t0.562751   \t0.562751   \n",
            "54 \t29    \t0.559913\t0.00851275 \t0.562751   \t0.53425    \n",
            "55 \t29    \t0.561338\t0.00615705 \t0.562751   \t0.5345     \n",
            "56 \t27    \t0.562751\t1.11022e-16\t0.562751   \t0.562751   \n",
            "57 \t30    \t0.562751\t1.11022e-16\t0.562751   \t0.562751   \n",
            "58 \t28    \t0.561663\t0.00474124 \t0.562751   \t0.540996   \n",
            "59 \t26    \t0.560101\t0.00802651 \t0.562751   \t0.532752   \n",
            "60 \t27    \t0.561613\t0.0049592  \t0.562751   \t0.539996   \n",
            "61 \t27    \t0.561626\t0.0049038  \t0.562751   \t0.540251   \n",
            "62 \t26    \t0.560288\t0.00741808 \t0.562751   \t0.536      \n",
            "63 \t29    \t0.559488\t0.00806734 \t0.562751   \t0.534501   \n",
            "64 \t26    \t0.556963\t0.00914951 \t0.562751   \t0.53975    \n",
            "65 \t25    \t0.559451\t0.00791495 \t0.562751   \t0.53475    \n",
            "66 \t31    \t0.561676\t0.00503328 \t0.563251   \t0.53975    \n",
            "67 \t26    \t0.557926\t0.0098766  \t0.563251   \t0.534998   \n",
            "68 \t29    \t0.560026\t0.0073509  \t0.563251   \t0.535748   \n",
            "69 \t31    \t0.560101\t0.00895317 \t0.563251   \t0.533002   \n",
            "70 \t27    \t0.563101\t0.000229272\t0.563251   \t0.562751   \n",
            "71 \t31    \t0.559601\t0.00855071 \t0.563251   \t0.538998   \n",
            "72 \t23    \t0.560563\t0.00803202 \t0.563251   \t0.533749   \n",
            "73 \t26    \t0.562438\t0.00354184 \t0.563251   \t0.547      \n",
            "74 \t26    \t0.562026\t0.00533939 \t0.563251   \t0.538752   \n",
            "75 \t28    \t0.562288\t0.00419563 \t0.563251   \t0.544      \n",
            "76 \t28    \t0.563251\t0          \t0.563251   \t0.563251   \n",
            "77 \t23    \t0.563251\t0          \t0.563251   \t0.563251   \n",
            "78 \t27    \t0.558888\t0.0105781  \t0.563251   \t0.530498   \n",
            "79 \t29    \t0.558001\t0.00957027 \t0.563251   \t0.535752   \n",
            "80 \t27    \t0.562251\t0.00435905 \t0.563251   \t0.54325    \n",
            "81 \t27    \t0.561763\t0.00648467 \t0.563251   \t0.533497   \n",
            "82 \t29    \t0.561788\t0.00637542 \t0.563251   \t0.533999   \n",
            "83 \t27    \t0.561713\t0.00610017 \t0.563251   \t0.535252   \n",
            "84 \t26    \t0.561413\t0.00800985 \t0.563251   \t0.526499   \n",
            "85 \t26    \t0.560851\t0.00765884 \t0.563251   \t0.530999   \n",
            "86 \t27    \t0.561951\t0.00566664 \t0.563251   \t0.537251   \n",
            "87 \t30    \t0.562339\t0.00397739 \t0.563251   \t0.545002   \n",
            "88 \t30    \t0.561001\t0.00675113 \t0.563251   \t0.540746   \n",
            "89 \t28    \t0.560726\t0.00762447 \t0.563251   \t0.535254   \n",
            "90 \t26    \t0.558088\t0.0103621  \t0.563251   \t0.535992   \n",
            "91 \t30    \t0.559951\t0.00802997 \t0.563251   \t0.535996   \n",
            "92 \t20    \t0.563251\t0          \t0.563251   \t0.563251   \n",
            "93 \t28    \t0.561226\t0.00608015 \t0.563251   \t0.542248   \n",
            "94 \t25    \t0.563251\t0          \t0.563251   \t0.563251   \n",
            "95 \t26    \t0.560826\t0.00727733 \t0.563251   \t0.538499   \n",
            "96 \t26    \t0.563251\t0          \t0.563251   \t0.563251   \n",
            "97 \t30    \t0.561914\t0.00583001 \t0.563251   \t0.536501   \n",
            "98 \t27    \t0.563251\t0          \t0.563251   \t0.563251   \n",
            "99 \t28    \t0.562051\t0.00523186 \t0.563251   \t0.539246   \n",
            "100\t29    \t0.557713\t0.0111368  \t0.563251   \t0.5315     \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected features: [False False False False False False False  True  True  True  True False\n",
            "  True False  True  True False  True False  True False  True False  True\n",
            " False  True  True  True  True  True False False  True  True  True  True\n",
            "  True False  True  True  True  True  True False False False  True  True\n",
            " False]\n",
            "Accuracy with selected features: 0.554\n",
            "Selecting for params: {'population_size': 20, 'mutation_probability': 0.5, 'crossover_probability': 0.2} and scoring: functools.partial(<function calculate_score at 0x78b445b73e20>, reward=10, punishment=200)\n",
            "gen\tnevals\tfitness\tfitness_std\tfitness_max\tfitness_min\n",
            "0  \t20    \t387.259\t574.395    \t1768.01    \t-492.06    \n",
            "1  \t24    \t905.945\t471.894    \t2087.87    \t122.902    \n",
            "2  \t27    \t1305.92\t431.484    \t2092.92    \t563.047    \n",
            "3  \t32    \t1590.2 \t406.044    \t2092.92    \t563.047    \n",
            "4  \t28    \t1873.74\t265.876    \t2512.93    \t1158       \n",
            "5  \t24    \t2055.18\t369.916    \t2512.93    \t808.025    \n",
            "6  \t24    \t2238.17\t292.008    \t2512.93    \t1558.06    \n",
            "7  \t31    \t2282.14\t413.534    \t2512.93    \t778.107    \n",
            "8  \t26    \t2286.92\t467.164    \t2512.93    \t778.107    \n",
            "9  \t29    \t2454.91\t145.473    \t2512.93    \t2042.75    \n",
            "10 \t25    \t2501.93\t47.9637    \t2512.93    \t2292.86    \n",
            "11 \t29    \t2448.19\t282.209    \t2512.93    \t1218.06    \n",
            "12 \t25    \t2512.93\t4.54747e-13\t2512.93    \t2512.93    \n",
            "13 \t24    \t2451.67\t267.038    \t2512.93    \t1287.67    \n",
            "14 \t26    \t2249.93\t631.032    \t2512.93    \t472.95     \n",
            "15 \t26    \t2512.93\t4.54747e-13\t2512.93    \t2512.93    \n",
            "16 \t29    \t2512.93\t4.54747e-13\t2512.93    \t2512.93    \n",
            "17 \t30    \t2512.93\t4.54747e-13\t2512.93    \t2512.93    \n",
            "18 \t27    \t2512.93\t4.54747e-13\t2512.93    \t2512.93    \n",
            "19 \t32    \t2444.43\t298.571    \t2512.93    \t1142.99    \n",
            "20 \t19    \t2530.43\t76.2633    \t2862.85    \t2512.93    \n",
            "21 \t34    \t2407.42\t391.894    \t2862.85    \t1012.81    \n",
            "22 \t28    \t2292.41\t593.395    \t2862.85    \t477.925    \n",
            "23 \t23    \t2412.16\t433.772    \t2862.85    \t1022.79    \n",
            "24 \t28    \t2469.93\t278.341    \t2862.85    \t1302.96    \n",
            "25 \t29    \t2528.67\t77.0444    \t2862.85    \t2477.86    \n",
            "26 \t28    \t2405.67\t576.425    \t2952.68    \t-72.0329   \n",
            "27 \t27    \t2622.87\t190.417    \t2952.68    \t2512.93    \n",
            "28 \t23    \t2715.78\t439.06     \t3182.79    \t1062.9     \n",
            "29 \t27    \t2830.51\t585.712    \t3392.79    \t467.915    \n",
            "30 \t29    \t2785.26\t630.988    \t3392.79    \t232.857    \n",
            "31 \t24    \t3100.26\t201.92     \t3392.79    \t2512.93    \n",
            "32 \t32    \t3129.52\t167.703    \t3392.79    \t2617.77    \n",
            "33 \t28    \t3162.02\t179.703    \t3392.79    \t2617.77    \n",
            "34 \t31    \t3001.3 \t734.886    \t3392.79    \t702.957    \n",
            "35 \t25    \t3277.29\t104.471    \t3392.79    \t3182.79    \n",
            "36 \t30    \t3329.79\t96.2318    \t3392.79    \t3182.79    \n",
            "37 \t24    \t3392.79\t1.36424e-12\t3392.79    \t3392.79    \n",
            "38 \t32    \t3146.8 \t742.702    \t3392.79    \t667.832    \n",
            "39 \t27    \t3303.78\t387.956    \t3392.79    \t1612.72    \n",
            "40 \t28    \t3392.79\t1.36424e-12\t3392.79    \t3392.79    \n",
            "41 \t29    \t3124.81\t803.958    \t3392.79    \t692.985    \n",
            "42 \t27    \t2870.83\t1046.86    \t3392.79    \t523.007    \n",
            "43 \t23    \t3271.05\t530.657    \t3392.79    \t957.967    \n",
            "44 \t30    \t3147.04\t738.904    \t3392.79    \t778.092    \n",
            "45 \t23    \t3392.79\t1.36424e-12\t3392.79    \t3392.79    \n",
            "46 \t30    \t3392.79\t1.36424e-12\t3392.79    \t3392.79    \n",
            "47 \t31    \t3270.8 \t531.739    \t3392.79    \t953        \n",
            "48 \t33    \t3016.8 \t899.268    \t3392.79    \t568        \n",
            "49 \t32    \t3276.3 \t407.026    \t3392.79    \t1568.01    \n",
            "50 \t30    \t3270.8 \t531.744    \t3392.79    \t952.977    \n",
            "51 \t24    \t3213.55\t548.522    \t3392.79    \t1257.81    \n",
            "52 \t25    \t3286.04\t465.306    \t3392.79    \t1257.81    \n",
            "53 \t30    \t3380.78\t52.3286    \t3392.79    \t3152.69    \n",
            "54 \t26    \t3392.79\t1.36424e-12\t3392.79    \t3392.79    \n",
            "55 \t27    \t3392.79\t1.36424e-12\t3392.79    \t3392.79    \n",
            "56 \t29    \t2933.06\t1094.36    \t3392.79    \t327.975    \n",
            "57 \t30    \t3189.81\t608.939    \t3392.79    \t1353.02    \n",
            "58 \t28    \t3064.32\t866.278    \t3392.79    \t-51.9753   \n",
            "59 \t26    \t3166.56\t683.014    \t3392.79    \t887.935    \n",
            "60 \t31    \t3037.06\t847.724    \t3392.79    \t877.917    \n",
            "61 \t31    \t3134.55\t775.253    \t3392.79    \t717.987    \n",
            "62 \t28    \t3392.79\t1.36424e-12\t3392.79    \t3392.79    \n",
            "63 \t29    \t3392.79\t1.36424e-12\t3392.79    \t3392.79    \n",
            "64 \t23    \t3392.79\t1.36424e-12\t3392.79    \t3392.79    \n",
            "65 \t29    \t3392.79\t1.36424e-12\t3392.79    \t3392.79    \n",
            "66 \t27    \t3262.05\t569.871    \t3392.79    \t778.04     \n",
            "67 \t29    \t3392.79\t1.36424e-12\t3392.79    \t3392.79    \n",
            "68 \t27    \t3305.04\t382.467    \t3392.79    \t1637.91    \n",
            "69 \t27    \t3392.79\t1.36424e-12\t3392.79    \t3392.79    \n",
            "70 \t32    \t3143.05\t760.749    \t3392.79    \t477.902    \n",
            "71 \t29    \t3376.04\t72.9892    \t3392.79    \t3057.89    \n",
            "72 \t25    \t3281.3 \t485.97     \t3392.79    \t1163       \n",
            "73 \t24    \t3392.79\t1.36424e-12\t3392.79    \t3392.79    \n",
            "74 \t28    \t3392.79\t1.36424e-12\t3392.79    \t3392.79    \n",
            "75 \t25    \t3392.79\t1.36424e-12\t3392.79    \t3392.79    \n",
            "76 \t26    \t3228.81\t491.938    \t3392.79    \t1752.99    \n",
            "77 \t30    \t3392.79\t1.36424e-12\t3392.79    \t3392.79    \n",
            "78 \t31    \t3293.79\t431.523    \t3392.79    \t1412.82    \n",
            "79 \t28    \t3392.79\t1.36424e-12\t3392.79    \t3392.79    \n",
            "80 \t28    \t3274.8 \t514.276    \t3392.79    \t1033.12    \n",
            "81 \t28    \t3293.79\t431.528    \t3392.79    \t1412.8     \n",
            "82 \t29    \t3392.79\t1.36424e-12\t3392.79    \t3392.79    \n",
            "83 \t28    \t3261.54\t572.096    \t3392.79    \t767.829    \n",
            "84 \t33    \t3245.3 \t642.867    \t3392.79    \t443.112    \n",
            "85 \t31    \t3287.05\t509.72     \t3462.82    \t1067.88    \n",
            "86 \t34    \t3268.8 \t572.953    \t3462.82    \t773.027    \n",
            "87 \t27    \t3374.05\t322.135    \t3977.79    \t2082.94    \n",
            "88 \t31    \t3412.55\t42.1451    \t3462.82    \t3297.88    \n",
            "89 \t27    \t3420.8 \t34.3087    \t3462.82    \t3392.79    \n",
            "90 \t34    \t2980.58\t1038.16    \t3462.82    \t72.912     \n",
            "91 \t32    \t3216.83\t677.501    \t3592.79    \t1008.11    \n",
            "92 \t32    \t3163.58\t844.391    \t3592.79    \t502.912    \n",
            "93 \t28    \t3491.3 \t70.6522    \t3592.79    \t3392.79    \n",
            "94 \t27    \t3396.8 \t589.323    \t3592.79    \t842.844    \n",
            "95 \t29    \t3429.8 \t534.779    \t3592.79    \t1112.93    \n",
            "96 \t26    \t3586.29\t28.3258    \t3592.79    \t3462.82    \n",
            "97 \t27    \t3467.3 \t547.004    \t3592.79    \t1082.96    \n",
            "98 \t30    \t3592.79\t1.36424e-12\t3592.79    \t3592.79    \n",
            "99 \t36    \t3197.57\t941.856    \t3592.79    \t803.14     \n",
            "100\t29    \t3301.31\t903.2      \t3592.79    \t-37.0506   \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected features: [ True  True False False False False False False False False False False\n",
            " False False  True False False False False False False False False False\n",
            " False  True False  True False False False False False False False False\n",
            " False False False False False False False False False False  True False\n",
            " False]\n",
            "Accuracy with selected features: 0.508\n",
            "Selecting for params: {'population_size': 20, 'mutation_probability': 0.5, 'crossover_probability': 0.2} and scoring: functools.partial(<function calculate_score at 0x78b445b73e20>, reward=10, punishment=100)\n",
            "gen\tnevals\tfitness\tfitness_std\tfitness_max\tfitness_min\n",
            "0  \t20    \t2930.47\t358.247    \t3518.07    \t2302.91    \n",
            "1  \t25    \t3330.73\t141.48     \t3518.07    \t3027.81    \n",
            "2  \t32    \t3387.71\t119.561    \t3572.87    \t3182.92    \n",
            "3  \t25    \t3451.69\t178.94     \t3572.87    \t2733.06    \n",
            "4  \t25    \t3519.18\t162.015    \t3767.88    \t2958.06    \n",
            "5  \t26    \t3566.17\t159.607    \t3767.88    \t3117.86    \n",
            "6  \t32    \t3524.96\t282.877    \t3767.88    \t2603.07    \n",
            "7  \t26    \t3696.64\t84.5715    \t3767.88    \t3493.03    \n",
            "8  \t31    \t3659.37\t175.458    \t3767.88    \t3157.96    \n",
            "9  \t28    \t3699.61\t124.65     \t3792.83    \t3363.07    \n",
            "10 \t31    \t3729.04\t117.255    \t3792.83    \t3302.89    \n",
            "11 \t32    \t3762.03\t54.771     \t3792.83    \t3527.94    \n",
            "12 \t28    \t3751.55\t175.688    \t3887.8     \t2998.01    \n",
            "13 \t34    \t3741.33\t177.44     \t3942.85    \t3197.88    \n",
            "14 \t27    \t3856.08\t77.0352    \t4042.87    \t3792.83    \n",
            "15 \t27    \t3950.84\t88.6628    \t4097.78    \t3792.83    \n",
            "16 \t26    \t3984.09\t84.6151    \t4097.78    \t3792.83    \n",
            "17 \t32    \t3971.35\t205.402    \t4097.78    \t3092.72    \n",
            "18 \t28    \t4044.34\t41.0177    \t4097.78    \t3962.89    \n",
            "19 \t26    \t4041.55\t225.622    \t4137.84    \t3062.83    \n",
            "20 \t22    \t4105.05\t22.34      \t4137.84    \t4042.87    \n",
            "21 \t28    \t4072.32\t256.475    \t4242.87    \t2962.77    \n",
            "22 \t27    \t4101.83\t130.153    \t4137.84    \t3537.91    \n",
            "23 \t25    \t4133.83\t12.0165    \t4137.84    \t4097.78    \n",
            "24 \t33    \t4077.6 \t253.555    \t4137.84    \t2973.03    \n",
            "25 \t26    \t4137.84\t9.09495e-13\t4137.84    \t4137.84    \n",
            "26 \t22    \t4073.6 \t280.021    \t4137.84    \t2853.01    \n",
            "27 \t28    \t4137.84\t9.09495e-13\t4137.84    \t4137.84    \n",
            "28 \t28    \t4086.59\t223.369    \t4137.84    \t3112.95    \n",
            "29 \t27    \t4032.35\t316.864    \t4137.84    \t3033.01    \n",
            "30 \t31    \t4082.6 \t239.642    \t4137.84    \t3038.03    \n",
            "31 \t30    \t4091.35\t202.646    \t4137.84    \t3208.04    \n",
            "32 \t35    \t4084.59\t232.099    \t4137.84    \t3072.9     \n",
            "33 \t26    \t4014.85\t380.203    \t4137.84    \t2617.96    \n",
            "34 \t26    \t4064.35\t320.347    \t4137.84    \t2667.98    \n",
            "35 \t27    \t4094.85\t187.374    \t4137.84    \t3278.11    \n",
            "36 \t30    \t4074.6 \t275.645    \t4137.84    \t2873.09    \n",
            "37 \t25    \t3986.86\t344.356    \t4137.84    \t2897.9     \n",
            "38 \t30    \t4042.35\t303.292    \t4137.84    \t2867.9     \n",
            "39 \t25    \t4060.84\t231.774    \t4137.84    \t3307.79    \n",
            "40 \t32    \t4046.11\t282.908    \t4137.84    \t3013.09    \n",
            "41 \t30    \t4108.6 \t127.433    \t4137.84    \t3553.14    \n",
            "42 \t33    \t3988.87\t304.164    \t4137.84    \t3192.97    \n",
            "43 \t29    \t4072.6 \t213.223    \t4137.84    \t3217.87    \n",
            "44 \t27    \t4137.84\t9.09495e-13\t4137.84    \t4137.84    \n",
            "45 \t28    \t4078.59\t258.261    \t4137.84    \t2952.85    \n",
            "46 \t28    \t4078.59\t258.261    \t4137.84    \t2952.85    \n",
            "47 \t24    \t4137.84\t9.09495e-13\t4137.84    \t4137.84    \n",
            "48 \t23    \t4093.1 \t195.033    \t4137.84    \t3242.97    \n",
            "49 \t31    \t4139.84\t6.01504    \t4157.89    \t4137.84    \n",
            "50 \t27    \t4085.59\t237.013    \t4157.89    \t3052.81    \n",
            "51 \t29    \t4085.59\t237.013    \t4157.89    \t3052.81    \n",
            "52 \t26    \t3993.61\t356.796    \t4157.89    \t3042.97    \n",
            "53 \t27    \t4039.14\t342.081    \t4157.89    \t2832.98    \n",
            "54 \t26    \t4117.63\t166.382    \t4157.89    \t3392.86    \n",
            "55 \t29    \t4153.39\t19.6161    \t4157.89    \t4067.88    \n",
            "56 \t32    \t4121.39\t159.078    \t4157.89    \t3427.99    \n",
            "57 \t28    \t4157.89\t9.09495e-13\t4157.89    \t4157.89    \n",
            "58 \t24    \t4092.39\t285.522    \t4157.89    \t2847.83    \n",
            "59 \t27    \t4120.9 \t161.243    \t4157.89    \t3418.05    \n",
            "60 \t31    \t4120.9 \t161.243    \t4157.89    \t3418.05    \n",
            "61 \t29    \t4038.39\t305.054    \t4157.89    \t2987.87    \n",
            "62 \t32    \t4012.16\t366.337    \t4157.89    \t2903.19    \n",
            "63 \t32    \t4059.9 \t258.425    \t4157.89    \t3112.79    \n",
            "64 \t25    \t4130.39\t119.849    \t4157.89    \t3607.98    \n",
            "65 \t28    \t4083.15\t240.784    \t4157.89    \t3133       \n",
            "66 \t24    \t4157.89\t9.09495e-13\t4157.89    \t4157.89    \n",
            "67 \t30    \t4095.64\t271.339    \t4157.89    \t2912.9     \n",
            "68 \t33    \t3978.41\t433.23     \t4157.89    \t2712.94    \n",
            "69 \t25    \t4118.9 \t169.948    \t4157.89    \t3378.11    \n",
            "70 \t30    \t4157.89\t9.09495e-13\t4157.89    \t4157.89    \n",
            "71 \t28    \t4088.15\t304.002    \t4157.89    \t2763.03    \n",
            "72 \t29    \t4157.89\t9.09495e-13\t4157.89    \t4157.89    \n",
            "73 \t27    \t4157.89\t9.09495e-13\t4157.89    \t4157.89    \n",
            "74 \t23    \t4157.89\t9.09495e-13\t4157.89    \t4157.89    \n",
            "75 \t32    \t4157.89\t9.09495e-13\t4157.89    \t4157.89    \n",
            "76 \t26    \t4157.89\t9.09495e-13\t4157.89    \t4157.89    \n",
            "77 \t26    \t4125.64\t140.588    \t4157.89    \t3512.83    \n",
            "78 \t34    \t4101.64\t170.762    \t4157.89    \t3512.83    \n",
            "79 \t29    \t4104.15\t199.668    \t4157.89    \t3248       \n",
            "80 \t30    \t4040.14\t302.989    \t4157.89    \t2952.88    \n",
            "81 \t29    \t3979.39\t407.394    \t4157.89    \t2952.88    \n",
            "82 \t26    \t4067.9 \t269.959    \t4157.89    \t3258.03    \n",
            "83 \t25    \t4067.9 \t269.959    \t4157.89    \t3258.03    \n",
            "84 \t30    \t4064.91\t253.803    \t4157.89    \t3153.01    \n",
            "85 \t25    \t4036.91\t302.953    \t4157.89    \t3008.05    \n",
            "86 \t32    \t4110.9 \t204.84     \t4157.89    \t3218.02    \n",
            "87 \t26    \t4097.39\t263.699    \t4157.89    \t2947.96    \n",
            "88 \t25    \t4157.89\t9.09495e-13\t4157.89    \t4157.89    \n",
            "89 \t27    \t4157.89\t9.09495e-13\t4157.89    \t4157.89    \n",
            "90 \t26    \t4108.65\t214.636    \t4157.89    \t3173.07    \n",
            "91 \t23    \t4089.64\t297.487    \t4157.89    \t2792.92    \n",
            "92 \t28    \t4050.91\t267.303    \t4157.89    \t3148.02    \n",
            "93 \t32    \t4096.39\t268.062    \t4157.89    \t2927.94    \n",
            "94 \t30    \t4036.64\t363.784    \t4157.89    \t2927.94    \n",
            "95 \t27    \t4100.9 \t248.417    \t4157.89    \t3018.07    \n",
            "96 \t30    \t4109.14\t212.474    \t4157.89    \t3182.99    \n",
            "97 \t21    \t4134.63\t111.768    \t4172.77    \t3647.99    \n",
            "98 \t29    \t4109.62\t227.585    \t4172.77    \t3117.99    \n",
            "99 \t32    \t4173.6 \t30.5462    \t4262.88    \t4157.89    \n",
            "100\t27    \t4146.85\t169.7      \t4262.88    \t3427.84    \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected features: [False False  True False False False False False False False False False\n",
            " False False False False  True False False False  True False False  True\n",
            " False False False False False False  True False False  True False False\n",
            " False  True False False  True  True False False False False False False\n",
            " False]\n",
            "Accuracy with selected features: 0.543\n",
            "Selecting for params: {'population_size': 25, 'mutation_probability': 0.6, 'crossover_probability': 0.3} and scoring: accuracy\n",
            "gen\tnevals\tfitness \tfitness_std\tfitness_max\tfitness_min\n",
            "0  \t25    \t0.532979\t0.00699387 \t0.544747   \t0.521502   \n",
            "1  \t43    \t0.538479\t0.00616492 \t0.546501   \t0.522498   \n",
            "2  \t43    \t0.54403 \t0.00497639 \t0.552999   \t0.534999   \n",
            "3  \t41    \t0.54744 \t0.00471913 \t0.557748   \t0.538499   \n",
            "4  \t43    \t0.550089\t0.00398139 \t0.555747   \t0.541248   \n",
            "5  \t46    \t0.550449\t0.00658547 \t0.555747   \t0.526248   \n",
            "6  \t45    \t0.552889\t0.00342656 \t0.556747   \t0.538747   \n",
            "7  \t44    \t0.553669\t0.00170896 \t0.556747   \t0.548501   \n",
            "8  \t44    \t0.552249\t0.00711226 \t0.557749   \t0.522751   \n",
            "9  \t47    \t0.553989\t0.00506475 \t0.557749   \t0.536998   \n",
            "10 \t46    \t0.555058\t0.00608959 \t0.558747   \t0.533746   \n",
            "11 \t47    \t0.554268\t0.00782719 \t0.558748   \t0.529246   \n",
            "12 \t44    \t0.554118\t0.00836112 \t0.558998   \t0.533      \n",
            "13 \t46    \t0.555938\t0.00562533 \t0.558998   \t0.534498   \n",
            "14 \t47    \t0.557248\t0.00300404 \t0.558998   \t0.548501   \n",
            "15 \t48    \t0.556878\t0.00336177 \t0.558998   \t0.548501   \n",
            "16 \t44    \t0.555948\t0.0058441  \t0.558998   \t0.534751   \n",
            "17 \t44    \t0.557297\t0.00399695 \t0.558998   \t0.540247   \n",
            "18 \t43    \t0.554448\t0.00759305 \t0.558998   \t0.534748   \n",
            "19 \t44    \t0.556458\t0.0059507  \t0.558998   \t0.540247   \n",
            "20 \t47    \t0.555498\t0.00760187 \t0.558998   \t0.53275    \n",
            "21 \t44    \t0.558158\t0.00244381 \t0.558998   \t0.5465     \n",
            "22 \t49    \t0.558578\t0.000870925\t0.558998   \t0.555498   \n",
            "23 \t44    \t0.557738\t0.00596981 \t0.558998   \t0.528501   \n",
            "24 \t44    \t0.558098\t0.00425797 \t0.558998   \t0.53725    \n",
            "25 \t44    \t0.557878\t0.0054858  \t0.558998   \t0.531003   \n",
            "26 \t47    \t0.556848\t0.00730071 \t0.558998   \t0.530749   \n",
            "27 \t46    \t0.558448\t0.00269372 \t0.558998   \t0.545251   \n",
            "28 \t47    \t0.552718\t0.0114109  \t0.558998   \t0.528502   \n",
            "29 \t44    \t0.556568\t0.00720845 \t0.558998   \t0.532246   \n",
            "30 \t47    \t0.556718\t0.00788586 \t0.558998   \t0.525002   \n",
            "31 \t46    \t0.555138\t0.00895672 \t0.558998   \t0.529752   \n",
            "32 \t48    \t0.554768\t0.00874085 \t0.558998   \t0.530752   \n",
            "33 \t46    \t0.556668\t0.00687343 \t0.558998   \t0.528751   \n",
            "34 \t48    \t0.556118\t0.00713518 \t0.558998   \t0.53675    \n",
            "35 \t45    \t0.554828\t0.00832885 \t0.558998   \t0.530253   \n",
            "36 \t46    \t0.552188\t0.0111677  \t0.558998   \t0.52975    \n",
            "37 \t47    \t0.554038\t0.00889222 \t0.558998   \t0.535246   \n",
            "38 \t47    \t0.555608\t0.00740301 \t0.558998   \t0.532001   \n",
            "39 \t48    \t0.558388\t0.00203184 \t0.558998   \t0.551499   \n",
            "40 \t44    \t0.558448\t0.00240596 \t0.558998   \t0.546749   \n",
            "41 \t50    \t0.554158\t0.00900482 \t0.558998   \t0.530498   \n",
            "42 \t45    \t0.555308\t0.0071212  \t0.558998   \t0.534003   \n",
            "43 \t41    \t0.556518\t0.00704221 \t0.558998   \t0.534001   \n",
            "44 \t45    \t0.557878\t0.00548676 \t0.558998   \t0.530998   \n",
            "45 \t43    \t0.557838\t0.00490317 \t0.561749   \t0.53825    \n",
            "46 \t42    \t0.557418\t0.00658606 \t0.561749   \t0.532248   \n",
            "47 \t40    \t0.559398\t0.00118148 \t0.561749   \t0.556498   \n",
            "48 \t44    \t0.556638\t0.00752485 \t0.561749   \t0.53       \n",
            "49 \t43    \t0.558798\t0.0040912  \t0.561749   \t0.53925    \n",
            "50 \t48    \t0.556578\t0.00695724 \t0.561749   \t0.537501   \n",
            "51 \t48    \t0.554789\t0.00971571 \t0.561498   \t0.526749   \n",
            "52 \t47    \t0.552289\t0.00903756 \t0.561498   \t0.536749   \n",
            "53 \t45    \t0.556018\t0.0069952  \t0.561498   \t0.538001   \n",
            "54 \t46    \t0.556178\t0.00783709 \t0.561498   \t0.538001   \n",
            "55 \t46    \t0.557569\t0.00571056 \t0.561498   \t0.538995   \n",
            "56 \t45    \t0.559369\t0.00504237 \t0.565748   \t0.542502   \n",
            "57 \t44    \t0.562008\t0.00138122 \t0.565748   \t0.561498   \n",
            "58 \t44    \t0.561968\t0.00140959 \t0.565748   \t0.560498   \n",
            "59 \t43    \t0.558708\t0.00926546 \t0.565748   \t0.533497   \n",
            "60 \t47    \t0.557208\t0.0100524  \t0.565748   \t0.53075    \n",
            "61 \t47    \t0.560629\t0.00560343 \t0.565748   \t0.54225    \n",
            "62 \t45    \t0.558619\t0.0083568  \t0.565748   \t0.532998   \n",
            "63 \t43    \t0.559509\t0.00927762 \t0.565748   \t0.523997   \n",
            "64 \t44    \t0.562569\t0.00479874 \t0.565748   \t0.540247   \n",
            "65 \t42    \t0.562869\t0.00631658 \t0.565748   \t0.535002   \n",
            "66 \t46    \t0.559349\t0.0103173  \t0.565748   \t0.531749   \n",
            "67 \t46    \t0.559649\t0.00817721 \t0.565748   \t0.539498   \n",
            "68 \t47    \t0.562159\t0.00717607 \t0.565748   \t0.53825    \n",
            "69 \t43    \t0.562859\t0.00816355 \t0.565748   \t0.532998   \n",
            "70 \t47    \t0.558609\t0.0137318  \t0.565748   \t0.524749   \n",
            "71 \t43    \t0.558349\t0.0130027  \t0.565748   \t0.533998   \n",
            "72 \t42    \t0.563059\t0.00725483 \t0.565748   \t0.540248   \n",
            "73 \t48    \t0.563329\t0.00787056 \t0.565748   \t0.530751   \n",
            "74 \t41    \t0.561809\t0.010159   \t0.565748   \t0.530256   \n",
            "75 \t48    \t0.558108\t0.0135758  \t0.565748   \t0.526248   \n",
            "76 \t47    \t0.559499\t0.0122964  \t0.565748   \t0.526248   \n",
            "77 \t47    \t0.562799\t0.00786513 \t0.565748   \t0.538998   \n",
            "78 \t46    \t0.561979\t0.0102843  \t0.565748   \t0.530252   \n",
            "79 \t46    \t0.562339\t0.00927567 \t0.565748   \t0.53375    \n",
            "80 \t41    \t0.563308\t0.00836113 \t0.565748   \t0.531      \n",
            "81 \t47    \t0.562519\t0.00943747 \t0.565748   \t0.530501   \n",
            "82 \t48    \t0.562449\t0.00855199 \t0.565748   \t0.532751   \n",
            "83 \t43    \t0.564438\t0.00641769 \t0.565748   \t0.532998   \n",
            "84 \t42    \t0.562249\t0.00981776 \t0.565748   \t0.526999   \n",
            "85 \t44    \t0.564738\t0.0049484  \t0.565748   \t0.540496   \n",
            "86 \t43    \t0.565748\t2.22045e-16\t0.565748   \t0.565748   \n",
            "87 \t45    \t0.564398\t0.00661395 \t0.565748   \t0.531997   \n",
            "88 \t47    \t0.560869\t0.011204   \t0.565748   \t0.533002   \n",
            "89 \t42    \t0.561979\t0.0102386  \t0.565748   \t0.531499   \n",
            "90 \t46    \t0.565308\t0.00215547 \t0.565748   \t0.554749   \n",
            "91 \t44    \t0.564349\t0.00545498 \t0.565748   \t0.53875    \n",
            "92 \t45    \t0.564448\t0.00636854 \t0.565748   \t0.533249   \n",
            "93 \t46    \t0.561979\t0.0103704  \t0.565748   \t0.528498   \n",
            "94 \t44    \t0.562918\t0.00961154 \t0.565748   \t0.528498   \n",
            "95 \t48    \t0.564288\t0.0071529  \t0.565748   \t0.529246   \n",
            "96 \t44    \t0.561989\t0.0101953  \t0.565748   \t0.532249   \n",
            "97 \t44    \t0.560479\t0.0121914  \t0.565748   \t0.527747   \n",
            "98 \t48    \t0.562739\t0.0088763  \t0.565748   \t0.531248   \n",
            "99 \t44    \t0.563808\t0.0054041  \t0.565748   \t0.546997   \n",
            "100\t48    \t0.564429\t0.00646629 \t0.565748   \t0.53275    \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected features: [ True  True False False False False False  True False  True  True  True\n",
            "  True  True  True  True False  True False  True False False False False\n",
            "  True  True False  True  True  True False  True False  True  True  True\n",
            "  True False False  True  True False  True False  True False  True  True\n",
            "  True]\n",
            "Accuracy with selected features: 0.545\n",
            "Selecting for params: {'population_size': 25, 'mutation_probability': 0.6, 'crossover_probability': 0.3} and scoring: functools.partial(<function calculate_score at 0x78b445b73e20>, reward=10, punishment=200)\n",
            "gen\tnevals\tfitness\tfitness_std\tfitness_max\tfitness_min\n",
            "0  \t25    \t554.616\t599.935    \t1953.03    \t-211.935   \n",
            "1  \t43    \t1080.82\t551.753    \t1953.03    \t-36.9151   \n",
            "2  \t45    \t1602.63\t404.167    \t1963.08    \t593.13     \n",
            "3  \t46    \t1807.62\t373.469    \t2092.95    \t398.067    \n",
            "4  \t44    \t1741.82\t400.345    \t2092.95    \t532.957    \n",
            "5  \t45    \t1933.81\t337.727    \t2378.05    \t507.932    \n",
            "6  \t42    \t2146.79\t402.817    \t2753.04    \t822.772    \n",
            "7  \t47    \t2273.03\t614.217    \t2753.04    \t142.952    \n",
            "8  \t45    \t2374.82\t612.148    \t2758.04    \t908.037    \n",
            "9  \t46    \t2324.99\t722.725    \t3107.99    \t107.887    \n",
            "10 \t44    \t2298.42\t827.672    \t2753.04    \t-142.156   \n",
            "11 \t45    \t2593.22\t397.231    \t3002.86    \t823.145    \n",
            "12 \t44    \t2629.2 \t644.827    \t3333.02    \t347.965    \n",
            "13 \t44    \t2832.58\t318.934    \t3333.02    \t1647.8     \n",
            "14 \t47    \t2810.19\t755.747    \t3333.02    \t-747.191   \n",
            "15 \t44    \t2642.39\t965.509    \t3333.02    \t433.11     \n",
            "16 \t46    \t2780.79\t823.833    \t3333.02    \t-272.025   \n",
            "17 \t44    \t3060.8 \t536.865    \t3573.06    \t697.967    \n",
            "18 \t44    \t2887.58\t924.507    \t3573.06    \t-381.883   \n",
            "19 \t36    \t3109.99\t644.246    \t3573.06    \t722.917    \n",
            "20 \t49    \t2728.8 \t1139.11    \t3573.06    \t477.917    \n",
            "21 \t44    \t3032   \t965.696    \t3573.06    \t-137.143   \n",
            "22 \t44    \t2926.19\t1130.26    \t3642.92    \t-627.01    \n",
            "23 \t44    \t3133.41\t858.067    \t3642.92    \t393.077    \n",
            "24 \t45    \t3332.8 \t531.201    \t3642.92    \t1272.9     \n",
            "25 \t43    \t3419.04\t559.495    \t3642.92    \t748.032    \n",
            "26 \t43    \t3141.99\t1124.7     \t3642.92    \t-212.173   \n",
            "27 \t43    \t3336.77\t680.558    \t3642.92    \t662.872    \n",
            "28 \t40    \t3329.56\t820.255    \t3642.92    \t107.962    \n",
            "29 \t44    \t3276.96\t966.914    \t3642.92    \t107.962    \n",
            "30 \t44    \t3591.54\t196.334    \t3642.92    \t2637.85    \n",
            "31 \t44    \t3019.76\t1374.71    \t3642.92    \t-451.983   \n",
            "32 \t42    \t3450.56\t696.711    \t3642.92    \t72.9346    \n",
            "33 \t45    \t3213.13\t1105.47    \t3642.92    \t-56.9577   \n",
            "34 \t45    \t3339.32\t1037.91    \t3642.92    \t-616.97    \n",
            "35 \t45    \t3221.55\t988.284    \t3642.92    \t167.985    \n",
            "36 \t39    \t3298.73\t863.985    \t3642.92    \t62.8493    \n",
            "37 \t46    \t3307.34\t971.957    \t3642.92    \t-1.86502   \n",
            "38 \t41    \t3281.72\t1017.33    \t3642.92    \t-437.051   \n",
            "39 \t45    \t3025.93\t1240.79    \t3642.92    \t63.1775    \n",
            "40 \t46    \t3145.95\t1140.65    \t3642.92    \t408.07     \n",
            "41 \t43    \t3428.92\t750.048    \t3642.92    \t297.877    \n",
            "42 \t48    \t3471.32\t639.182    \t3642.92    \t563.032    \n",
            "43 \t45    \t3308.73\t872.583    \t3642.92    \t127.96     \n",
            "44 \t46    \t3394.32\t788.094    \t3642.92    \t157.975    \n",
            "45 \t45    \t3226.72\t937.568    \t3642.92    \t343.005    \n",
            "46 \t45    \t3642.92\t9.09495e-13\t3642.92    \t3642.92    \n",
            "47 \t45    \t3642.92\t9.09495e-13\t3642.92    \t3642.92    \n",
            "48 \t43    \t3642.92\t9.09495e-13\t3642.92    \t3642.92    \n",
            "49 \t46    \t3376.73\t912.544    \t3642.92    \t-157.065   \n",
            "50 \t46    \t3376.73\t912.544    \t3642.92    \t-157.065   \n",
            "51 \t40    \t3642.92\t9.09495e-13\t3642.92    \t3642.92    \n",
            "52 \t46    \t3506.33\t669.164    \t3642.92    \t228.105    \n",
            "53 \t45    \t3383.52\t880.874    \t3642.92    \t237.772    \n",
            "54 \t44    \t3421.72\t750.134    \t3642.92    \t872.927    \n",
            "55 \t44    \t3454.32\t773.41     \t3642.92    \t-252.043   \n",
            "56 \t43    \t3380.12\t816.747    \t3642.92    \t-6.88508   \n",
            "57 \t45    \t3222.93\t897.251    \t3642.92    \t763.077    \n",
            "58 \t47    \t3530.11\t457.919    \t3642.92    \t1342.99    \n",
            "59 \t45    \t3444.53\t717.813    \t3642.92    \t278.215    \n",
            "60 \t44    \t3302.52\t979.454    \t3642.92    \t-277       \n",
            "61 \t45    \t3486.12\t768.144    \t3642.92    \t-277       \n",
            "62 \t43    \t3228.33\t1137.92    \t3642.92    \t-231.963   \n",
            "63 \t47    \t2883.34\t1271.91    \t3642.92    \t-97.0279   \n",
            "64 \t43    \t2546.37\t1433.5     \t3642.92    \t-97.0279   \n",
            "65 \t49    \t2818.15\t1356.4     \t3642.92    \t-77.0154   \n",
            "66 \t41    \t3200.93\t979.4      \t3642.92    \t182.777    \n",
            "67 \t43    \t3401.72\t829.363    \t3642.92    \t142.982    \n",
            "68 \t45    \t3011.92\t1216.96    \t3642.92    \t-276.993   \n",
            "69 \t47    \t3300.33\t912.046    \t3713       \t397.984    \n",
            "70 \t44    \t3463.52\t639.829    \t3642.92    \t448.11     \n",
            "71 \t42    \t3294.91\t920.275    \t3642.92    \t72.897     \n",
            "72 \t46    \t3281.93\t981.289    \t3642.92    \t448.11     \n",
            "73 \t46    \t3642.92\t9.09495e-13\t3642.92    \t3642.92    \n",
            "74 \t41    \t3377.72\t899.324    \t3642.92    \t327.967    \n",
            "75 \t43    \t2998.93\t1270.61    \t3642.92    \t327.967    \n",
            "76 \t46    \t3018.32\t1216.46    \t3642.92    \t38.1148    \n",
            "77 \t44    \t2958.71\t1170.24    \t3642.92    \t-497.035   \n",
            "78 \t48    \t3125.12\t929.662    \t3642.92    \t312.975    \n",
            "79 \t45    \t2877.5 \t1095.23    \t3642.92    \t348.002    \n",
            "80 \t46    \t3027.72\t1181.33    \t3642.92    \t343.02     \n",
            "81 \t45    \t2810.91\t1249.24    \t3642.92    \t-112.058   \n",
            "82 \t42    \t3048.33\t1088.62    \t3642.92    \t-51.9979   \n",
            "83 \t41    \t3483.93\t581.503    \t3872.97    \t1492.9     \n",
            "84 \t45    \t3456.32\t705.556    \t3657.91    \t252.907    \n",
            "85 \t45    \t3322.31\t881.374    \t3812.84    \t107.947    \n",
            "86 \t48    \t3245.11\t935.84     \t3812.84    \t557.952    \n",
            "87 \t42    \t3024.9 \t1213.85    \t3812.84    \t427.859    \n",
            "88 \t45    \t3254.29\t1065.43    \t3812.84    \t427.859    \n",
            "89 \t44    \t3773.86\t69.4644    \t3812.84    \t3642.92    \n",
            "90 \t45    \t3706.64\t520.242    \t3812.84    \t1157.99    \n",
            "91 \t42    \t3651.84\t596.259    \t3812.84    \t952.985    \n",
            "92 \t47    \t3575.65\t804.898    \t3812.84    \t742.944    \n",
            "93 \t46    \t3443.66\t1006.48    \t3812.84    \t297.967    \n",
            "94 \t45    \t3415.26\t942.147    \t3812.84    \t738.03     \n",
            "95 \t44    \t3753.64\t289.986    \t3812.84    \t2333.01    \n",
            "96 \t46    \t3812.84\t9.09495e-13\t3812.84    \t3812.84    \n",
            "97 \t45    \t3428.24\t1067.17    \t3812.84    \t112.929    \n",
            "98 \t47    \t3271.45\t1142.83    \t3812.84    \t-112.096   \n",
            "99 \t46    \t3357.84\t923.666    \t3812.84    \t1062.87    \n",
            "100\t46    \t3317.06\t1058.42    \t3812.84    \t198.06     \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected features: [False False False False False  True False False False False False False\n",
            "  True False False False False False False False False False False False\n",
            " False False False  True False False False False False  True False False\n",
            " False False False False False False False False  True False  True False\n",
            "  True]\n",
            "Accuracy with selected features: 0.537\n",
            "Selecting for params: {'population_size': 25, 'mutation_probability': 0.6, 'crossover_probability': 0.3} and scoring: functools.partial(<function calculate_score at 0x78b445b73e20>, reward=10, punishment=100)\n",
            "gen\tnevals\tfitness\tfitness_std\tfitness_max\tfitness_min\n",
            "0  \t25    \t2921.93\t252.192    \t3542.96    \t2462.99    \n",
            "1  \t46    \t3144.17\t201.148    \t3542.96    \t2807.99    \n",
            "2  \t43    \t3258.77\t204.355    \t3622.95    \t2933.01    \n",
            "3  \t44    \t3306.36\t198.375    \t3622.95    \t2933.01    \n",
            "4  \t41    \t3459.95\t146.208    \t3742.96    \t3212.91    \n",
            "5  \t43    \t3554.57\t195.478    \t3868.02    \t2947.96    \n",
            "6  \t43    \t3670.95\t166.496    \t3903.03    \t3232.96    \n",
            "7  \t41    \t3691.14\t102.517    \t3903.03    \t3482.94    \n",
            "8  \t47    \t3672.34\t218.22     \t3937.97    \t2777.93    \n",
            "9  \t43    \t3598.52\t301.901    \t3937.97    \t2888.03    \n",
            "10 \t40    \t3668.14\t318.653    \t3937.97    \t2922.85    \n",
            "11 \t47    \t3691.15\t377.878    \t3937.97    \t2603       \n",
            "12 \t45    \t3741.32\t274.289    \t3937.97    \t2788.09    \n",
            "13 \t45    \t3832.31\t81.9515    \t3967.87    \t3677.84    \n",
            "14 \t45    \t3741.9 \t356.436    \t3967.87    \t2633.02    \n",
            "15 \t49    \t3770.7 \t380.644    \t4027.93    \t2707.9     \n",
            "16 \t44    \t3815.51\t343.065    \t4027.93    \t2833.17    \n",
            "17 \t43    \t3847.48\t290.465    \t4027.93    \t2767.98    \n",
            "18 \t47    \t3842.5 \t330.111    \t4027.93    \t2738.11    \n",
            "19 \t45    \t3941.11\t125.487    \t4027.93    \t3617.75    \n",
            "20 \t47    \t3996.12\t74.9705    \t4027.93    \t3653.01    \n",
            "21 \t41    \t4030.92\t19.4318    \t4072.86    \t3967.87    \n",
            "22 \t47    \t3954.91\t304.585    \t4072.86    \t2778.03    \n",
            "23 \t43    \t3935.5 \t321.663    \t4072.86    \t2877.84    \n",
            "24 \t47    \t3785.28\t462.208    \t4072.86    \t2727.83    \n",
            "25 \t44    \t3895.69\t356.171    \t4072.86    \t2902.94    \n",
            "26 \t43    \t3922.3 \t287.41     \t4072.86    \t3097.89    \n",
            "27 \t45    \t3910.69\t357.468    \t4132.75    \t2868.04    \n",
            "28 \t43    \t3935.68\t318.431    \t4197.9     \t3032.94    \n",
            "29 \t47    \t3998.84\t251.398    \t4197.9     \t3357.86    \n",
            "30 \t45    \t4035.46\t314.042    \t4197.9     \t2758.18    \n",
            "31 \t44    \t4083.46\t301.205    \t4197.9     \t2742.95    \n",
            "32 \t46    \t4054.71\t467.207    \t4287.9     \t2742.95    \n",
            "33 \t46    \t4193.3 \t237.204    \t4287.9     \t3093.01    \n",
            "34 \t46    \t4008.71\t535.94     \t4302.67    \t2523.05    \n",
            "35 \t46    \t4090.7 \t418.609    \t4322.67    \t2932.95    \n",
            "36 \t42    \t4141.65\t426.056    \t4322.67    \t2437.87    \n",
            "37 \t44    \t4218.24\t293.259    \t4322.67    \t2912.82    \n",
            "38 \t45    \t4086.65\t521.842    \t4322.67    \t2513.13    \n",
            "39 \t41    \t4260.56\t248.031    \t4322.67    \t3048.01    \n",
            "40 \t44    \t4264.52\t256.797    \t4322.67    \t3008.02    \n",
            "41 \t46    \t4260.49\t297.599    \t4322.67    \t2802.94    \n",
            "42 \t46    \t4113.12\t484.136    \t4322.67    \t2848.01    \n",
            "43 \t46    \t4092.95\t492.217    \t4322.67    \t2762.99    \n",
            "44 \t44    \t4277.88\t195.89     \t4397.62    \t3377.89    \n",
            "45 \t43    \t4335.86\t32.1092    \t4397.62    \t4277.58    \n",
            "46 \t47    \t4232.89\t401.075    \t4397.62    \t2743.16    \n",
            "47 \t44    \t4317.45\t325.95     \t4507.63    \t2733       \n",
            "48 \t43    \t4309.65\t398.674    \t4507.63    \t2368.08    \n",
            "49 \t46    \t4390.83\t43.2704    \t4507.63    \t4242.71    \n",
            "50 \t47    \t4354.85\t256.824    \t4507.63    \t3108.05    \n",
            "51 \t43    \t4205.69\t563.799    \t4507.63    \t2413.12    \n",
            "52 \t43    \t4181.3 \t575.459    \t4507.63    \t2493.1     \n",
            "53 \t47    \t4263.87\t455.558    \t4507.63    \t2877.84    \n",
            "54 \t48    \t4288.06\t537.101    \t4507.63    \t2277.86    \n",
            "55 \t48    \t4245.89\t525.338    \t4507.63    \t2888.12    \n",
            "56 \t44    \t4143.31\t628.963    \t4507.63    \t2552.87    \n",
            "57 \t46    \t4267.9 \t487.795    \t4507.63    \t2868.1     \n",
            "58 \t43    \t4478.65\t157.764    \t4647.72    \t3752.96    \n",
            "59 \t47    \t4362.49\t491.666    \t4647.72    \t2678.18    \n",
            "60 \t40    \t4452.08\t373.892    \t4647.72    \t2887.98    \n",
            "61 \t43    \t4440.52\t507.017    \t4647.72    \t2473.06    \n",
            "62 \t46    \t4429.16\t458.869    \t4647.72    \t2958.07    \n",
            "63 \t44    \t4411.56\t598.038    \t4647.72    \t2422.91    \n",
            "64 \t44    \t4504.17\t400.88     \t4647.72    \t2748       \n",
            "65 \t47    \t4507.94\t435.474    \t4647.72    \t2972.94    \n",
            "66 \t43    \t4530.14\t316.688    \t4647.72    \t3117.85    \n",
            "67 \t44    \t4576.33\t277.181    \t4647.72    \t3278.08    \n",
            "68 \t40    \t4536.93\t389.879    \t4647.72    \t2707.98    \n",
            "69 \t46    \t4562.93\t415.379    \t4647.72    \t2528       \n",
            "70 \t42    \t4647.72\t9.09495e-13\t4647.72    \t4647.72    \n",
            "71 \t47    \t4509.94\t467.623    \t4647.72    \t2858.05    \n",
            "72 \t44    \t4557.53\t441.816    \t4647.72    \t2393.08    \n",
            "73 \t47    \t4333.97\t721.119    \t4647.72    \t2448.12    \n",
            "74 \t45    \t4363.56\t667.431    \t4647.72    \t2383.04    \n",
            "75 \t45    \t4470.54\t518.111    \t4647.72    \t2637.98    \n",
            "76 \t47    \t4431.13\t539.712    \t4647.72    \t2503.09    \n",
            "77 \t46    \t4595.11\t241.609    \t4647.72    \t3412.67    \n",
            "78 \t47    \t4598.32\t242.019    \t4647.72    \t3412.67    \n",
            "79 \t48    \t4647.72\t9.09495e-13\t4647.72    \t4647.72    \n",
            "80 \t44    \t4579.53\t334.052    \t4647.72    \t2943.02    \n",
            "81 \t48    \t4590.53\t280.172    \t4647.72    \t3217.97    \n",
            "82 \t42    \t4505.54\t484.487    \t4647.72    \t2702.9     \n",
            "83 \t40    \t4516.72\t459.643    \t4647.72    \t2592.92    \n",
            "84 \t48    \t4647.72\t9.09495e-13\t4647.72    \t4647.72    \n",
            "85 \t46    \t4350.59\t683.706    \t4647.72    \t2578.22    \n",
            "86 \t44    \t4441.76\t568.048    \t4647.72    \t2518.12    \n",
            "87 \t48    \t4498.94\t508.359    \t4647.72    \t2567.88    \n",
            "88 \t44    \t4508.95\t475.87     \t4647.72    \t2663.14    \n",
            "89 \t46    \t4434.95\t580.438    \t4647.72    \t2663.14    \n",
            "90 \t43    \t4577.94\t341.869    \t4647.72    \t2903.12    \n",
            "91 \t44    \t4570.33\t379.121    \t4647.72    \t2713.03    \n",
            "92 \t47    \t4548.53\t410.174    \t4647.72    \t2577.93    \n",
            "93 \t47    \t4458.95\t490.726    \t4647.72    \t2793.05    \n",
            "94 \t43    \t4566.13\t399.716    \t4647.72    \t2607.92    \n",
            "95 \t49    \t4414.96\t639.075    \t4647.72    \t2332.93    \n",
            "96 \t47    \t4493.54\t522.874    \t4647.72    \t2702.99    \n",
            "97 \t47    \t4148.8 \t806.151    \t4647.72    \t2548.23    \n",
            "98 \t47    \t4160.01\t783.361    \t4647.72    \t2513.24    \n",
            "99 \t45    \t4343.57\t641.883    \t4647.72    \t2563.16    \n",
            "100\t46    \t4312.99\t655.966    \t4647.72    \t2353.11    \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected features: [False False False  True False False False False False False False False\n",
            " False False False False False False False  True False False False  True\n",
            " False False False False False False False False False False False False\n",
            " False False False False False False False  True False False False False\n",
            " False]\n",
            "Accuracy with selected features: 0.52\n",
            "Selecting for params: {'population_size': 15, 'mutation_probability': 0.5, 'crossover_probability': 0.4} and scoring: accuracy\n",
            "gen\tnevals\tfitness \tfitness_std\tfitness_max\tfitness_min\n",
            "0  \t15    \t0.533266\t0.00588773 \t0.5455     \t0.52175    \n",
            "1  \t28    \t0.5421  \t0.00411444 \t0.549001   \t0.534754   \n",
            "2  \t25    \t0.541549\t0.00517566 \t0.549001   \t0.532249   \n",
            "3  \t25    \t0.542799\t0.003035   \t0.549001   \t0.535999   \n",
            "4  \t29    \t0.543916\t0.00418929 \t0.55025    \t0.53425    \n",
            "5  \t28    \t0.544732\t0.00370424 \t0.55025    \t0.53425    \n",
            "6  \t29    \t0.546865\t0.00330183 \t0.55325    \t0.538749   \n",
            "7  \t25    \t0.548532\t0.0026472  \t0.55325    \t0.5435     \n",
            "8  \t26    \t0.548632\t0.00496096 \t0.55325    \t0.53875    \n",
            "9  \t29    \t0.549782\t0.00348881 \t0.55325    \t0.542999   \n",
            "10 \t26    \t0.551399\t0.00168614 \t0.55325    \t0.548495   \n",
            "11 \t28    \t0.550233\t0.00494797 \t0.55325    \t0.535248   \n",
            "12 \t28    \t0.545349\t0.00649567 \t0.55325    \t0.535749   \n",
            "13 \t27    \t0.548366\t0.00424811 \t0.551999   \t0.538498   \n",
            "14 \t27    \t0.550483\t0.00182228 \t0.551999   \t0.545999   \n",
            "15 \t29    \t0.550817\t0.00143275 \t0.551999   \t0.546749   \n",
            "16 \t24    \t0.550667\t0.0018942  \t0.551999   \t0.544499   \n",
            "17 \t29    \t0.549967\t0.00329905 \t0.551999   \t0.539999   \n",
            "18 \t22    \t0.5513  \t0.00135056 \t0.552499   \t0.547502   \n",
            "19 \t27    \t0.550416\t0.00407024 \t0.552499   \t0.537247   \n",
            "20 \t26    \t0.550533\t0.00357833 \t0.552499   \t0.54175    \n",
            "21 \t27    \t0.550066\t0.00392073 \t0.554247   \t0.537251   \n",
            "22 \t27    \t0.550316\t0.00372582 \t0.554247   \t0.541503   \n",
            "23 \t24    \t0.552399\t0.00220905 \t0.554498   \t0.545499   \n",
            "24 \t27    \t0.553349\t0.00242814 \t0.554999   \t0.54725    \n",
            "25 \t28    \t0.552966\t0.00420171 \t0.554999   \t0.5375     \n",
            "26 \t29    \t0.553449\t0.00427826 \t0.55775    \t0.538749   \n",
            "27 \t30    \t0.554233\t0.00255902 \t0.55775    \t0.545755   \n",
            "28 \t29    \t0.555199\t0.00134584 \t0.55775    \t0.553998   \n",
            "29 \t27    \t0.553549\t0.00347472 \t0.55775    \t0.542497   \n",
            "30 \t27    \t0.555116\t0.00135394 \t0.55775    \t0.553998   \n",
            "31 \t29    \t0.555016\t0.00117444 \t0.55775    \t0.554249   \n",
            "32 \t28    \t0.554232\t0.00297496 \t0.55775    \t0.543496   \n",
            "33 \t25    \t0.554915\t0.000174535\t0.554999   \t0.554499   \n",
            "34 \t25    \t0.554865\t0.000220887\t0.554999   \t0.554249   \n",
            "35 \t28    \t0.555182\t0.000412926\t0.555998   \t0.554749   \n",
            "36 \t25    \t0.555265\t0.000442161\t0.555998   \t0.554999   \n",
            "37 \t27    \t0.555399\t0.000489837\t0.555998   \t0.554999   \n",
            "38 \t29    \t0.553599\t0.00659008 \t0.555998   \t0.529002   \n",
            "39 \t29    \t0.551866\t0.00781427 \t0.555998   \t0.534249   \n",
            "40 \t28    \t0.554099\t0.00465733 \t0.555998   \t0.541752   \n",
            "41 \t22    \t0.556032\t0.000125735\t0.556503   \t0.555998   \n",
            "42 \t28    \t0.555249\t0.00294148 \t0.556503   \t0.544253   \n",
            "43 \t26    \t0.555849\t0.000705748\t0.556503   \t0.55325    \n",
            "44 \t28    \t0.554633\t0.00499785 \t0.556503   \t0.536001   \n",
            "45 \t30    \t0.554715\t0.00500517 \t0.556749   \t0.536001   \n",
            "46 \t28    \t0.556215\t0.000638878\t0.558501   \t0.555998   \n",
            "47 \t27    \t0.556382\t0.000851797\t0.558501   \t0.555998   \n",
            "48 \t27    \t0.556499\t0.00100115 \t0.558501   \t0.555998   \n",
            "49 \t27    \t0.554099\t0.00639057 \t0.558501   \t0.538      \n",
            "50 \t26    \t0.553883\t0.00521668 \t0.558501   \t0.542502   \n",
            "51 \t25    \t0.555882\t0.00196487 \t0.558501   \t0.549247   \n",
            "52 \t26    \t0.555266\t0.00619475 \t0.558501   \t0.532496   \n",
            "53 \t30    \t0.556751\t0.00465096 \t0.558501   \t0.539749   \n",
            "54 \t29    \t0.555951\t0.0065713  \t0.558501   \t0.536753   \n",
            "55 \t25    \t0.558568\t0.000248945\t0.559499   \t0.558501   \n",
            "56 \t28    \t0.558285\t0.000810521\t0.558501   \t0.555252   \n",
            "57 \t27    \t0.558501\t1.11022e-16\t0.558501   \t0.558501   \n",
            "58 \t26    \t0.558501\t1.11022e-16\t0.558501   \t0.558501   \n",
            "59 \t28    \t0.558501\t1.11022e-16\t0.558501   \t0.558501   \n",
            "60 \t28    \t0.558435\t0.000249132\t0.558501   \t0.557503   \n",
            "61 \t25    \t0.558501\t1.11022e-16\t0.558501   \t0.558501   \n",
            "62 \t27    \t0.555951\t0.00718578 \t0.558501   \t0.530997   \n",
            "63 \t29    \t0.558501\t1.11022e-16\t0.558501   \t0.558501   \n",
            "64 \t27    \t0.558501\t1.11022e-16\t0.558501   \t0.558501   \n",
            "65 \t27    \t0.556884\t0.0043895  \t0.558501   \t0.542247   \n",
            "66 \t30    \t0.554334\t0.00836022 \t0.558501   \t0.536501   \n",
            "67 \t27    \t0.556118\t0.00612603 \t0.558501   \t0.5385     \n",
            "68 \t28    \t0.556968\t0.00573766 \t0.558501   \t0.5355     \n",
            "69 \t29    \t0.557468\t0.0038673  \t0.558501   \t0.542998   \n",
            "70 \t27    \t0.557435\t0.00399144 \t0.558501   \t0.5425     \n",
            "71 \t28    \t0.558501\t1.11022e-16\t0.558501   \t0.558501   \n",
            "72 \t29    \t0.558501\t1.11022e-16\t0.558501   \t0.558501   \n",
            "73 \t30    \t0.555751\t0.00741636 \t0.558501   \t0.531254   \n",
            "74 \t28    \t0.555201\t0.00749751 \t0.558501   \t0.531254   \n",
            "75 \t25    \t0.556501\t0.00548069 \t0.558501   \t0.538      \n",
            "76 \t28    \t0.556584\t0.00520146 \t0.558501   \t0.539248   \n",
            "77 \t28    \t0.556584\t0.00520166 \t0.558501   \t0.539248   \n",
            "78 \t29    \t0.556018\t0.00639143 \t0.558501   \t0.537497   \n",
            "79 \t26    \t0.558501\t1.11022e-16\t0.558501   \t0.558501   \n",
            "80 \t28    \t0.557768\t0.00274303 \t0.558501   \t0.547505   \n",
            "81 \t26    \t0.555768\t0.0057208  \t0.558501   \t0.539498   \n",
            "82 \t28    \t0.555634\t0.00739138 \t0.558501   \t0.533998   \n",
            "83 \t25    \t0.556718\t0.00667368 \t0.558501   \t0.531747   \n",
            "84 \t27    \t0.554434\t0.00813863 \t0.558501   \t0.537749   \n",
            "85 \t28    \t0.558501\t1.11022e-16\t0.558501   \t0.558501   \n",
            "86 \t27    \t0.557551\t0.00355598 \t0.558501   \t0.544246   \n",
            "87 \t26    \t0.557584\t0.00343052 \t0.558501   \t0.544749   \n",
            "88 \t29    \t0.555451\t0.00783585 \t0.558501   \t0.533001   \n",
            "89 \t27    \t0.555368\t0.00806385 \t0.558501   \t0.532001   \n",
            "90 \t28    \t0.557218\t0.00480169 \t0.558501   \t0.539252   \n",
            "91 \t28    \t0.554784\t0.00746464 \t0.558501   \t0.538495   \n",
            "92 \t27    \t0.558501\t1.11022e-16\t0.558501   \t0.558501   \n",
            "93 \t28    \t0.556901\t0.00598703 \t0.558501   \t0.5345     \n",
            "94 \t26    \t0.558501\t1.11022e-16\t0.558501   \t0.558501   \n",
            "95 \t28    \t0.557301\t0.00449083 \t0.558501   \t0.540498   \n",
            "96 \t28    \t0.558501\t1.11022e-16\t0.558501   \t0.558501   \n",
            "97 \t26    \t0.558501\t1.11022e-16\t0.558501   \t0.558501   \n",
            "98 \t28    \t0.556951\t0.00579971 \t0.558501   \t0.535251   \n",
            "99 \t27    \t0.555218\t0.00838627 \t0.558501   \t0.5325     \n",
            "100\t24    \t0.555784\t0.00700569 \t0.558501   \t0.53525    \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected features: [False  True  True False  True  True False  True False  True False False\n",
            " False False  True  True  True  True False  True  True False  True  True\n",
            "  True  True  True  True  True  True  True  True  True False  True False\n",
            " False False False False  True False  True  True False False  True False\n",
            "  True]\n",
            "Accuracy with selected features: 0.538\n",
            "Selecting for params: {'population_size': 15, 'mutation_probability': 0.5, 'crossover_probability': 0.4} and scoring: functools.partial(<function calculate_score at 0x78b445b73e20>, reward=10, punishment=200)\n",
            "gen\tnevals\tfitness\tfitness_std\tfitness_max\tfitness_min\n",
            "0  \t15    \t530.978\t727.315    \t1692.93    \t-771.888   \n",
            "1  \t26    \t1241.31\t432.776    \t1692.93    \t338        \n",
            "2  \t28    \t1475.34\t438.313    \t2337.95    \t338        \n",
            "3  \t27    \t1736.32\t354.289    \t2337.95    \t917.942    \n",
            "4  \t28    \t1989.27\t283.4      \t2467.91    \t1363.02    \n",
            "5  \t27    \t2243.25\t145.799    \t2467.91    \t2017.89    \n",
            "6  \t28    \t2287.92\t237.513    \t2733.06    \t2017.89    \n",
            "7  \t28    \t2229.21\t522.085    \t2733.06    \t393.07     \n",
            "8  \t27    \t2323.24\t270.412    \t2733.06    \t1628.1     \n",
            "9  \t25    \t2399.51\t580.17     \t3092.6     \t403.05     \n",
            "10 \t26    \t2570.17\t529.742    \t3222.82    \t1117.94    \n",
            "11 \t29    \t2686.46\t493.463    \t3092.6     \t1092.88    \n",
            "12 \t28    \t2963.7 \t153.985    \t3092.6     \t2602.83    \n",
            "13 \t28    \t2886.66\t409.971    \t3092.6     \t1627.86    \n",
            "14 \t23    \t3079.93\t47.39      \t3092.6     \t2902.61    \n",
            "15 \t27    \t2977.96\t381.106    \t3092.6     \t1563       \n",
            "16 \t26    \t2893.29\t745.738    \t3092.6     \t102.995    \n",
            "17 \t28    \t3092.6 \t9.09495e-13\t3092.6     \t3092.6     \n",
            "18 \t28    \t3092.6 \t9.09495e-13\t3092.6     \t3092.6     \n",
            "19 \t26    \t3092.6 \t9.09495e-13\t3092.6     \t3092.6     \n",
            "20 \t26    \t2951.3 \t528.691    \t3092.6     \t973.117    \n",
            "21 \t29    \t2760.33\t847.127    \t3092.6     \t598.06     \n",
            "22 \t27    \t2582.36\t1024.22    \t3092.6     \t393.047    \n",
            "23 \t27    \t3092.6 \t9.09495e-13\t3092.6     \t3092.6     \n",
            "24 \t27    \t2926.29\t622.262    \t3092.6     \t598        \n",
            "25 \t27    \t3092.6 \t9.09495e-13\t3092.6     \t3092.6     \n",
            "26 \t27    \t2831.31\t666.165    \t3092.6     \t1132.91    \n",
            "27 \t27    \t2457.68\t1068.38    \t3092.6     \t302.897    \n",
            "28 \t26    \t3077.95\t54.8102    \t3092.6     \t2872.87    \n",
            "29 \t27    \t3092.6 \t9.09495e-13\t3092.6     \t3092.6     \n",
            "30 \t28    \t3001.61\t340.428    \t3092.6     \t1727.85    \n",
            "31 \t29    \t2950.96\t529.96     \t3092.6     \t968.03     \n",
            "32 \t29    \t2882.95\t784.407    \t3092.6     \t-52.028    \n",
            "33 \t26    \t3092.6 \t9.09495e-13\t3092.6     \t3092.6     \n",
            "34 \t26    \t3012.62\t299.26     \t3092.6     \t1892.89    \n",
            "35 \t25    \t2940.61\t568.679    \t3092.6     \t812.807    \n",
            "36 \t26    \t2946.95\t544.955    \t3092.6     \t907.917    \n",
            "37 \t29    \t3092.6 \t9.09495e-13\t3092.6     \t3092.6     \n",
            "38 \t28    \t3092.6 \t9.09495e-13\t3092.6     \t3092.6     \n",
            "39 \t25    \t2946.28\t547.454    \t3092.6     \t897.899    \n",
            "40 \t28    \t2855.64\t698.908    \t3092.6     \t352.97     \n",
            "41 \t27    \t3038.28\t203.226    \t3092.6     \t2277.88    \n",
            "42 \t28    \t3092.6 \t9.09495e-13\t3092.6     \t3092.6     \n",
            "43 \t28    \t3092.6 \t9.09495e-13\t3092.6     \t3092.6     \n",
            "44 \t27    \t3092.6 \t9.09495e-13\t3092.6     \t3092.6     \n",
            "45 \t28    \t3092.6 \t9.09495e-13\t3092.6     \t3092.6     \n",
            "46 \t29    \t3092.6 \t9.09495e-13\t3092.6     \t3092.6     \n",
            "47 \t28    \t2986.62\t396.531    \t3092.6     \t1502.93    \n",
            "48 \t23    \t3092.6 \t9.09495e-13\t3092.6     \t3092.6     \n",
            "49 \t28    \t2963.96\t481.31     \t3092.6     \t1163.06    \n",
            "50 \t26    \t3092.6 \t9.09495e-13\t3092.6     \t3092.6     \n",
            "51 \t26    \t2922.63\t635.969    \t3092.6     \t543.05     \n",
            "52 \t25    \t3092.6 \t9.09495e-13\t3092.6     \t3092.6     \n",
            "53 \t28    \t2903.64\t707.028    \t3092.6     \t258.18     \n",
            "54 \t26    \t2782.33\t626.251    \t3092.6     \t1407.8     \n",
            "55 \t29    \t2794.32\t793.17     \t3092.6     \t238.175    \n",
            "56 \t25    \t2867.29\t574.414    \t3092.6     \t1402.82    \n",
            "57 \t26    \t2818.63\t711.075    \t3092.6     \t672.904    \n",
            "58 \t27    \t3092.6 \t9.09495e-13\t3092.6     \t3092.6     \n",
            "59 \t29    \t3092.6 \t9.09495e-13\t3092.6     \t3092.6     \n",
            "60 \t28    \t3092.6 \t9.09495e-13\t3092.6     \t3092.6     \n",
            "61 \t27    \t2949.96\t533.7      \t3092.6     \t953.037    \n",
            "62 \t24    \t3045.61\t175.817    \t3092.6     \t2387.76    \n",
            "63 \t28    \t2932.62\t598.583    \t3092.6     \t692.925    \n",
            "64 \t29    \t2668.63\t854.059    \t3092.6     \t692.925    \n",
            "65 \t28    \t3026.26\t248.19     \t3092.6     \t2097.62    \n",
            "66 \t28    \t3092.6 \t9.09495e-13\t3092.6     \t3092.6     \n",
            "67 \t27    \t3092.6 \t9.09495e-13\t3092.6     \t3092.6     \n",
            "68 \t28    \t3092.6 \t9.09495e-13\t3092.6     \t3092.6     \n",
            "69 \t27    \t3092.6 \t9.09495e-13\t3092.6     \t3092.6     \n",
            "70 \t29    \t3092.6 \t9.09495e-13\t3092.6     \t3092.6     \n",
            "71 \t28    \t2956.61\t508.801    \t3092.6     \t1052.85    \n",
            "72 \t26    \t2956.61\t508.801    \t3092.6     \t1052.85    \n",
            "73 \t27    \t2797.97\t751.174    \t3092.6     \t867.884    \n",
            "74 \t24    \t3092.6 \t9.09495e-13\t3092.6     \t3092.6     \n",
            "75 \t25    \t3092.6 \t9.09495e-13\t3092.6     \t3092.6     \n",
            "76 \t29    \t2894.96\t739.497    \t3092.6     \t128.012    \n",
            "77 \t26    \t2792.96\t766.23     \t3092.6     \t682.877    \n",
            "78 \t27    \t2856.64\t627.106    \t3092.6     \t837.824    \n",
            "79 \t29    \t2768.65\t698.131    \t3092.6     \t837.824    \n",
            "80 \t27    \t3092.6 \t9.09495e-13\t3092.6     \t3092.6     \n",
            "81 \t26    \t3092.6 \t9.09495e-13\t3092.6     \t3092.6     \n",
            "82 \t28    \t3092.6 \t9.09495e-13\t3092.6     \t3092.6     \n",
            "83 \t26    \t2730.98\t738.771    \t3092.6     \t807.794    \n",
            "84 \t29    \t2691.33\t666.516    \t3092.6     \t1522.9     \n",
            "85 \t29    \t2915.63\t549.174    \t3092.6     \t907.887    \n",
            "86 \t27    \t3044.61\t147.378    \t3092.6     \t2507.65    \n",
            "87 \t29    \t2775.7 \t805.945    \t3142.74    \t473.195    \n",
            "88 \t25    \t3121.98\t62.5957    \t3342.73    \t3092.6     \n",
            "89 \t26    \t2771.05\t860.714    \t3342.73    \t472.882    \n",
            "90 \t27    \t2845.41\t1023.31    \t3832.82    \t-616.963   \n",
            "91 \t24    \t3084.41\t714.79     \t3832.82    \t1082.91    \n",
            "92 \t29    \t3345.43\t495.754    \t3832.82    \t1647.88    \n",
            "93 \t28    \t3591.46\t157.088    \t3832.82    \t3342.73    \n",
            "94 \t26    \t3353.18\t871.309    \t3832.82    \t917.95     \n",
            "95 \t27    \t3365.5 \t977.944    \t3867.9     \t857.987    \n",
            "96 \t26    \t3813.11\t77.9076    \t3947.72    \t3682.66    \n",
            "97 \t27    \t3848.14\t39.0584    \t3947.72    \t3832.82    \n",
            "98 \t24    \t3863.46\t50.8106    \t3947.72    \t3832.82    \n",
            "99 \t30    \t3630.49\t850.411    \t3947.72    \t453.107    \n",
            "100\t28    \t3855.8 \t45.9599    \t3947.72    \t3832.82    \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected features: [False False  True False False  True False False False False False False\n",
            "  True False False False  True False False False False False False False\n",
            " False False False False False False False False False False False False\n",
            " False False False False False  True False False False False  True False\n",
            " False]\n",
            "Accuracy with selected features: 0.496\n",
            "Selecting for params: {'population_size': 15, 'mutation_probability': 0.5, 'crossover_probability': 0.4} and scoring: functools.partial(<function calculate_score at 0x78b445b73e20>, reward=10, punishment=100)\n",
            "gen\tnevals\tfitness\tfitness_std\tfitness_max\tfitness_min\n",
            "0  \t15    \t2700.99\t361.878    \t3283.04    \t2183.03    \n",
            "1  \t27    \t3105.29\t237.39     \t3392.92    \t2668.01    \n",
            "2  \t27    \t3255.68\t184.881    \t3602.94    \t2973.06    \n",
            "3  \t26    \t3345.98\t172.466    \t3602.94    \t2823.09    \n",
            "4  \t28    \t3431.64\t86.4298    \t3542.95    \t3277.95    \n",
            "5  \t30    \t3552.31\t118.807    \t3737.88    \t3392.92    \n",
            "6  \t28    \t3687.93\t140.077    \t3912.91    \t3423.02    \n",
            "7  \t25    \t3770.55\t143.239    \t3912.91    \t3423.02    \n",
            "8  \t24    \t3916.93\t183.64     \t4363.1     \t3697.79    \n",
            "9  \t28    \t3928.32\t411.259    \t4363.1     \t2813.02    \n",
            "10 \t28    \t4066.33\t288.066    \t4363.1     \t3282.88    \n",
            "11 \t24    \t4113.71\t247.051    \t4363.1     \t3607.98    \n",
            "12 \t29    \t4122.71\t286.364    \t4392.93    \t3458.16    \n",
            "13 \t27    \t4042.71\t458.209    \t4413.02    \t3097.92    \n",
            "14 \t26    \t4224.66\t270.341    \t4413.02    \t3647.72    \n",
            "15 \t26    \t4318.66\t177.359    \t4423.05    \t3817.96    \n",
            "16 \t25    \t4306.7 \t224.018    \t4423.05    \t3512.96    \n",
            "17 \t28    \t4359.36\t170.352    \t4423.05    \t3737.91    \n",
            "18 \t24    \t4443.38\t41.3117    \t4498.02    \t4363.1     \n",
            "19 \t26    \t4188.01\t570.221    \t4498.02    \t2717.9     \n",
            "20 \t28    \t3916.96\t550.247    \t4498.02    \t2907.72    \n",
            "21 \t30    \t4209.98\t364.976    \t4498.02    \t3382.88    \n",
            "22 \t27    \t4078.94\t478.419    \t4517.8     \t3008.17    \n",
            "23 \t27    \t4322.28\t241.681    \t4517.8     \t3742.9     \n",
            "24 \t30    \t4468.26\t120.645    \t4612.84    \t4118.03    \n",
            "25 \t28    \t4468.26\t117.534    \t4612.84    \t4118.03    \n",
            "26 \t27    \t4520.24\t118.156    \t4612.84    \t4118.03    \n",
            "27 \t26    \t4562.2 \t54.7312    \t4612.84    \t4492.96    \n",
            "28 \t27    \t4520.19\t291.561    \t4612.84    \t3437.9     \n",
            "29 \t26    \t4612.84\t9.09495e-13\t4612.84    \t4612.84    \n",
            "30 \t27    \t4612.84\t9.09495e-13\t4612.84    \t4612.84    \n",
            "31 \t27    \t4555.85\t213.269    \t4612.84    \t3757.87    \n",
            "32 \t28    \t4524.85\t329.259    \t4612.84    \t3292.87    \n",
            "33 \t23    \t4546.53\t248.132    \t4612.84    \t3618.1     \n",
            "34 \t29    \t4434.54\t456.963    \t4612.84    \t3148.07    \n",
            "35 \t27    \t4612.84\t9.09495e-13\t4612.84    \t4612.84    \n",
            "36 \t27    \t4612.84\t9.09495e-13\t4612.84    \t4612.84    \n",
            "37 \t27    \t4424.2 \t496.735    \t4612.84    \t2857.86    \n",
            "38 \t28    \t4612.84\t9.09495e-13\t4612.84    \t4612.84    \n",
            "39 \t28    \t4612.84\t9.09495e-13\t4612.84    \t4612.84    \n",
            "40 \t24    \t4612.84\t9.09495e-13\t4612.84    \t4612.84    \n",
            "41 \t29    \t4612.84\t9.09495e-13\t4612.84    \t4612.84    \n",
            "42 \t25    \t4323.54\t578.73     \t4612.84    \t3148.01    \n",
            "43 \t24    \t4487.86\t362.373    \t4612.84    \t3202.9     \n",
            "44 \t29    \t4551.53\t156.327    \t4612.84    \t4147.96    \n",
            "45 \t28    \t4520.85\t344.201    \t4612.84    \t3232.97    \n",
            "46 \t28    \t4528.51\t315.543    \t4612.84    \t3347.86    \n",
            "47 \t29    \t4495.52\t438.981    \t4612.84    \t2853.01    \n",
            "48 \t26    \t4612.84\t9.09495e-13\t4612.84    \t4612.84    \n",
            "49 \t28    \t4426.19\t478.617    \t4612.84    \t3072.9     \n",
            "50 \t25    \t4612.84\t9.09495e-13\t4612.84    \t4612.84    \n",
            "51 \t28    \t4507.52\t394.097    \t4612.84    \t3032.94    \n",
            "52 \t27    \t4612.84\t9.09495e-13\t4612.84    \t4612.84    \n",
            "53 \t26    \t4612.84\t9.09495e-13\t4612.84    \t4612.84    \n",
            "54 \t29    \t4612.84\t9.09495e-13\t4612.84    \t4612.84    \n",
            "55 \t26    \t4525.85\t325.499    \t4612.84    \t3307.95    \n",
            "56 \t29    \t4428.85\t469.894    \t4612.84    \t3157.84    \n",
            "57 \t27    \t4612.84\t9.09495e-13\t4612.84    \t4612.84    \n",
            "58 \t28    \t4545.18\t253.177    \t4612.84    \t3597.88    \n",
            "59 \t29    \t4604.84\t29.9645    \t4612.84    \t4492.72    \n",
            "60 \t28    \t4604.83\t29.9876    \t4612.84    \t4492.63    \n",
            "61 \t27    \t4416.84\t520.918    \t4677.77    \t2857.9     \n",
            "62 \t27    \t4463.19\t420.757    \t4677.77    \t3058.06    \n",
            "63 \t29    \t4582.17\t114.78     \t4612.84    \t4152.7     \n",
            "64 \t27    \t4580.17\t96.8059    \t4612.84    \t4232.73    \n",
            "65 \t28    \t4507.52\t394.091    \t4612.84    \t3032.96    \n",
            "66 \t28    \t4612.84\t9.09495e-13\t4612.84    \t4612.84    \n",
            "67 \t26    \t4422.86\t484.53     \t4612.84    \t3152.97    \n",
            "68 \t28    \t4329.87\t511.46     \t4612.84    \t3097.94    \n",
            "69 \t27    \t4446.52\t357.409    \t4612.84    \t3287.92    \n",
            "70 \t25    \t4618.23\t8.92408    \t4633.02    \t4612.84    \n",
            "71 \t29    \t4619.57\t9.5131     \t4633.02    \t4612.84    \n",
            "72 \t28    \t4544.6 \t337.053    \t4707.82    \t3287.99    \n",
            "73 \t26    \t4465.93\t489.797    \t4707.82    \t3137.89    \n",
            "74 \t27    \t4652.6 \t37.1341    \t4707.82    \t4612.84    \n",
            "75 \t22    \t4668.22\t41.6442    \t4707.82    \t4612.84    \n",
            "76 \t27    \t4705.84\t3.9459     \t4707.82    \t4697.95    \n",
            "77 \t28    \t4601.49\t397.854    \t4707.82    \t3112.85    \n",
            "78 \t26    \t4518.16\t485.589    \t4707.82    \t3162.89    \n",
            "79 \t29    \t4671.49\t239.977    \t4887.8     \t3802.92    \n",
            "80 \t27    \t4770.8 \t90.6381    \t4952.71    \t4707.82    \n",
            "81 \t28    \t4748.77\t388.61     \t4982.68    \t3337.95    \n",
            "82 \t27    \t4508.81\t730.538    \t4982.68    \t2878.02    \n",
            "83 \t27    \t4858.08\t82.2323    \t4982.68    \t4707.82    \n",
            "84 \t28    \t4918.72\t89.4944    \t5022.66    \t4707.82    \n",
            "85 \t30    \t4833.38\t429.512    \t5022.66    \t3242.97    \n",
            "86 \t30    \t4944.38\t62.1014    \t5022.66    \t4812.74    \n",
            "87 \t27    \t4871.73\t491.303    \t5062.78    \t3038.17    \n",
            "88 \t28    \t4787.39\t621.77     \t5062.78    \t2883.09    \n",
            "89 \t28    \t4428.41\t862.587    \t5042.67    \t2663.08    \n",
            "90 \t26    \t4393.75\t836.919    \t5042.67    \t2567.93    \n",
            "91 \t26    \t4646.03\t555.938    \t5042.67    \t2958.02    \n",
            "92 \t28    \t4618.69\t676.254    \t5042.67    \t2258.11    \n",
            "93 \t29    \t4511.71\t838.953    \t5042.67    \t2772.95    \n",
            "94 \t27    \t4846.01\t321.645    \t5042.67    \t4217.8     \n",
            "95 \t26    \t4973   \t201.993    \t5042.67    \t4217.8     \n",
            "96 \t25    \t5027.99\t8.84986    \t5042.67    \t5022.66    \n",
            "97 \t26    \t4904.35\t480.206    \t5042.67    \t3107.95    \n",
            "98 \t28    \t4954.34\t238.057    \t5042.67    \t4082.55    \n",
            "99 \t28    \t4911.36\t491.309    \t5042.67    \t3073.05    \n",
            "100\t27    \t5026.66\t44.0171    \t5042.67    \t4877.5     \n",
            "Selected features: [False False False False False False False False False False False False\n",
            " False False False False False False False False False False False False\n",
            " False False False False False False False False False False False  True\n",
            " False False False False False False False False False False False False\n",
            " False]\n",
            "Accuracy with selected features: 0.511\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        }
      ],
      "source": [
        "from functools import partial\n",
        "\n",
        "params_list = [{'population_size': 20, 'mutation_probability': 0.5, 'crossover_probability': 0.2},\n",
        "          {'population_size': 25, 'mutation_probability': 0.6, 'crossover_probability': 0.3},\n",
        "          {'population_size': 15, 'mutation_probability': 0.5, 'crossover_probability': 0.4}]\n",
        "\n",
        "scoring_list = ['accuracy',\n",
        "           partial(calculate_score, reward=10, punishment=200),\n",
        "           partial(calculate_score, reward=10, punishment=100)]\n",
        "\n",
        "results = []\n",
        "\n",
        "for params in params_list:\n",
        "  for scoring in scoring_list:\n",
        "    estimator = LogisticRegression()\n",
        "    selector = GAFeatureSelectionCV(\n",
        "        estimator=estimator,\n",
        "        cv=3,\n",
        "        scoring=scoring,\n",
        "        generations=100,\n",
        "        elitism=True,\n",
        "        n_jobs=-1,\n",
        "        verbose=2,\n",
        "        keep_top_k=8,\n",
        "        criteria=\"max\",\n",
        "        **params\n",
        "    )\n",
        "    print(f'Selecting for params: {params} and scoring: {scoring}')\n",
        "    # Fit the genetic algorithm feature selector\n",
        "    selector.fit(X_train, np.ravel(y_train))\n",
        "    best_features = selector.support_\n",
        "\n",
        "    # Transform the dataset to keep only the best features\n",
        "    X_train_selected = selector.transform(X_train)\n",
        "    X_test_selected = selector.transform(X_test)\n",
        "    estimator.fit(X_train_selected, y_train)\n",
        "    accuracy = estimator.score(X_test_selected, y_test)\n",
        "    results.append({'params': params, 'scoring': scoring, 'features': best_features, 'test_accuracy': accuracy,\n",
        "                    'test_score': calculate_score(estimator, X_test_selected, np.ravel(y_test))})\n",
        "\n",
        "    print(\"Selected features:\", best_features)\n",
        "    print(\"Accuracy with selected features:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKyXNQ_FTil0",
        "outputId": "7f9a72c9-31f1-481c-cc4c-15079fb33aab",
        "collapsed": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'params': {'population_size': 20,\n",
              "   'mutation_probability': 0.5,\n",
              "   'crossover_probability': 0.2},\n",
              "  'scoring': 'accuracy',\n",
              "  'features': array([False, False, False, False, False, False, False,  True,  True,\n",
              "          True,  True, False,  True, False,  True,  True, False,  True,\n",
              "         False,  True, False,  True, False,  True, False,  True,  True,\n",
              "          True,  True,  True, False, False,  True,  True,  True,  True,\n",
              "          True, False,  True,  True,  True,  True,  True, False, False,\n",
              "         False,  True,  True, False]),\n",
              "  'test_accuracy': 0.554,\n",
              "  'test_score': -88.97795591182421},\n",
              " {'params': {'population_size': 20,\n",
              "   'mutation_probability': 0.5,\n",
              "   'crossover_probability': 0.2},\n",
              "  'scoring': functools.partial(<function calculate_score at 0x78b445b73e20>, reward=10, punishment=200),\n",
              "  'features': array([ True,  True, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False,  True, False, False, False,\n",
              "         False, False, False, False, False, False, False,  True, False,\n",
              "          True, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False,\n",
              "         False,  True, False, False]),\n",
              "  'test_accuracy': 0.508,\n",
              "  'test_score': 3870.1402805611215},\n",
              " {'params': {'population_size': 20,\n",
              "   'mutation_probability': 0.5,\n",
              "   'crossover_probability': 0.2},\n",
              "  'scoring': functools.partial(<function calculate_score at 0x78b445b73e20>, reward=10, punishment=100),\n",
              "  'features': array([False, False,  True, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False,  True, False,\n",
              "         False, False,  True, False, False,  True, False, False, False,\n",
              "         False, False, False,  True, False, False,  True, False, False,\n",
              "         False,  True, False, False,  True,  True, False, False, False,\n",
              "         False, False, False, False]),\n",
              "  'test_accuracy': 0.543,\n",
              "  'test_score': 3630.861723446893},\n",
              " {'params': {'population_size': 25,\n",
              "   'mutation_probability': 0.6,\n",
              "   'crossover_probability': 0.3},\n",
              "  'scoring': 'accuracy',\n",
              "  'features': array([ True,  True, False, False, False, False, False,  True, False,\n",
              "          True,  True,  True,  True,  True,  True,  True, False,  True,\n",
              "         False,  True, False, False, False, False,  True,  True, False,\n",
              "          True,  True,  True, False,  True, False,  True,  True,  True,\n",
              "          True, False, False,  True,  True, False,  True, False,  True,\n",
              "         False,  True,  True,  True]),\n",
              "  'test_accuracy': 0.545,\n",
              "  'test_score': -349.0981963927861},\n",
              " {'params': {'population_size': 25,\n",
              "   'mutation_probability': 0.6,\n",
              "   'crossover_probability': 0.3},\n",
              "  'scoring': functools.partial(<function calculate_score at 0x78b445b73e20>, reward=10, punishment=200),\n",
              "  'features': array([False, False, False, False, False,  True, False, False, False,\n",
              "         False, False, False,  True, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False,\n",
              "          True, False, False, False, False, False,  True, False, False,\n",
              "         False, False, False, False, False, False, False, False,  True,\n",
              "         False,  True, False,  True]),\n",
              "  'test_accuracy': 0.537,\n",
              "  'test_score': 3990.781563126252},\n",
              " {'params': {'population_size': 25,\n",
              "   'mutation_probability': 0.6,\n",
              "   'crossover_probability': 0.3},\n",
              "  'scoring': functools.partial(<function calculate_score at 0x78b445b73e20>, reward=10, punishment=100),\n",
              "  'features': array([False, False, False,  True, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False,\n",
              "         False,  True, False, False, False,  True, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False,  True, False,\n",
              "         False, False, False, False]),\n",
              "  'test_accuracy': 0.52,\n",
              "  'test_score': 4390.380761523045},\n",
              " {'params': {'population_size': 15,\n",
              "   'mutation_probability': 0.5,\n",
              "   'crossover_probability': 0.4},\n",
              "  'scoring': 'accuracy',\n",
              "  'features': array([False,  True,  True, False,  True,  True, False,  True, False,\n",
              "          True, False, False, False, False,  True,  True,  True,  True,\n",
              "         False,  True,  True, False,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True, False,  True, False,\n",
              "         False, False, False, False,  True, False,  True,  True, False,\n",
              "         False,  True, False,  True]),\n",
              "  'test_accuracy': 0.538,\n",
              "  'test_score': -409.21843687374803},\n",
              " {'params': {'population_size': 15,\n",
              "   'mutation_probability': 0.5,\n",
              "   'crossover_probability': 0.4},\n",
              "  'scoring': functools.partial(<function calculate_score at 0x78b445b73e20>, reward=10, punishment=200),\n",
              "  'features': array([False, False,  True, False, False,  True, False, False, False,\n",
              "         False, False, False,  True, False, False, False,  True, False,\n",
              "         False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False,  True, False, False, False,\n",
              "         False,  True, False, False]),\n",
              "  'test_accuracy': 0.496,\n",
              "  'test_score': 3749.8997995991976},\n",
              " {'params': {'population_size': 15,\n",
              "   'mutation_probability': 0.5,\n",
              "   'crossover_probability': 0.4},\n",
              "  'scoring': functools.partial(<function calculate_score at 0x78b445b73e20>, reward=10, punishment=100),\n",
              "  'features': array([False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False,  True,\n",
              "         False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False]),\n",
              "  'test_accuracy': 0.511,\n",
              "  'test_score': 5110.621242484969}]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwBJKEGWY41Z"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(results).to_csv(\"genetic_results_29052024.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgMl_e4csrLB"
      },
      "source": [
        "# 2nd test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnkeFbsYsyLa",
        "collapsed": true,
        "outputId": "be8542ea-2ec6-453a-b858-15a556e64471"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selecting for params: {'population_size': 20, 'mutation_probability': 0.6, 'crossover_probability': 0.1} and scoring: functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  pid = os.fork()\n",
            "/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  pid = os.fork()\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gen\tnevals\tfitness\tfitness_std\tfitness_max\tfitness_min\n",
            "0  \t20    \t542.189\t482.209    \t1532.94    \t-106.77    \n",
            "1  \t29    \t772.952\t474.518    \t1532.94    \t-72.0179   \n",
            "2  \t24    \t949.444\t436.487    \t1532.94    \t223.115    \n",
            "3  \t27    \t1128.73\t396.084    \t1758.03    \t553.03     \n",
            "4  \t24    \t1449.48\t286.755    \t1758.05    \t778.025    \n",
            "5  \t30    \t1325.45\t488.532    \t1758.05    \t-242.041   \n",
            "6  \t33    \t1466.97\t373.633    \t1758.05    \t92.9998    \n",
            "7  \t26    \t1669.75\t213.759    \t2183.13    \t1338.02    \n",
            "8  \t27    \t1814.75\t338.834    \t2343.04    \t1357.94    \n",
            "9  \t30    \t1614.24\t563.286    \t2343.04    \t318.04     \n",
            "10 \t27    \t1832.99\t446.529    \t2343.04    \t1012.75    \n",
            "11 \t26    \t2022.76\t302.623    \t2392.82    \t1532.94    \n",
            "12 \t30    \t2142.77\t257.012    \t2392.82    \t1532.94    \n",
            "13 \t35    \t1973.5 \t707.674    \t2392.82    \t-297.088   \n",
            "14 \t27    \t2078.95\t534.542    \t2392.82    \t203.042    \n",
            "15 \t33    \t2183.64\t487.604    \t2392.82    \t332.965    \n",
            "16 \t27    \t2382.34\t45.7024    \t2392.82    \t2183.13    \n",
            "17 \t26    \t2320.58\t314.9      \t2392.82    \t947.965    \n",
            "18 \t26    \t2308.83\t366.128    \t2392.82    \t712.914    \n",
            "19 \t27    \t2092.85\t735.281    \t2392.82    \t-246.933   \n",
            "20 \t30    \t1914.1 \t843.615    \t2392.82    \t42.8142    \n",
            "21 \t25    \t2201.58\t578.426    \t2392.82    \t247.849    \n",
            "22 \t27    \t2085.09\t738.72     \t2392.82    \t97.9596    \n",
            "23 \t25    \t2308.08\t369.389    \t2392.82    \t697.952    \n",
            "24 \t29    \t2172.85\t540.584    \t2392.82    \t553.022    \n",
            "25 \t28    \t1955.61\t690.983    \t2392.82    \t553.022    \n",
            "26 \t30    \t2198.11\t404.497    \t2392.82    \t1198.06    \n",
            "27 \t30    \t2392.82\t9.09495e-13\t2392.82    \t2392.82    \n",
            "28 \t22    \t2392.82\t9.09495e-13\t2392.82    \t2392.82    \n",
            "29 \t26    \t2392.82\t9.09495e-13\t2392.82    \t2392.82    \n",
            "30 \t26    \t2392.82\t9.09495e-13\t2392.82    \t2392.82    \n",
            "31 \t33    \t2191.35\t608.428    \t2392.82    \t157.975    \n",
            "32 \t25    \t2287.83\t457.666    \t2392.82    \t292.909    \n",
            "33 \t28    \t2190.83\t613.604    \t2392.82    \t67.907     \n",
            "34 \t27    \t2392.82\t9.09495e-13\t2392.82    \t2392.82    \n",
            "35 \t32    \t2392.82\t9.09495e-13\t2392.82    \t2392.82    \n",
            "36 \t31    \t2158.84\t596.245    \t2392.82    \t237.869    \n",
            "37 \t26    \t2134.35\t536.671    \t2392.82    \t808.1      \n",
            "38 \t29    \t2057.59\t681.326    \t2392.82    \t388.087    \n",
            "39 \t30    \t2135.59\t543.338    \t2392.82    \t613.075    \n",
            "40 \t25    \t2142.09\t607.679    \t2392.82    \t512.892    \n",
            "41 \t28    \t2035.61\t594.044    \t2392.82    \t613.097    \n",
            "42 \t26    \t2267.34\t376.441    \t2392.82    \t1138.02    \n",
            "43 \t28    \t2095.09\t720.269    \t2392.82    \t-47.0305   \n",
            "44 \t32    \t2144.58\t528.388    \t2392.82    \t697.93     \n",
            "45 \t27    \t2292.34\t438.018    \t2392.82    \t383.06     \n",
            "46 \t31    \t1913.89\t742.133    \t2392.82    \t378.07     \n",
            "47 \t25    \t2178.84\t514.217    \t2392.82    \t742.944    \n",
            "48 \t28    \t2166.09\t551.482    \t2392.82    \t487.889    \n",
            "49 \t28    \t2177.08\t529.242    \t2392.82    \t487.889    \n",
            "50 \t22    \t2272.32\t361.504    \t2392.82    \t1187.81    \n",
            "51 \t28    \t2392.82\t9.09495e-13\t2392.82    \t2392.82    \n",
            "52 \t32    \t2206.82\t566.28     \t2392.82    \t227.777    \n",
            "53 \t25    \t2315.08\t338.899    \t2392.82    \t837.847    \n",
            "54 \t24    \t2288.58\t454.398    \t2392.82    \t307.902    \n",
            "55 \t27    \t2336.08\t247.357    \t2392.82    \t1257.87    \n",
            "56 \t31    \t2290.08\t447.839    \t2392.82    \t338        \n",
            "57 \t31    \t2278.08\t500.151    \t2392.82    \t97.9747    \n",
            "58 \t24    \t2203.6 \t569.591    \t2392.82    \t353.052    \n",
            "59 \t31    \t2237.1 \t470.924    \t2392.82    \t648.08     \n",
            "60 \t26    \t2170.61\t532.826    \t2392.82    \t678.208    \n",
            "61 \t28    \t2285.09\t352.048    \t2542.77    \t1112.91    \n",
            "62 \t29    \t2038.09\t771.637    \t2542.77    \t213.007    \n",
            "63 \t29    \t2279.08\t469.634    \t2542.77    \t732.965    \n",
            "64 \t30    \t2392.06\t316.598    \t2542.77    \t1048.15    \n",
            "65 \t26    \t2511.28\t59.3298    \t2542.77    \t2392.82    \n",
            "66 \t32    \t2540.76\t4.02207    \t2542.77    \t2532.72    \n",
            "67 \t28    \t2425.04\t510.9      \t2542.77    \t198.097    \n",
            "68 \t32    \t2234.56\t754.399    \t2542.77    \t137.962    \n",
            "69 \t29    \t2439.53\t450.035    \t2542.77    \t477.872    \n",
            "70 \t32    \t2450.28\t403.169    \t2542.77    \t692.909    \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected features: [ True False False False False False False False False  True False False\n",
            "  True False False False  True False  True  True False False False  True\n",
            "  True False False False  True False False  True  True False  True False\n",
            " False False False False False False False False False False False  True\n",
            " False]\n",
            "Accuracy with selected features: 0.524\n",
            "Selecting for params: {'population_size': 20, 'mutation_probability': 0.6, 'crossover_probability': 0.2} and scoring: functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200)\n",
            "gen\tnevals\tfitness\tfitness_std\tfitness_max\tfitness_min\n",
            "0  \t20    \t425.248\t707.665    \t2042.83    \t-966.958   \n",
            "1  \t30    \t906.76 \t450.587    \t1543.02    \t142.982    \n",
            "2  \t31    \t1219.49\t313.234    \t1563.04    \t683.19     \n",
            "3  \t32    \t1247.99\t340.245    \t1563.04    \t418.042    \n",
            "4  \t34    \t1313   \t335.044    \t1563.04    \t442.829    \n",
            "5  \t33    \t1399.04\t282.005    \t1563.04    \t833.162    \n",
            "6  \t32    \t1397.01\t353.046    \t1563.04    \t237.787    \n",
            "7  \t33    \t1362.79\t439.586    \t1733       \t-216.925   \n",
            "8  \t33    \t1558.04\t294.461    \t2518.02    \t742.937    \n",
            "9  \t27    \t1749.76\t415.59     \t2518.02    \t557.959    \n",
            "10 \t31    \t1893.26\t471.722    \t2518.02    \t407.802    \n",
            "11 \t34    \t2091.53\t385.506    \t2518.02    \t877.887    \n",
            "12 \t31    \t2223.52\t359.01     \t2518.02    \t1112.97    \n",
            "13 \t35    \t2016.99\t833.443    \t2887.95    \t122.97     \n",
            "14 \t35    \t2518.98\t562.535    \t3097.96    \t982.769    \n",
            "15 \t29    \t2733.45\t532.504    \t3097.96    \t982.769    \n",
            "16 \t33    \t2786.19\t583.443    \t3097.96    \t1042.88    \n",
            "17 \t32    \t2819.69\t745.842    \t3242.89    \t-97.0655   \n",
            "18 \t31    \t2867.95\t655.186    \t3242.89    \t568.045    \n",
            "19 \t36    \t3014.94\t565.184    \t3242.89    \t568.045    \n",
            "20 \t32    \t3058.67\t541.505    \t3242.89    \t717.965    \n",
            "21 \t31    \t3122.16\t365.327    \t3242.89    \t1552.99    \n",
            "22 \t37    \t2887.89\t888.734    \t3242.89    \t307.917    \n",
            "23 \t30    \t3019.15\t678.467    \t3242.89    \t692.947    \n",
            "24 \t37    \t3040.15\t627.851    \t3242.89    \t722.985    \n",
            "25 \t35    \t2816.9 \t892.681    \t3242.89    \t472.889    \n",
            "26 \t33    \t2941.9 \t904.386    \t3242.89    \t72.8744    \n",
            "27 \t29    \t3242.89\t9.09495e-13\t3242.89    \t3242.89    \n",
            "28 \t28    \t3134.39\t472.949    \t3242.89    \t1072.85    \n",
            "29 \t26    \t2854.15\t930.52     \t3242.89    \t312.997    \n",
            "30 \t30    \t3102.9 \t610.202    \t3242.89    \t443.09     \n",
            "31 \t31    \t3082.9 \t697.38     \t3242.89    \t43.0897    \n",
            "32 \t29    \t2923.15\t959.382    \t3242.89    \t-11.9051   \n",
            "33 \t34    \t3010.9 \t696.034    \t3242.89    \t892.955    \n",
            "34 \t29    \t3062.91\t784.534    \t3242.89    \t-356.797   \n",
            "35 \t32    \t3137.64\t458.752    \t3242.89    \t1137.99    \n",
            "36 \t33    \t3242.89\t9.09495e-13\t3242.89    \t3242.89    \n",
            "37 \t29    \t2972.65\t820.165    \t3242.89    \t148.01     \n",
            "38 \t36    \t3092.15\t657.062    \t3242.89    \t228.082    \n",
            "39 \t32    \t3136.14\t465.31     \t3242.89    \t1107.9     \n",
            "40 \t31    \t3014.89\t685.548    \t3242.89    \t817.797    \n",
            "41 \t33    \t3136.14\t465.31     \t3242.89    \t1107.9     \n",
            "42 \t33    \t3242.89\t9.09495e-13\t3242.89    \t3242.89    \n",
            "43 \t30    \t3242.89\t9.09495e-13\t3242.89    \t3242.89    \n",
            "44 \t32    \t3096.64\t637.481    \t3242.89    \t317.927    \n",
            "45 \t33    \t2846.4 \t947.192    \t3242.89    \t348.062    \n",
            "46 \t34    \t2960.65\t850.632    \t3242.89    \t163.01     \n",
            "47 \t32    \t3105.14\t600.433    \t3242.89    \t487.912    \n",
            "48 \t37    \t3111.89\t571.018    \t3242.89    \t622.877    \n",
            "49 \t35    \t2974.14\t813.929    \t3242.89    \t202.812    \n",
            "50 \t33    \t2969.89\t818.988    \t3242.89    \t512.929    \n",
            "51 \t32    \t3242.89\t9.09495e-13\t3242.89    \t3242.89    \n",
            "52 \t29    \t3242.89\t9.09495e-13\t3242.89    \t3242.89    \n",
            "53 \t28    \t3242.89\t9.09495e-13\t3242.89    \t3242.89    \n",
            "54 \t33    \t2957.64\t873.729    \t3242.89    \t-167.188   \n",
            "55 \t33    \t2756.9 \t1158.29    \t3242.89    \t-167.188   \n",
            "56 \t36    \t3101.14\t617.891    \t3242.89    \t407.809    \n",
            "57 \t27    \t2963.15\t840.892    \t3242.89    \t277.834    \n",
            "58 \t30    \t3094.64\t646.219    \t3242.89    \t277.834    \n",
            "59 \t34    \t2655.41\t1177.8     \t3242.89    \t-1.89513   \n",
            "60 \t31    \t2880.14\t874.222    \t3242.89    \t442.897    \n",
            "61 \t30    \t3117.65\t545.925    \t3242.89    \t738.015    \n",
            "62 \t33    \t3042.39\t664.073    \t3242.89    \t347.949    \n",
            "63 \t32    \t3241.39\t6.54817    \t3242.89    \t3212.84    \n",
            "64 \t30    \t2890.16\t1068.9     \t3242.89    \t-761.878   \n",
            "65 \t36    \t2952.88\t886.924    \t3242.89    \t-202.216   \n",
            "66 \t31    \t3157.89\t370.49     \t3242.89    \t1542.97    \n",
            "67 \t35    \t3060.4 \t548.9      \t3242.89    \t1292.98    \n",
            "68 \t32    \t2618.4 \t1147.2     \t3242.89    \t-517.086   \n",
            "69 \t27    \t2909.41\t905.998    \t3592.84    \t-276.97    \n",
            "70 \t35    \t2887.67\t967.79     \t3592.84    \t-221.983   \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected features: [False False  True False False False False False False  True  True False\n",
            "  True False False False False False  True False False  True False False\n",
            " False False False False False  True False False False False False False\n",
            " False False False  True False False False False False False False False\n",
            " False]\n",
            "Accuracy with selected features: 0.51\n",
            "Selecting for params: {'population_size': 20, 'mutation_probability': 0.6, 'crossover_probability': 0.3} and scoring: functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200)\n",
            "gen\tnevals\tfitness\tfitness_std\tfitness_max\tfitness_min\n",
            "0  \t20    \t457.947\t533.241    \t1817.83    \t-451.99    \n",
            "1  \t38    \t939.709\t463.595    \t2163.06    \t87.9571    \n",
            "2  \t34    \t1093.19\t366.507    \t2163.06    \t293.007    \n",
            "3  \t34    \t1311.94\t466.535    \t2163.06    \t293.007    \n",
            "4  \t38    \t1457.73\t422.693    \t2163.06    \t727.952    \n",
            "5  \t35    \t1734.72\t299.765    \t2163.06    \t1097.92    \n",
            "6  \t36    \t1834.97\t323.412    \t2163.06    \t553.007    \n",
            "7  \t32    \t1957.22\t128.439    \t2163.06    \t1578.07    \n",
            "8  \t36    \t1864.22\t464.612    \t2287.9     \t322.992    \n",
            "9  \t38    \t1933.95\t531.184    \t2112.91    \t-356.858   \n",
            "10 \t36    \t1783.69\t615.434    \t2107.92    \t32.8042    \n",
            "11 \t34    \t1878.93\t553.231    \t2107.92    \t62.8418    \n",
            "12 \t37    \t2109.16\t178.051    \t2587.77    \t1492.91    \n",
            "13 \t38    \t2073.4 \t306.181    \t2457.83    \t987.827    \n",
            "14 \t40    \t1782.94\t643.854    \t2457.83    \t112.982    \n",
            "15 \t37    \t1969.91\t523.614    \t2307.94    \t472.927    \n",
            "16 \t35    \t1858.41\t535.712    \t2457.82    \t472.927    \n",
            "17 \t37    \t2117.65\t335.188    \t2457.82    \t852.869    \n",
            "18 \t36    \t2130.17\t454.952    \t2457.82    \t213.105    \n",
            "19 \t33    \t2215.88\t474.833    \t2457.82    \t213.105    \n",
            "20 \t37    \t2259.12\t490.307    \t2487.86    \t193.047    \n",
            "21 \t38    \t2342.09\t468.618    \t2642.76    \t343.065    \n",
            "22 \t35    \t2254.1 \t624.262    \t2642.76    \t-1.87255   \n",
            "23 \t35    \t2404.56\t408.531    \t2642.76    \t652.862    \n",
            "24 \t39    \t2441.31\t295.009    \t2642.76    \t1202.81    \n",
            "25 \t38    \t2531.8 \t90.6016    \t2642.76    \t2457.82    \n",
            "26 \t37    \t2417.81\t479.359    \t2642.76    \t362.995    \n",
            "27 \t38    \t2485.05\t481.906    \t2642.76    \t413.158    \n",
            "28 \t38    \t2537.53\t258.997    \t2642.76    \t1462.92    \n",
            "29 \t38    \t2240.57\t700.997    \t2642.76    \t278.155    \n",
            "30 \t36    \t2442.57\t414.377    \t2712.99    \t1158.09    \n",
            "31 \t36    \t2408.05\t624.92     \t2642.76    \t547.972    \n",
            "32 \t37    \t2643.02\t1.11378    \t2647.87    \t2642.76    \n",
            "33 \t37    \t2642.76\t9.09495e-13\t2642.76    \t2642.76    \n",
            "34 \t38    \t2525.02\t513.218    \t2642.76    \t287.957    \n",
            "35 \t29    \t2554.77\t383.561    \t2642.76    \t882.862    \n",
            "36 \t37    \t2642.76\t9.09495e-13\t2642.76    \t2642.76    \n",
            "37 \t36    \t2519.28\t538.253    \t2642.76    \t173.087    \n",
            "38 \t36    \t2627.31\t262.854    \t2862.86    \t1533.06    \n",
            "39 \t39    \t2747.38\t148.839    \t3042.86    \t2287.89    \n",
            "40 \t35    \t2624.89\t771.088    \t3247.89    \t-386.94    \n",
            "41 \t39    \t2628.88\t830.705    \t3247.89    \t-386.925   \n",
            "42 \t35    \t2880.89\t68.7821    \t3042.86    \t2697.93    \n",
            "43 \t39    \t2803.39\t568.329    \t3122.97    \t357.982    \n",
            "44 \t33    \t2868.87\t535.531    \t3202.78    \t597.777    \n",
            "45 \t39    \t2907.86\t463.535    \t3397.72    \t1228.14    \n",
            "46 \t38    \t2785.84\t844.931    \t3397.72    \t162.995    \n",
            "47 \t38    \t2970.8 \t722.312    \t3397.72    \t532.995    \n",
            "48 \t35    \t3232.25\t169.643    \t3397.72    \t2837.82    \n",
            "49 \t35    \t2976.51\t1041.7     \t3452.6     \t-256.98    \n",
            "50 \t36    \t2942.53\t922.066    \t3452.6     \t467.945    \n",
            "51 \t36    \t3113.26\t749.215    \t3582.72    \t327.922    \n",
            "52 \t35    \t3086.02\t932.449    \t3642.87    \t447.857    \n",
            "53 \t35    \t3200.06\t711.527    \t3642.87    \t418.117    \n",
            "54 \t36    \t3372.82\t636.46     \t3642.87    \t637.832    \n",
            "55 \t39    \t3241.12\t927.276    \t3642.87    \t313.212    \n",
            "56 \t35    \t3420.84\t467.415    \t3642.87    \t1617.85    \n",
            "57 \t35    \t3429.86\t652.859    \t3642.87    \t613.075    \n",
            "58 \t35    \t3155.12\t966.715    \t3642.87    \t577.972    \n",
            "59 \t34    \t3330.63\t937.656    \t3642.87    \t388.065    \n",
            "60 \t35    \t3500.88\t433.524    \t3642.87    \t1968.05    \n",
            "61 \t37    \t3642.87\t9.09495e-13\t3642.87    \t3642.87    \n",
            "62 \t37    \t2865.66\t1584.93    \t3642.87    \t-926.797   \n",
            "63 \t35    \t3262.63\t932.583    \t3642.87    \t697.96     \n",
            "64 \t33    \t3603.87\t169.998    \t3642.87    \t2862.86    \n",
            "65 \t34    \t3274.38\t1114.19    \t3642.87    \t-482.058   \n",
            "66 \t37    \t3355.89\t860.927    \t3642.87    \t773.11     \n",
            "67 \t36    \t2845.38\t1426.66    \t3642.87    \t-662.046   \n",
            "68 \t33    \t2788.9 \t1326.47    \t3642.87    \t92.9471    \n",
            "69 \t35    \t3178.87\t946.128    \t3642.87    \t707.909    \n",
            "70 \t38    \t3458.37\t671.947    \t3642.87    \t593.062    \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected features: [False False False False False False False  True False False  True  True\n",
            " False False False False False False False False False False False False\n",
            " False False False False  True False False False False False False  True\n",
            "  True  True False False  True False False False False False False False\n",
            " False]\n",
            "Accuracy with selected features: 0.522\n",
            "Selecting for params: {'population_size': 20, 'mutation_probability': 0.5, 'crossover_probability': 0.1} and scoring: functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200)\n",
            "gen\tnevals\tfitness\tfitness_std\tfitness_max\tfitness_min\n",
            "0  \t20    \t370.736\t775.25     \t1582.78    \t-1322.11   \n",
            "1  \t22    \t896.704\t513.64     \t1582.78    \t187.774    \n",
            "2  \t21    \t1217.47\t445.008    \t1732.99    \t393.055    \n",
            "3  \t24    \t1504.21\t229.952    \t1897.9     \t727.982    \n",
            "4  \t27    \t1494.95\t139.201    \t1732.99    \t1247.84    \n",
            "5  \t29    \t1553.41\t234.683    \t1958.07    \t993.055    \n",
            "6  \t19    \t1627.4 \t231.371    \t1958.07    \t1117.92    \n",
            "7  \t32    \t1779.2 \t190.312    \t2107.94    \t1462.88    \n",
            "8  \t24    \t1793.94\t258.991    \t2107.94    \t1122.98    \n",
            "9  \t22    \t1786.23\t438.664    \t2107.94    \t363.062    \n",
            "10 \t26    \t1853.44\t314.59     \t2107.94    \t992.749    \n",
            "11 \t28    \t2017.96\t158.422    \t2107.94    \t1582.78    \n",
            "12 \t25    \t2012.96\t315.402    \t2107.94    \t657.897    \n",
            "13 \t24    \t2107.94\t4.54747e-13\t2107.94    \t2107.94    \n",
            "14 \t25    \t2049.19\t256.07     \t2107.94    \t933.01     \n",
            "15 \t24    \t2107.94\t4.54747e-13\t2107.94    \t2107.94    \n",
            "16 \t21    \t2107.94\t4.54747e-13\t2107.94    \t2107.94    \n",
            "17 \t23    \t2036.94\t309.493    \t2107.94    \t687.889    \n",
            "18 \t24    \t1982.69\t380.412    \t2107.94    \t667.915    \n",
            "19 \t23    \t2107.94\t4.54747e-13\t2107.94    \t2107.94    \n",
            "20 \t26    \t2107.94\t4.54747e-13\t2107.94    \t2107.94    \n",
            "21 \t28    \t2102.2 \t25.0353    \t2107.94    \t1993.07    \n",
            "22 \t25    \t2107.94\t4.54747e-13\t2107.94    \t2107.94    \n",
            "23 \t25    \t2107.94\t4.54747e-13\t2107.94    \t2107.94    \n",
            "24 \t23    \t2107.94\t4.54747e-13\t2107.94    \t2107.94    \n",
            "25 \t23    \t2019.19\t386.847    \t2107.94    \t332.965    \n",
            "26 \t19    \t2107.94\t4.54747e-13\t2107.94    \t2107.94    \n",
            "27 \t24    \t2061.19\t203.763    \t2107.94    \t1173.01    \n",
            "28 \t25    \t1967.45\t343.828    \t2107.94    \t917.965    \n",
            "29 \t26    \t1876.7 \t586.329    \t2107.94    \t168.007    \n",
            "30 \t20    \t2067.69\t175.445    \t2107.94    \t1302.94    \n",
            "31 \t27    \t2033.19\t362.739    \t2472.95    \t968.045    \n",
            "32 \t25    \t2180.94\t146.004    \t2472.95    \t2107.94    \n",
            "33 \t23    \t2096.69\t435.386    \t2472.95    \t702.897    \n",
            "34 \t24    \t2250.9 \t259.719    \t2472.95    \t1372.8     \n",
            "35 \t22    \t2408.39\t114.417    \t2472.95    \t2107.94    \n",
            "36 \t26    \t2366.45\t388.688    \t2472.95    \t707.962    \n",
            "37 \t28    \t2372.69\t437.007    \t2472.95    \t467.824    \n",
            "38 \t22    \t2296.7 \t532.439    \t2472.95    \t512.907    \n",
            "39 \t23    \t2380.45\t277.487    \t2472.95    \t1547.99    \n",
            "40 \t20    \t2437.95\t264.507    \t2697.99    \t1322.95    \n",
            "41 \t26    \t2423.21\t484.658    \t2697.99    \t353.015    \n",
            "42 \t22    \t2516.98\t391.252    \t2697.99    \t878.185    \n",
            "43 \t29    \t2577.23\t379.996    \t2697.99    \t957.975    \n",
            "44 \t24    \t2345.98\t682.516    \t2697.99    \t732.942    \n",
            "45 \t26    \t2358.74\t682.828    \t2697.99    \t762.987    \n",
            "46 \t20    \t2456.23\t582.281    \t2697.99    \t762.987    \n",
            "47 \t19    \t2697.99\t0          \t2697.99    \t2697.99    \n",
            "48 \t25    \t2697.99\t0          \t2697.99    \t2697.99    \n",
            "49 \t24    \t2697.99\t0          \t2697.99    \t2697.99    \n",
            "50 \t24    \t2697.99\t0          \t2697.99    \t2697.99    \n",
            "51 \t22    \t2612.74\t371.579    \t2697.99    \t993.07     \n",
            "52 \t28    \t2638.74\t258.275    \t2697.99    \t1512.94    \n",
            "53 \t21    \t2638.74\t258.275    \t2697.99    \t1512.94    \n",
            "54 \t23    \t2624.73\t319.325    \t2697.99    \t1232.83    \n",
            "55 \t20    \t2624.73\t319.325    \t2697.99    \t1232.83    \n",
            "56 \t25    \t2622.25\t330.161    \t2697.99    \t1183.1     \n",
            "57 \t16    \t2697.99\t0          \t2697.99    \t2697.99    \n",
            "58 \t23    \t2697.99\t0          \t2697.99    \t2697.99    \n",
            "59 \t24    \t2697.99\t0          \t2697.99    \t2697.99    \n",
            "60 \t24    \t2627.25\t308.329    \t2697.99    \t1283.28    \n",
            "61 \t26    \t2626.48\t311.684    \t2697.99    \t1267.88    \n",
            "62 \t25    \t2547.99\t450.557    \t2697.99    \t1128       \n",
            "63 \t24    \t2619.49\t342.17     \t2697.99    \t1128       \n",
            "64 \t20    \t2581.74\t506.73     \t2697.99    \t372.952    \n",
            "65 \t28    \t2697.99\t0          \t2697.99    \t2697.99    \n",
            "66 \t27    \t2607.74\t393.392    \t2697.99    \t892.985    \n",
            "67 \t27    \t2563.99\t402.005    \t2697.99    \t1357.97    \n",
            "68 \t26    \t2511.99\t566.816    \t2697.99    \t523.03     \n",
            "69 \t27    \t2636.73\t267.009    \t2697.99    \t1472.87    \n",
            "70 \t21    \t2636.73\t267.009    \t2697.99    \t1472.87    \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected features: [ True False  True  True  True False False  True False  True False False\n",
            " False False False False  True False False False False  True False False\n",
            " False False False False False False  True False  True False  True False\n",
            " False False False False False  True  True False False False False False\n",
            " False]\n",
            "Accuracy with selected features: 0.524\n",
            "Selecting for params: {'population_size': 20, 'mutation_probability': 0.5, 'crossover_probability': 0.2} and scoring: functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200)\n",
            "gen\tnevals\tfitness\tfitness_std\tfitness_max\tfitness_min\n",
            "0  \t20    \t526.447\t808.663    \t2417.8     \t-1052.07   \n",
            "1  \t28    \t1167.42\t605.866    \t2417.8     \t-141.812   \n",
            "2  \t28    \t1705.39\t522.665    \t2417.8     \t892.925    \n",
            "3  \t27    \t1827.44\t451.218    \t2417.8     \t847.842    \n",
            "4  \t23    \t2069.63\t334.372    \t2652.84    \t1632.77    \n",
            "5  \t27    \t1991.4 \t652.089    \t2652.84    \t358.08     \n",
            "6  \t29    \t2377.61\t546.422    \t2652.84    \t132.98     \n",
            "7  \t25    \t2617.58\t83.927     \t2652.84    \t2417.8     \n",
            "8  \t30    \t2289.1 \t881.765    \t2652.84    \t-342.058   \n",
            "9  \t36    \t2402.37\t527.917    \t2652.84    \t848.125    \n",
            "10 \t29    \t2652.84\t4.54747e-13\t2652.84    \t2652.84    \n",
            "11 \t33    \t2603.34\t215.749    \t2652.84    \t1662.92    \n",
            "12 \t26    \t2652.84\t4.54747e-13\t2652.84    \t2652.84    \n",
            "13 \t28    \t2606.34\t202.667    \t2652.84    \t1722.94    \n",
            "14 \t26    \t2606.34\t202.667    \t2652.84    \t1722.94    \n",
            "15 \t30    \t2652.84\t4.54747e-13\t2652.84    \t2652.84    \n",
            "16 \t28    \t2586.59\t288.754    \t2652.84    \t1327.94    \n",
            "17 \t29    \t2405.61\t594.792    \t2652.84    \t437.862    \n",
            "18 \t32    \t2477.85\t532.639    \t2652.84    \t617.895    \n",
            "19 \t31    \t2483.11\t511.473    \t2652.84    \t803.042    \n",
            "20 \t27    \t2611.34\t180.902    \t2652.84    \t1822.8     \n",
            "21 \t23    \t2652.84\t4.54747e-13\t2652.84    \t2652.84    \n",
            "22 \t30    \t2557.1 \t417.321    \t2652.84    \t738.037    \n",
            "23 \t27    \t2383.37\t651.641    \t2652.84    \t503.195    \n",
            "24 \t32    \t2500.37\t458.515    \t2652.84    \t1028.05    \n",
            "25 \t30    \t2388.37\t663.815    \t2652.84    \t122.97     \n",
            "26 \t26    \t2496.11\t482.145    \t2652.84    \t748.017    \n",
            "27 \t28    \t2380.6 \t655.331    \t2652.84    \t482.914    \n",
            "28 \t29    \t2570.1 \t360.657    \t2652.84    \t998.03     \n",
            "29 \t28    \t2404.86\t744.077    \t2652.84    \t127.967    \n",
            "30 \t29    \t2447.1 \t655.971    \t2652.84    \t-107.023   \n",
            "31 \t34    \t2571.84\t276.927    \t2652.84    \t1422.82    \n",
            "32 \t28    \t2437.35\t633.432    \t2652.84    \t-211.943   \n",
            "33 \t26    \t2350.1 \t547.93     \t2652.84    \t1022.78    \n",
            "34 \t26    \t2454.58\t426.687    \t2782.74    \t1333.07    \n",
            "35 \t27    \t2518.09\t487.908    \t2782.74    \t492.97     \n",
            "36 \t26    \t2665.83\t109.942    \t2892.85    \t2282.85    \n",
            "37 \t29    \t2748.29\t77.0145    \t2892.85    \t2652.84    \n",
            "38 \t29    \t2712.28\t384.573    \t2912.89    \t1062.85    \n",
            "39 \t29    \t2721.57\t503.343    \t2912.89    \t547.972    \n",
            "40 \t31    \t2826.34\t242.984    \t3037.79    \t1798.05    \n",
            "41 \t27    \t2899.6 \t41.369     \t3037.79    \t2782.74    \n",
            "42 \t26    \t2914.36\t42.1742    \t3037.79    \t2892.85    \n",
            "43 \t28    \t2942.84\t63.8229    \t3047.81    \t2892.85    \n",
            "44 \t29    \t2869.09\t402.476    \t3037.79    \t1137.92    \n",
            "45 \t28    \t2968.07\t117.704    \t3037.79    \t2652.89    \n",
            "46 \t26    \t3019.05\t44.5964    \t3037.79    \t2912.89    \n",
            "47 \t30    \t3032.04\t27.4221    \t3047.81    \t2912.89    \n",
            "48 \t30    \t3047.54\t35.7845    \t3202.74    \t3037.79    \n",
            "49 \t32    \t3041.8 \t49.244     \t3202.74    \t2892.85    \n",
            "50 \t30    \t2922.32\t494.312    \t3202.74    \t783.097    \n",
            "51 \t23    \t3043.3 \t49.2517    \t3202.74    \t2892.85    \n",
            "52 \t27    \t3100.53\t75.0788    \t3202.74    \t3037.79    \n",
            "53 \t29    \t3042.78\t382.697    \t3202.74    \t1407.84    \n",
            "54 \t29    \t3163.51\t67.9855    \t3202.74    \t3037.79    \n",
            "55 \t27    \t3178.5 \t57.7374    \t3202.74    \t3037.79    \n",
            "56 \t27    \t3202.74\t1.36424e-12\t3202.74    \t3202.74    \n",
            "57 \t28    \t3202.74\t1.36424e-12\t3202.74    \t3202.74    \n",
            "58 \t29    \t3136.25\t289.835    \t3202.74    \t1872.89    \n",
            "59 \t26    \t3202.74\t1.36424e-12\t3202.74    \t3202.74    \n",
            "60 \t24    \t3202.74\t1.36424e-12\t3202.74    \t3202.74    \n",
            "61 \t30    \t3202.74\t1.36424e-12\t3202.74    \t3202.74    \n",
            "62 \t28    \t3008.76\t581.959    \t3202.74    \t1262.88    \n",
            "63 \t30    \t3202.74\t1.36424e-12\t3202.74    \t3202.74    \n",
            "64 \t27    \t3202.74\t1.36424e-12\t3202.74    \t3202.74    \n",
            "65 \t31    \t3202.74\t1.36424e-12\t3202.74    \t3202.74    \n",
            "66 \t26    \t3104   \t430.424    \t3202.74    \t1227.82    \n",
            "67 \t30    \t3070.76\t395.94     \t3202.74    \t1882.94    \n",
            "68 \t33    \t3099.26\t451.083    \t3202.74    \t1133.03    \n",
            "69 \t27    \t3099.26\t451.083    \t3202.74    \t1133.03    \n",
            "70 \t28    \t3028.76\t532.273    \t3202.74    \t1133.03    \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected features: [ True False False False  True  True False  True False False  True False\n",
            " False False False  True False False False False False False False  True\n",
            " False False  True  True False False False False False False False False\n",
            " False False False False False False False False False False False False\n",
            "  True]\n",
            "Accuracy with selected features: 0.533\n",
            "Selecting for params: {'population_size': 20, 'mutation_probability': 0.5, 'crossover_probability': 0.3} and scoring: functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200)\n",
            "gen\tnevals\tfitness\tfitness_std\tfitness_max\tfitness_min\n",
            "0  \t20    \t749.942\t758.891    \t2612.73    \t-467.051   \n",
            "1  \t33    \t1257.65\t593.945    \t2612.73    \t492.932    \n",
            "2  \t32    \t1679.42\t493.122    \t2612.73    \t1017.76    \n",
            "3  \t34    \t1915.72\t490.994    \t2612.73    \t1112.98    \n",
            "4  \t29    \t2376.63\t491.4      \t3203.03    \t1148.02    \n",
            "5  \t29    \t2643.66\t411.705    \t3203.03    \t2112.97    \n",
            "6  \t30    \t2786.41\t498.092    \t3203.03    \t1497.97    \n",
            "7  \t28    \t2949.93\t417.038    \t3257.87    \t1508.01    \n",
            "8  \t33    \t2891.21\t724.753    \t3482.91    \t958.133    \n",
            "9  \t35    \t2930.21\t787.036    \t3332.97    \t867.915    \n",
            "10 \t31    \t3223.96\t320.205    \t3343.11    \t1842.84    \n",
            "11 \t34    \t3382.48\t112.956    \t3662.85    \t3203.03    \n",
            "12 \t34    \t3138.46\t831.912    \t3662.85    \t907.909    \n",
            "13 \t30    \t3344.99\t665.153    \t3862.91    \t512.975    \n",
            "14 \t32    \t3312.97\t788.953    \t3862.91    \t177.995    \n",
            "15 \t33    \t3368.72\t741.413    \t3862.91    \t687.904    \n",
            "16 \t29    \t3565.44\t588.783    \t3862.91    \t1092.91    \n",
            "17 \t33    \t3510.15\t924.222    \t3862.91    \t92.9471    \n",
            "18 \t34    \t3862.91\t9.09495e-13\t3862.91    \t3862.91    \n",
            "19 \t33    \t3579.65\t883.313    \t3862.91    \t267.907    \n",
            "20 \t33    \t3407.16\t1109.77    \t3862.91    \t267.907    \n",
            "21 \t34    \t3661.92\t662.118    \t3862.91    \t987.997    \n",
            "22 \t28    \t3855.16\t33.7782    \t3862.91    \t3707.92    \n",
            "23 \t30    \t3862.91\t9.09495e-13\t3862.91    \t3862.91    \n",
            "24 \t31    \t3715.66\t641.842    \t3862.91    \t917.934    \n",
            "25 \t26    \t3862.91\t9.09495e-13\t3862.91    \t3862.91    \n",
            "26 \t28    \t3862.91\t9.09495e-13\t3862.91    \t3862.91    \n",
            "27 \t33    \t3407.66\t1092.87    \t3862.91    \t548.025    \n",
            "28 \t35    \t3213.15\t1173.23    \t3862.91    \t292.917    \n",
            "29 \t34    \t3613.91\t763.886    \t3862.91    \t867.869    \n",
            "30 \t31    \t3763.66\t432.621    \t3862.91    \t1877.91    \n",
            "31 \t29    \t3709.42\t669.063    \t3862.91    \t793.04     \n",
            "32 \t32    \t3673.91\t823.836    \t3862.91    \t82.8919    \n",
            "33 \t36    \t3579.42\t863.911    \t3862.91    \t548.04     \n",
            "34 \t28    \t3862.91\t9.09495e-13\t3862.91    \t3862.91    \n",
            "35 \t35    \t3735.91\t553.569    \t3862.91    \t1322.96    \n",
            "36 \t31    \t3862.91\t9.09495e-13\t3862.91    \t3862.91    \n",
            "37 \t28    \t3862.91\t9.09495e-13\t3862.91    \t3862.91    \n",
            "38 \t31    \t3713.15\t652.765    \t3862.91    \t867.817    \n",
            "39 \t30    \t3427.93\t1035.56    \t3862.91    \t912.96     \n",
            "40 \t33    \t3719.17\t626.561    \t3862.91    \t988.05     \n",
            "41 \t32    \t3862.91\t9.09495e-13\t3862.91    \t3862.91    \n",
            "42 \t34    \t3415.67\t1095.02    \t3862.91    \t-21.9829   \n",
            "43 \t38    \t3538.68\t847.442    \t3862.91    \t778.062    \n",
            "44 \t33    \t3818.16\t195.039    \t3862.91    \t2968.01    \n",
            "45 \t31    \t3783.41\t346.534    \t3862.91    \t2272.9     \n",
            "46 \t29    \t3862.91\t9.09495e-13\t3862.91    \t3862.91    \n",
            "47 \t32    \t3740.17\t535.013    \t3862.91    \t1408.1     \n",
            "48 \t31    \t3693.91\t736.638    \t3862.91    \t482.982    \n",
            "49 \t33    \t3862.91\t9.09495e-13\t3862.91    \t3862.91    \n",
            "50 \t29    \t3862.91\t9.09495e-13\t3862.91    \t3862.91    \n",
            "51 \t31    \t3750.16\t491.475    \t3862.91    \t1607.87    \n",
            "52 \t36    \t3733.91\t562.29     \t3862.91    \t1282.94    \n",
            "53 \t35    \t3862.91\t9.09495e-13\t3862.91    \t3862.91    \n",
            "54 \t28    \t3767.41\t416.257    \t3862.91    \t1952.99    \n",
            "55 \t33    \t3754.41\t472.939    \t3862.91    \t1692.92    \n",
            "56 \t29    \t3454.91\t1019.01    \t3862.91    \t538.067    \n",
            "57 \t28    \t2976.9 \t1260.97    \t3862.91    \t477.962    \n",
            "58 \t30    \t3518.67\t854.544    \t3862.91    \t687.935    \n",
            "59 \t29    \t3862.91\t9.09495e-13\t3862.91    \t3862.91    \n",
            "60 \t25    \t3862.91\t9.09495e-13\t3862.91    \t3862.91    \n",
            "61 \t32    \t3702.17\t700.626    \t3862.91    \t648.215    \n",
            "62 \t33    \t3862.91\t9.09495e-13\t3862.91    \t3862.91    \n",
            "63 \t33    \t3579.67\t863.604    \t3862.91    \t542.96     \n",
            "64 \t33    \t3862.91\t9.09495e-13\t3862.91    \t3862.91    \n",
            "65 \t31    \t3541.65\t963.842    \t3862.91    \t612.792    \n",
            "66 \t34    \t3862.91\t9.09495e-13\t3862.91    \t3862.91    \n",
            "67 \t28    \t3696.16\t726.842    \t3862.91    \t527.929    \n",
            "68 \t30    \t3862.91\t9.09495e-13\t3862.91    \t3862.91    \n",
            "69 \t33    \t3862.91\t9.09495e-13\t3862.91    \t3862.91    \n",
            "70 \t32    \t3862.91\t9.09495e-13\t3862.91    \t3862.91    \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected features: [False False False False  True False False False False  True False False\n",
            " False  True False False False False False False False False  True False\n",
            " False False False False False False False False False  True False False\n",
            " False False False False  True False False False False False  True False\n",
            " False]\n",
            "Accuracy with selected features: 0.541\n",
            "Selecting for params: {'population_size': 20, 'mutation_probability': 0.4, 'crossover_probability': 0.1} and scoring: functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200)\n",
            "gen\tnevals\tfitness\tfitness_std\tfitness_max\tfitness_min\n",
            "0  \t20    \t390.473\t437.641    \t1337.98    \t-682.088   \n",
            "1  \t21    \t800.443\t419.732    \t1702.97    \t213.097    \n",
            "2  \t18    \t1055.92\t395.454    \t1702.97    \t303.01     \n",
            "3  \t17    \t1313.37\t236.214    \t1702.97    \t1067.86    \n",
            "4  \t19    \t1565.4 \t228.271    \t2232.86    \t1203.09    \n",
            "5  \t18    \t1787.9 \t310.389    \t2232.86    \t1382.76    \n",
            "6  \t16    \t2055.16\t281.007    \t2523.02    \t1568.06    \n",
            "7  \t21    \t2218.15\t220.442    \t2523.02    \t1568.06    \n",
            "8  \t19    \t2350.43\t141.072    \t2523.02    \t2232.86    \n",
            "9  \t18    \t2384.21\t194.813    \t2523.02    \t1778.02    \n",
            "10 \t19    \t2422.98\t317.678    \t2652.88    \t1132.97    \n",
            "11 \t23    \t2466   \t371.316    \t2652.88    \t863.177    \n",
            "12 \t15    \t2581.46\t64.6056    \t2652.88    \t2523.02    \n",
            "13 \t19    \t2626.91\t51.9449    \t2652.88    \t2523.02    \n",
            "14 \t20    \t2652.88\t4.54747e-13\t2652.88    \t2652.88    \n",
            "15 \t18    \t2603.14\t216.852    \t2652.88    \t1657.9     \n",
            "16 \t21    \t2615.14\t225.736    \t2892.94    \t1657.9     \n",
            "17 \t18    \t2676.89\t72.0165    \t2892.94    \t2652.88    \n",
            "18 \t21    \t2724.9 \t110.007    \t2892.94    \t2652.88    \n",
            "19 \t17    \t2796.92\t117.602    \t2892.94    \t2652.88    \n",
            "20 \t18    \t2793.69\t381.151    \t2892.94    \t1147.97    \n",
            "21 \t20    \t2801.94\t396.671    \t2892.94    \t1072.89    \n",
            "22 \t21    \t2739.45\t468.919    \t2892.94    \t1077.91    \n",
            "23 \t17    \t2854.93\t165.694    \t2892.94    \t2132.68    \n",
            "24 \t18    \t2892.94\t9.09495e-13\t2892.94    \t2892.94    \n",
            "25 \t20    \t2892.94\t9.09495e-13\t2892.94    \t2892.94    \n",
            "26 \t21    \t2928.92\t71.9649    \t3072.85    \t2892.94    \n",
            "27 \t18    \t2919.93\t64.2415    \t3072.85    \t2892.94    \n",
            "28 \t20    \t2928.92\t71.9649    \t3072.85    \t2892.94    \n",
            "29 \t18    \t3060.91\t215.573    \t3462.92    \t2892.94    \n",
            "30 \t20    \t2936.89\t320.534    \t3462.92    \t1667.92    \n",
            "31 \t19    \t3068.39\t184.748    \t3462.92    \t2892.94    \n",
            "32 \t22    \t3209.39\t213.346    \t3462.92    \t2892.94    \n",
            "33 \t15    \t3315.64\t184.052    \t3462.92    \t3072.85    \n",
            "34 \t16    \t3345.9 \t178.75     \t3462.92    \t3072.85    \n",
            "35 \t19    \t3365.4 \t168.903    \t3462.92    \t3072.85    \n",
            "36 \t19    \t3423.91\t117.02     \t3462.92    \t3072.85    \n",
            "37 \t20    \t3462.92\t4.54747e-13\t3462.92    \t3462.92    \n",
            "38 \t16    \t3462.92\t4.54747e-13\t3462.92    \t3462.92    \n",
            "39 \t18    \t3462.92\t4.54747e-13\t3462.92    \t3462.92    \n",
            "40 \t19    \t3462.92\t4.54747e-13\t3462.92    \t3462.92    \n",
            "41 \t18    \t3462.92\t4.54747e-13\t3462.92    \t3462.92    \n",
            "42 \t18    \t3462.92\t4.54747e-13\t3462.92    \t3462.92    \n",
            "43 \t23    \t3462.92\t4.54747e-13\t3462.92    \t3462.92    \n",
            "44 \t28    \t3374.17\t386.852    \t3462.92    \t1687.92    \n",
            "45 \t14    \t3462.92\t4.54747e-13\t3462.92    \t3462.92    \n",
            "46 \t16    \t3462.92\t4.54747e-13\t3462.92    \t3462.92    \n",
            "47 \t23    \t3218.92\t732.803    \t3462.92    \t912.96     \n",
            "48 \t20    \t3340.91\t531.808    \t3462.92    \t1022.82    \n",
            "49 \t17    \t3391.41\t311.679    \t3462.92    \t2032.83    \n",
            "50 \t20    \t3236.67\t683.8      \t3462.92    \t938.037    \n",
            "51 \t21    \t3276.41\t452.719    \t3462.92    \t2057.82    \n",
            "52 \t26    \t3103.69\t723.401    \t3462.92    \t1467.9     \n",
            "53 \t24    \t3270.43\t587.101    \t3462.92    \t1203.11    \n",
            "54 \t23    \t3044.42\t852.034    \t3462.92    \t907.985    \n",
            "55 \t16    \t3284.92\t537.369    \t3462.92    \t1492.91    \n",
            "56 \t21    \t3378.41\t368.373    \t3462.92    \t1772.71    \n",
            "57 \t23    \t3462.92\t4.54747e-13\t3462.92    \t3462.92    \n",
            "58 \t21    \t3368.91\t409.767    \t3462.92    \t1582.78    \n",
            "59 \t17    \t3376.92\t374.845    \t3462.92    \t1743.01    \n",
            "60 \t18    \t3386.67\t332.353    \t3462.92    \t1937.98    \n",
            "61 \t21    \t3317.42\t634.208    \t3462.92    \t552.97     \n",
            "62 \t19    \t3462.92\t4.54747e-13\t3462.92    \t3462.92    \n",
            "63 \t21    \t3462.92\t4.54747e-13\t3462.92    \t3462.92    \n",
            "64 \t18    \t3462.92\t4.54747e-13\t3462.92    \t3462.92    \n",
            "65 \t21    \t3462.92\t4.54747e-13\t3462.92    \t3462.92    \n",
            "66 \t22    \t3176.66\t698.861    \t3462.92    \t988.065    \n",
            "67 \t18    \t3352.41\t481.669    \t3462.92    \t1252.87    \n",
            "68 \t20    \t3462.92\t4.54747e-13\t3462.92    \t3462.92    \n",
            "69 \t19    \t3299.92\t514.394    \t3462.92    \t1327.99    \n",
            "70 \t24    \t3345.17\t513.246    \t3462.92    \t1107.98    \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected features: [ True False False False False False False False False  True False False\n",
            " False False False False False False False False False  True False False\n",
            " False  True  True False  True False False False False False False False\n",
            " False False False False False False False False False  True  True False\n",
            " False]\n",
            "Accuracy with selected features: 0.489\n",
            "Selecting for params: {'population_size': 20, 'mutation_probability': 0.4, 'crossover_probability': 0.2} and scoring: functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200)\n",
            "gen\tnevals\tfitness\tfitness_std\tfitness_max\tfitness_min\n",
            "0  \t20    \t176.265\t419.613    \t908.007    \t-716.825   \n",
            "1  \t32    \t662.987\t291.571    \t1192.97    \t82.9747    \n",
            "2  \t22    \t1035.21\t347.425    \t1542.96    \t427.852    \n",
            "3  \t24    \t1469.48\t149.785    \t1747.95    \t1092.91    \n",
            "4  \t28    \t1425   \t239.655    \t1747.95    \t583.097    \n",
            "5  \t20    \t1678.46\t322.012    \t2497.89    \t1192.97    \n",
            "6  \t26    \t1889.67\t407.915    \t2497.89    \t1542.96    \n",
            "7  \t22    \t2218.4 \t478.126    \t2842.84    \t1542.96    \n",
            "8  \t27    \t2536.13\t417.617    \t2857.87    \t1423.08    \n",
            "9  \t24    \t2628.11\t414.684    \t2857.87    \t1462.9     \n",
            "10 \t25    \t2825.86\t119.78     \t3072.89    \t2497.89    \n",
            "11 \t22    \t2886.37\t124.239    \t3072.89    \t2497.89    \n",
            "12 \t22    \t2925.12\t138.335    \t3072.89    \t2497.89    \n",
            "13 \t22    \t3060.87\t96.0277    \t3252.85    \t2857.87    \n",
            "14 \t21    \t3127.1 \t77.435     \t3272.87    \t3072.89    \n",
            "15 \t27    \t3106.35\t378.043    \t3537.67    \t1542.95    \n",
            "16 \t26    \t3241.59\t153.483    \t3537.67    \t3072.89    \n",
            "17 \t28    \t3265.8 \t358.458    \t3607.81    \t1907.88    \n",
            "18 \t25    \t3304.51\t582.399    \t3607.81    \t867.899    \n",
            "19 \t22    \t3498.47\t130.44     \t3607.81    \t3072.89    \n",
            "20 \t25    \t3556.75\t58.3898    \t3607.81    \t3412.79    \n",
            "21 \t25    \t3577.79\t60.3302    \t3607.81    \t3412.79    \n",
            "22 \t24    \t3600.8 \t21.0413    \t3607.81    \t3537.67    \n",
            "23 \t28    \t3472.57\t589.515    \t3607.81    \t902.927    \n",
            "24 \t20    \t3607.81\t1.81899e-12\t3607.81    \t3607.81    \n",
            "25 \t25    \t3607.81\t1.81899e-12\t3607.81    \t3607.81    \n",
            "26 \t20    \t3607.81\t1.81899e-12\t3607.81    \t3607.81    \n",
            "27 \t30    \t3474.32\t581.868    \t3607.81    \t938.015    \n",
            "28 \t28    \t3607.81\t1.81899e-12\t3607.81    \t3607.81    \n",
            "29 \t24    \t3607.81\t1.81899e-12\t3607.81    \t3607.81    \n",
            "30 \t18    \t3607.81\t1.81899e-12\t3607.81    \t3607.81    \n",
            "31 \t23    \t3607.81\t1.81899e-12\t3607.81    \t3607.81    \n",
            "32 \t25    \t3481.82\t549.179    \t3607.81    \t1088       \n",
            "33 \t27    \t3607.81\t1.81899e-12\t3607.81    \t3607.81    \n",
            "34 \t25    \t3607.81\t1.81899e-12\t3607.81    \t3607.81    \n",
            "35 \t27    \t3607.81\t1.81899e-12\t3607.81    \t3607.81    \n",
            "36 \t21    \t3607.81\t1.81899e-12\t3607.81    \t3607.81    \n",
            "37 \t26    \t3540.31\t294.21     \t3607.81    \t2257.88    \n",
            "38 \t25    \t3420.06\t573.566    \t3607.81    \t1387.81    \n",
            "39 \t18    \t3607.81\t1.81899e-12\t3607.81    \t3607.81    \n",
            "40 \t23    \t3607.81\t1.81899e-12\t3607.81    \t3607.81    \n",
            "41 \t18    \t3607.81\t1.81899e-12\t3607.81    \t3607.81    \n",
            "42 \t20    \t3607.81\t1.81899e-12\t3607.81    \t3607.81    \n",
            "43 \t24    \t3607.81\t1.81899e-12\t3607.81    \t3607.81    \n",
            "44 \t25    \t3607.81\t1.81899e-12\t3607.81    \t3607.81    \n",
            "45 \t24    \t3498.82\t475.093    \t3607.81    \t1427.93    \n",
            "46 \t28    \t3372.31\t585.796    \t3607.81    \t1417.8     \n",
            "47 \t24    \t3607.81\t1.81899e-12\t3607.81    \t3607.81    \n",
            "48 \t23    \t3484.82\t536.104    \t3607.81    \t1147.99    \n",
            "49 \t23    \t3607.81\t1.81899e-12\t3607.81    \t3607.81    \n",
            "50 \t23    \t3541.31\t289.843    \t3607.81    \t2277.92    \n",
            "51 \t21    \t3490.31\t512.152    \t3607.81    \t1257.9     \n",
            "52 \t29    \t3276.07\t803.361    \t3607.81    \t897.975    \n",
            "53 \t25    \t3464.32\t625.462    \t3607.81    \t737.992    \n",
            "54 \t21    \t3607.81\t1.81899e-12\t3607.81    \t3607.81    \n",
            "55 \t22    \t3607.81\t1.81899e-12\t3607.81    \t3607.81    \n",
            "56 \t17    \t3607.81\t1.81899e-12\t3607.81    \t3607.81    \n",
            "57 \t26    \t3607.81\t1.81899e-12\t3607.81    \t3607.81    \n",
            "58 \t23    \t3607.81\t1.81899e-12\t3607.81    \t3607.81    \n",
            "59 \t19    \t3607.81\t1.81899e-12\t3607.81    \t3607.81    \n",
            "60 \t25    \t3607.81\t1.81899e-12\t3607.81    \t3607.81    \n",
            "61 \t20    \t3607.81\t1.81899e-12\t3607.81    \t3607.81    \n",
            "62 \t28    \t3518.56\t389.038    \t3607.81    \t1822.78    \n",
            "63 \t25    \t3607.81\t1.81899e-12\t3607.81    \t3607.81    \n",
            "64 \t24    \t3221.31\t934.934    \t3607.81    \t712.99     \n",
            "65 \t21    \t3607.81\t1.81899e-12\t3607.81    \t3607.81    \n",
            "66 \t24    \t3540.31\t294.225    \t3607.81    \t2257.81    \n",
            "67 \t25    \t3607.81\t1.81899e-12\t3607.81    \t3607.81    \n",
            "68 \t28    \t3495.07\t491.424    \t3607.81    \t1353       \n",
            "69 \t24    \t3560.07\t208.105    \t3607.81    \t2652.96    \n",
            "70 \t27    \t3560.07\t208.105    \t3607.81    \t2652.96    \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected features: [ True False False False False  True  True False  True False False  True\n",
            "  True False  True False False False False False False False False False\n",
            " False False False False False False False False False False False False\n",
            " False False False False False False False False False False False False\n",
            " False]\n",
            "Accuracy with selected features: 0.506\n",
            "Selecting for params: {'population_size': 20, 'mutation_probability': 0.4, 'crossover_probability': 0.3} and scoring: functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200)\n",
            "gen\tnevals\tfitness\tfitness_std\tfitness_max\tfitness_min\n",
            "0  \t20    \t358.455\t611.864    \t2002.82    \t-386.948   \n",
            "1  \t29    \t605.643\t551.987    \t2002.82    \t-337.098   \n",
            "2  \t29    \t1150.88\t514.917    \t2002.82    \t402.789    \n",
            "3  \t30    \t1450.58\t511.242    \t2002.82    \t188.042    \n",
            "4  \t31    \t1661.32\t392.85     \t2498       \t942.744    \n",
            "5  \t25    \t1940.06\t292.863    \t2498       \t1262.84    \n",
            "6  \t26    \t2155.84\t250.151    \t2498       \t1597.75    \n",
            "7  \t32    \t2330.86\t236        \t2657.89    \t2002.82    \n",
            "8  \t26    \t2415.88\t356.852    \t3252.9     \t1472.86    \n",
            "9  \t27    \t2534.88\t476.975    \t3252.9     \t892.94     \n",
            "10 \t25    \t2684.41\t516.075    \t3747.78    \t1452.85    \n",
            "11 \t25    \t2918.65\t378.853    \t3747.78    \t2107.93    \n",
            "12 \t31    \t3152.64\t423.756    \t3857.93    \t2607.9     \n",
            "13 \t31    \t3232.67\t395.912    \t3857.93    \t2607.9     \n",
            "14 \t27    \t3475.42\t282.254    \t3857.93    \t2992.86    \n",
            "15 \t24    \t3675.15\t236.339    \t3857.93    \t3082.98    \n",
            "16 \t27    \t3848.41\t80.6318    \t4017.85    \t3667.91    \n",
            "17 \t22    \t3889.92\t63.9659    \t4017.85    \t3857.93    \n",
            "18 \t35    \t3569.89\t783.98     \t4017.85    \t1262.95    \n",
            "19 \t34    \t3886.36\t272.77     \t4017.85    \t2977.73    \n",
            "20 \t28    \t4009.85\t34.8526    \t4017.85    \t3857.93    \n",
            "21 \t29    \t4017.85\t9.09495e-13\t4017.85    \t4017.85    \n",
            "22 \t31    \t4017.85\t9.09495e-13\t4017.85    \t4017.85    \n",
            "23 \t34    \t3776.38\t724.411    \t4017.85    \t1603.15    \n",
            "24 \t25    \t3851.87\t514.947    \t4017.85    \t1943.02    \n",
            "25 \t32    \t3841.37\t538.448    \t4017.85    \t1943.02    \n",
            "26 \t29    \t3744.12\t847.487    \t4017.85    \t618.095    \n",
            "27 \t31    \t3910.6 \t467.472    \t4017.85    \t1872.94    \n",
            "28 \t29    \t4017.85\t9.09495e-13\t4017.85    \t4017.85    \n",
            "29 \t27    \t4017.85\t9.09495e-13\t4017.85    \t4017.85    \n",
            "30 \t28    \t4017.85\t9.09495e-13\t4017.85    \t4017.85    \n",
            "31 \t29    \t4017.85\t9.09495e-13\t4017.85    \t4017.85    \n",
            "32 \t31    \t4017.85\t9.09495e-13\t4017.85    \t4017.85    \n",
            "33 \t31    \t3913.6 \t454.392    \t4017.85    \t1932.96    \n",
            "34 \t28    \t3913.6 \t454.392    \t4017.85    \t1932.96    \n",
            "35 \t27    \t3761.11\t649.935    \t4017.85    \t1872.84    \n",
            "36 \t29    \t3706.61\t843.194    \t4027.88    \t1142.96    \n",
            "37 \t31    \t3695.11\t879.111    \t4027.88    \t902.904    \n",
            "38 \t34    \t3840.61\t574.438    \t4017.85    \t1557.98    \n",
            "39 \t26    \t4017.85\t9.09495e-13\t4017.85    \t4017.85    \n",
            "40 \t30    \t4017.85\t9.09495e-13\t4017.85    \t4017.85    \n",
            "41 \t33    \t4017.85\t9.09495e-13\t4017.85    \t4017.85    \n",
            "42 \t27    \t4017.85\t9.09495e-13\t4017.85    \t4017.85    \n",
            "43 \t28    \t4017.85\t9.09495e-13\t4017.85    \t4017.85    \n",
            "44 \t31    \t4017.85\t9.09495e-13\t4017.85    \t4017.85    \n",
            "45 \t27    \t3895.34\t533.999    \t4017.85    \t1567.69    \n",
            "46 \t26    \t3878.35\t608.069    \t4017.85    \t1227.84    \n",
            "47 \t25    \t3902.1 \t424.558    \t4017.85    \t2087.9     \n",
            "48 \t23    \t3892.85\t544.849    \t4017.85    \t1517.91    \n",
            "49 \t26    \t4017.85\t9.09495e-13\t4017.85    \t4017.85    \n",
            "50 \t20    \t3916.1 \t443.535    \t4017.85    \t1982.77    \n",
            "51 \t21    \t3895.36\t533.919    \t4017.85    \t1568.06    \n",
            "52 \t29    \t4017.85\t9.09495e-13\t4017.85    \t4017.85    \n",
            "53 \t28    \t3913.11\t456.568    \t4017.85    \t1922.97    \n",
            "54 \t31    \t4017.85\t9.09495e-13\t4017.85    \t4017.85    \n",
            "55 \t30    \t3914.36\t451.103    \t4017.85    \t1948.05    \n",
            "56 \t29    \t3719.11\t711.464    \t4017.85    \t1948.05    \n",
            "57 \t29    \t3940.35\t337.802    \t4017.85    \t2467.91    \n",
            "58 \t34    \t4017.85\t9.09495e-13\t4017.85    \t4017.85    \n",
            "59 \t27    \t4017.85\t9.09495e-13\t4017.85    \t4017.85    \n",
            "60 \t27    \t4017.85\t9.09495e-13\t4017.85    \t4017.85    \n",
            "61 \t24    \t4017.85\t9.09495e-13\t4017.85    \t4017.85    \n",
            "62 \t34    \t3883.35\t586.261    \t4017.85    \t1327.9     \n",
            "63 \t30    \t4017.85\t9.09495e-13\t4017.85    \t4017.85    \n",
            "64 \t31    \t4017.85\t9.09495e-13\t4017.85    \t4017.85    \n",
            "65 \t31    \t3703.36\t962.371    \t4017.85    \t272.92     \n",
            "66 \t33    \t4017.85\t9.09495e-13\t4017.85    \t4017.85    \n",
            "67 \t24    \t4017.85\t9.09495e-13\t4017.85    \t4017.85    \n",
            "68 \t28    \t3912.85\t457.674    \t4017.85    \t1917.9     \n",
            "69 \t29    \t4017.85\t9.09495e-13\t4017.85    \t4017.85    \n",
            "70 \t31    \t3911.11\t465.277    \t4017.85    \t1883.01    \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected features: [ True  True False False False False False False False False False  True\n",
            " False False False False False False False False False False False  True\n",
            " False False False False False  True False False False False False False\n",
            " False False False False False False False False False False False False\n",
            " False]\n",
            "Accuracy with selected features: 0.511\n",
            "Selecting for params: {'population_size': 30, 'mutation_probability': 0.6, 'crossover_probability': 0.1} and scoring: functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200)\n",
            "gen\tnevals\tfitness\tfitness_std\tfitness_max\tfitness_min\n",
            "0  \t30    \t499.623\t556.682    \t1637.86    \t-482.066   \n",
            "1  \t43    \t902.939\t293.494    \t1637.86    \t507.917    \n",
            "2  \t42    \t1009.8 \t234.596    \t1302.96    \t413.105    \n",
            "3  \t45    \t1177.93\t171.578    \t1617.78    \t813.075    \n",
            "4  \t38    \t1256.13\t110.949    \t1383.03    \t932.987    \n",
            "5  \t42    \t1316.31\t78.6061    \t1383.03    \t1062.87    \n",
            "6  \t47    \t1339.17\t108.969    \t1477.91    \t968.037    \n",
            "7  \t42    \t1377.8 \t311.496    \t1837.88    \t147.995    \n",
            "8  \t43    \t1396.15\t324.672    \t1943.09    \t558.02     \n",
            "9  \t46    \t1298.82\t407.401    \t1837.88    \t122.977    \n",
            "10 \t40    \t1448.83\t333.511    \t1973.04    \t422.824    \n",
            "11 \t40    \t1598.32\t341.768    \t1973.04    \t667.862    \n",
            "12 \t42    \t1759.14\t167.871    \t1973.04    \t1322.95    \n",
            "13 \t41    \t1874.14\t134.852    \t2322.94    \t1603.06    \n",
            "14 \t43    \t1756.81\t427.582    \t2037.91    \t497.907    \n",
            "15 \t42    \t1986.85\t96.3269    \t2198.09    \t1648.13    \n",
            "16 \t42    \t1991.86\t397.936    \t2398.14    \t158.005    \n",
            "17 \t40    \t2073.17\t266.849    \t2398.14    \t817.782    \n",
            "18 \t38    \t2069.38\t448.939    \t2398.14    \t302.935    \n",
            "19 \t40    \t2239.24\t234.177    \t2398.14    \t1077.93    \n",
            "20 \t43    \t2226.92\t352.909    \t2398.14    \t763.002    \n",
            "21 \t42    \t2264.44\t353.978    \t2398.14    \t423.115    \n",
            "22 \t44    \t2132.1 \t574.045    \t2398.14    \t267.847    \n",
            "23 \t44    \t2130.62\t643.361    \t2398.14    \t127.937    \n",
            "24 \t39    \t2096.77\t662.88     \t2398.14    \t452.854    \n",
            "25 \t33    \t2149.12\t646.249    \t2398.14    \t22.7491    \n",
            "26 \t43    \t2241.79\t585.074    \t2398.14    \t22.7491    \n",
            "27 \t40    \t2318.96\t426.396    \t2398.14    \t22.7491    \n",
            "28 \t46    \t2336.14\t333.917    \t2398.14    \t537.939    \n",
            "29 \t39    \t2275.46\t466.422    \t2398.14    \t237.847    \n",
            "30 \t46    \t1960.08\t683.731    \t2398.14    \t218.087    \n",
            "31 \t39    \t2398.14\t9.09495e-13\t2398.14    \t2398.14    \n",
            "32 \t44    \t2398.14\t9.09495e-13\t2398.14    \t2398.14    \n",
            "33 \t43    \t2109.61\t661.957    \t2398.14    \t247.864    \n",
            "34 \t44    \t2266.96\t395.684    \t2398.14    \t902.942    \n",
            "35 \t43    \t2214.13\t569.277    \t2398.14    \t208.092    \n",
            "36 \t45    \t1994.44\t834.699    \t2398.14    \t-591.863   \n",
            "37 \t42    \t2191.95\t549.926    \t2398.14    \t162.934    \n",
            "38 \t45    \t2343.3 \t295.314    \t2398.14    \t752.992    \n",
            "39 \t44    \t2192.95\t622.845    \t2398.14    \t37.892     \n",
            "40 \t41    \t2295.3 \t385.705    \t2398.14    \t752.992    \n",
            "41 \t40    \t2244.96\t482.096    \t2398.14    \t532.987    \n",
            "42 \t48    \t2114.27\t657.189    \t2398.14    \t192.802    \n",
            "43 \t48    \t2105.43\t700.332    \t2548.05    \t-142.156   \n",
            "44 \t47    \t2262.11\t514.614    \t2548.05    \t62.8568    \n",
            "45 \t43    \t2336.46\t266.585    \t2398.14    \t933.01     \n",
            "46 \t45    \t2072.43\t716.912    \t2398.14    \t-97.0279   \n",
            "47 \t44    \t2173.27\t537.852    \t2398.14    \t427.882    \n",
            "48 \t44    \t1849.94\t1018.43    \t2398.14    \t-731.84    \n",
            "49 \t43    \t2275.47\t463.886    \t2398.14    \t297.99     \n",
            "50 \t46    \t2102.94\t757.174    \t2398.14    \t-191.96    \n",
            "51 \t40    \t2342.3 \t300.697    \t2398.14    \t723        \n",
            "52 \t46    \t2175.47\t672.717    \t2398.14    \t-181.83    \n",
            "53 \t40    \t2273.12\t385.887    \t2398.14    \t748.047    \n",
            "54 \t41    \t2363.63\t329.276    \t2583.08    \t622.862    \n",
            "55 \t42    \t2370.96\t394.553    \t2583.08    \t287.95     \n",
            "56 \t48    \t1900.06\t916.624    \t2583.08    \t47.8192    \n",
            "57 \t42    \t2229.44\t531.11     \t2583.08    \t558.057    \n",
            "58 \t39    \t2358.28\t281.157    \t2583.08    \t1337.92    \n",
            "59 \t41    \t2248.77\t659.108    \t2583.08    \t267.899    \n",
            "60 \t47    \t2138.76\t825.417    \t2583.08    \t-416.948   \n",
            "61 \t45    \t2185.91\t690.467    \t2583.08    \t472.904    \n",
            "62 \t45    \t2255.74\t657.876    \t2583.08    \t568.09     \n",
            "63 \t45    \t2376.58\t540.719    \t2583.08    \t183.015    \n",
            "64 \t42    \t2570.75\t46.1321    \t2583.08    \t2398.14    \n",
            "65 \t46    \t2278.08\t778.026    \t2583.08    \t178.025    \n",
            "66 \t41    \t2352.24\t697.053    \t2583.08    \t-51.9903   \n",
            "67 \t50    \t2249.55\t681.103    \t2583.08    \t443.09     \n",
            "68 \t33    \t2359.71\t684.272    \t2897.93    \t162.987    \n",
            "69 \t46    \t2399.04\t662.178    \t2897.93    \t363.032    \n",
            "70 \t42    \t2495.36\t631.479    \t2897.93    \t363.032    \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected features: [False False False False False False  True False False  True False False\n",
            "  True  True False False False False False False False False False False\n",
            " False False  True False False  True False  True  True  True  True False\n",
            " False False False False False  True False  True False False False False\n",
            " False]\n",
            "Accuracy with selected features: 0.528\n",
            "Selecting for params: {'population_size': 30, 'mutation_probability': 0.6, 'crossover_probability': 0.2} and scoring: functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200)\n",
            "gen\tnevals\tfitness\tfitness_std\tfitness_max\tfitness_min\n",
            "0  \t30    \t224.332\t529.12     \t1323.04    \t-871.727   \n",
            "1  \t44    \t795.474\t473.85     \t2072.87    \t138.06     \n",
            "2  \t45    \t986.96 \t305.533    \t1482.86    \t398.075    \n",
            "3  \t54    \t1232.46\t315.893    \t2383.1     \t642.874    \n",
            "4  \t45    \t1398.63\t484.499    \t2383.1     \t-52.0054   \n",
            "5  \t48    \t1475.28\t474.116    \t2383.1     \t452.854    \n",
            "6  \t43    \t1664.11\t384.171    \t2383.1     \t538.03     \n",
            "7  \t44    \t1731.97\t456.373    \t2383.1     \t837.824    \n",
            "8  \t47    \t1895.94\t516.923    \t2383.1     \t167.977    \n",
            "9  \t46    \t2119.64\t258.72     \t2383.1     \t1433.18    \n",
            "10 \t55    \t2198.99\t272.876    \t2482.91    \t938.045    \n",
            "11 \t50    \t2093.51\t512.653    \t2383.1     \t697.877    \n",
            "12 \t46    \t2220.89\t469.411    \t2383.1     \t58.1424    \n",
            "13 \t53    \t2101.55\t676.514    \t2383.1     \t-102.078   \n",
            "14 \t46    \t1981.71\t680.264    \t2383.1     \t337.962    \n",
            "15 \t49    \t1984.21\t705.506    \t2383.1     \t157.967    \n",
            "16 \t49    \t2134.07\t565.951    \t2553.07    \t307.94     \n",
            "17 \t52    \t2172.59\t594.696    \t2588.1     \t23.16      \n",
            "18 \t49    \t2268.26\t454.415    \t2588.1     \t588.035    \n",
            "19 \t46    \t2337.09\t397.164    \t2588.1     \t798.037    \n",
            "20 \t45    \t2417.1 \t350.996    \t2588.1     \t603.065    \n",
            "21 \t50    \t2167.59\t760.994    \t2677.92    \t-141.812   \n",
            "22 \t47    \t2155.24\t732.261    \t2677.92    \t507.977    \n",
            "23 \t49    \t2485.08\t350.715    \t2723       \t1018.12    \n",
            "24 \t48    \t2398.23\t606.949    \t2723       \t257.852    \n",
            "25 \t46    \t2437.41\t600.696    \t2677.92    \t128.005    \n",
            "26 \t55    \t2239.53\t803.949    \t2677.92    \t-426.958   \n",
            "27 \t45    \t2461.99\t590.404    \t2907.98    \t518.025    \n",
            "28 \t48    \t2476.13\t628.962    \t2907.98    \t477.94     \n",
            "29 \t45    \t2441.29\t779.5      \t2907.98    \t142.937    \n",
            "30 \t53    \t2590.12\t572.503    \t2907.98    \t403.057    \n",
            "31 \t52    \t2695.14\t530.703    \t2943       \t-92.1056   \n",
            "32 \t49    \t2713.8 \t428.711    \t2943       \t472.859    \n",
            "33 \t44    \t2622.99\t707.936    \t2943       \t107.94     \n",
            "34 \t51    \t2607.16\t664.691    \t2943       \t472.859    \n",
            "35 \t42    \t2893.66\t77.1747    \t2943       \t2723       \n",
            "36 \t52    \t2674.17\t724.463    \t3128       \t363.092    \n",
            "37 \t55    \t2770.83\t566.726    \t3128       \t628.12     \n",
            "38 \t53    \t2804   \t477.583    \t3128       \t598.127    \n",
            "39 \t51    \t2593.17\t956.608    \t3148.02    \t-416.955   \n",
            "40 \t47    \t2595.5 \t830.818    \t3128       \t393.055    \n",
            "41 \t50    \t2821.17\t606.783    \t3128       \t8.12993    \n",
            "42 \t51    \t3023.17\t91.6704    \t3128       \t2943       \n",
            "43 \t48    \t2830.84\t734.746    \t3128       \t403.102    \n",
            "44 \t49    \t2922.83\t613.366    \t3128       \t198.127    \n",
            "45 \t49    \t3005.5 \t417.579    \t3128       \t823.19     \n",
            "46 \t45    \t2905.34\t754.307    \t3128       \t-6.9754    \n",
            "47 \t49    \t2702.17\t954.005    \t3128       \t357.997    \n",
            "48 \t48    \t3041.16\t467.607    \t3128       \t523.022    \n",
            "49 \t48    \t3035.16\t499.93     \t3128       \t342.96     \n",
            "50 \t44    \t2941.67\t697.992    \t3128       \t203.178    \n",
            "51 \t46    \t2981.66\t624.713    \t3128       \t-231.97    \n",
            "52 \t45    \t3128   \t0          \t3128       \t3128       \n",
            "53 \t46    \t3128   \t0          \t3128       \t3128       \n",
            "54 \t44    \t3128   \t0          \t3128       \t3128       \n",
            "55 \t47    \t2821.49\t924.999    \t3128       \t-287.093   \n",
            "56 \t50    \t2838.33\t884.438    \t3128       \t-201.993   \n",
            "57 \t52    \t3128   \t0          \t3128       \t3128       \n",
            "58 \t48    \t3042.66\t459.536    \t3128       \t567.985    \n",
            "59 \t48    \t3055.82\t388.673    \t3128       \t962.757    \n",
            "60 \t44    \t2953.16\t654.176    \t3128       \t498.005    \n",
            "61 \t48    \t2844.16\t856.142    \t3128       \t-107.158   \n",
            "62 \t54    \t2873.82\t792.033    \t3128       \t-72.0329   \n",
            "63 \t54    \t2906.15\t710.011    \t3417.85    \t292.864    \n",
            "64 \t46    \t2750.96\t972.27     \t3417.85    \t-231.948   \n",
            "65 \t44    \t2519.1 \t1230.79    \t3417.85    \t-497.088   \n",
            "66 \t45    \t3126.59\t560.984    \t3417.85    \t178.055    \n",
            "67 \t52    \t2897.93\t1022.65    \t3617.78    \t-206.923   \n",
            "68 \t42    \t3136.56\t717.536    \t3617.78    \t512.937    \n",
            "69 \t47    \t2937.08\t1055.48    \t3558.03    \t-396.92    \n",
            "70 \t51    \t3222.06\t564.72     \t3558.03    \t398.09     \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected features: [False False False False False False  True False False False False  True\n",
            "  True  True  True False False False False False False False False False\n",
            "  True False False False False False  True False False False False False\n",
            " False False False False False False  True False False False False False\n",
            " False]\n",
            "Accuracy with selected features: 0.534\n",
            "Selecting for params: {'population_size': 30, 'mutation_probability': 0.6, 'crossover_probability': 0.3} and scoring: functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200)\n",
            "gen\tnevals\tfitness\tfitness_std\tfitness_max\tfitness_min\n",
            "0  \t30    \t253.789\t680.475    \t1598.07    \t-867.02    \n",
            "1  \t55    \t756.629\t491.989    \t2373.03    \t143.042    \n",
            "2  \t52    \t908.633\t358.286    \t1437.91    \t-67.0505   \n",
            "3  \t54    \t1084.64\t310.245    \t1773.03    \t577.734    \n",
            "4  \t55    \t1157.62\t330.827    \t1917.89    \t707.992    \n",
            "5  \t53    \t1363.95\t409.418    \t2028.03    \t367.985    \n",
            "6  \t55    \t1561.8 \t368.214    \t2097.91    \t797.769    \n",
            "7  \t54    \t1571.97\t491.492    \t2247.82    \t32.8569    \n",
            "8  \t56    \t1755.6 \t331.104    \t2247.82    \t1097.95    \n",
            "9  \t55    \t1730.43\t453.514    \t2247.82    \t307.932    \n",
            "10 \t57    \t1840.26\t488.544    \t2517.93    \t157.982    \n",
            "11 \t55    \t1819.77\t510.398    \t2362.99    \t347.987    \n",
            "12 \t57    \t1931.62\t462.818    \t2362.99    \t178.062    \n",
            "13 \t54    \t1914.61\t681.231    \t2517.96    \t-252.013   \n",
            "14 \t54    \t2077.93\t480.148    \t2733       \t902.98     \n",
            "15 \t52    \t2079.74\t624.557    \t2733       \t492.932    \n",
            "16 \t51    \t2316.75\t545.964    \t2733       \t568.075    \n",
            "17 \t56    \t2258.92\t725.777    \t2947.93    \t363.07     \n",
            "18 \t56    \t2473.93\t448.412    \t3107.99    \t822.809    \n",
            "19 \t49    \t2691.1 \t379.797    \t3217.83    \t1048.09    \n",
            "20 \t56    \t2703.04\t710.104    \t3217.83    \t-112.163   \n",
            "21 \t54    \t2912.69\t582.478    \t3357.72    \t442.777    \n",
            "22 \t56    \t2727.21\t898.933    \t3387.74    \t507.94     \n",
            "23 \t54    \t2548.53\t1097.73    \t3567.77    \t163.04     \n",
            "24 \t54    \t2481.37\t1244.43    \t3567.77    \t3.07225    \n",
            "25 \t54    \t2702.53\t1046.67    \t3567.77    \t3.07225    \n",
            "26 \t52    \t2881.7 \t922.493    \t3567.77    \t3.1174     \n",
            "27 \t53    \t3097.19\t855.999    \t3772.76    \t-21.9377   \n",
            "28 \t52    \t3103.85\t835.518    \t3772.76    \t568.045    \n",
            "29 \t53    \t3090.34\t931.772    \t3772.76    \t102.927    \n",
            "30 \t54    \t2870.53\t1222.92    \t3772.76    \t-366.86    \n",
            "31 \t54    \t3108.54\t1020.66    \t3973.04    \t132.949    \n",
            "32 \t53    \t3404.02\t673.658    \t3977.74    \t1097.9     \n",
            "33 \t56    \t3245.86\t960.66     \t3977.74    \t-92.068    \n",
            "34 \t54    \t3377   \t1037.05    \t3977.74    \t-441.973   \n",
            "35 \t56    \t3300.84\t1066.75    \t3977.74    \t167.985    \n",
            "36 \t55    \t3555.33\t984.169    \t3977.74    \t-477.015   \n",
            "37 \t54    \t3644.51\t948.789    \t3977.74    \t58.1725    \n",
            "38 \t53    \t3667   \t858.697    \t3977.74    \t238.152    \n",
            "39 \t54    \t3695.48\t815.088    \t3977.74    \t633.072    \n",
            "40 \t56    \t3302.96\t1407.27    \t3977.74    \t-312.118   \n",
            "41 \t53    \t3315.8 \t1333.49    \t3977.74    \t-201.963   \n",
            "42 \t57    \t3576.42\t984.221    \t3977.74    \t-342.201   \n",
            "43 \t51    \t3729.09\t782.851    \t3977.74    \t217.842    \n",
            "44 \t54    \t3707.1 \t855.278    \t4222.83    \t588.05     \n",
            "45 \t55    \t3566.1 \t1207.77    \t3977.74    \t-262.03    \n",
            "46 \t58    \t3459.11\t1309.11    \t3977.74    \t-642.056   \n",
            "47 \t57    \t3459.95\t1204.39    \t3977.74    \t153.007    \n",
            "48 \t53    \t3851.25\t527.337    \t3977.74    \t1072.87    \n",
            "49 \t51    \t3938.41\t211.793    \t3977.74    \t2797.87    \n",
            "50 \t53    \t3718.26\t831.743    \t3977.74    \t727.884    \n",
            "51 \t58    \t3537.77\t1220.14    \t3977.74    \t-277.098   \n",
            "52 \t48    \t3868.58\t587.84     \t3977.74    \t702.965    \n",
            "53 \t55    \t3486.28\t1266.61    \t3977.74    \t-576.863   \n",
            "54 \t56    \t2951.82\t1641.33    \t3977.74    \t-267.051   \n",
            "55 \t50    \t3633.28\t1017.71    \t3977.74    \t-771.843   \n",
            "56 \t55    \t3729.26\t908.271    \t3977.74    \t282.892    \n",
            "57 \t54    \t3638.27\t986.898    \t3977.74    \t282.892    \n",
            "58 \t53    \t3875.42\t554.638    \t4152.69    \t897.967    \n",
            "59 \t50    \t3681.76\t812.47     \t4152.69    \t877.94     \n",
            "60 \t52    \t3610.27\t944.373    \t4152.69    \t368.015    \n",
            "61 \t55    \t3927.08\t232.585    \t4152.69    \t2792.78    \n",
            "62 \t53    \t3983.24\t118.101    \t4152.69    \t3432.89    \n",
            "63 \t55    \t3773.92\t817.856    \t4152.69    \t737.954    \n",
            "64 \t51    \t3910.91\t593.76     \t4152.69    \t737.954    \n",
            "65 \t52    \t3942.72\t619.579    \t4152.69    \t637.809    \n",
            "66 \t50    \t3989.71\t718.198    \t4152.69    \t137.954    \n",
            "67 \t56    \t3645.07\t1255.96    \t4152.69    \t-106.948   \n",
            "68 \t55    \t3564.26\t1316.28    \t4152.69    \t218.14     \n",
            "69 \t48    \t3910.04\t907.913    \t4152.69    \t512.945    \n",
            "70 \t57    \t3727.4 \t1280.35    \t4152.69    \t-431.918   \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected features: [False False False False False False  True False False False False  True\n",
            " False False False False  True False False False False False False False\n",
            " False False False False False False False False False  True False  True\n",
            " False False False False False False False False False False False False\n",
            " False]\n",
            "Accuracy with selected features: 0.53\n",
            "Selecting for params: {'population_size': 30, 'mutation_probability': 0.5, 'crossover_probability': 0.1} and scoring: functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200)\n",
            "gen\tnevals\tfitness\tfitness_std\tfitness_max\tfitness_min\n",
            "0  \t30    \t376.311\t706.61     \t1627.81    \t-1026.99   \n",
            "1  \t34    \t1046.33\t605.956    \t2032.84    \t-441.98    \n",
            "2  \t33    \t1496.32\t383.004    \t2032.84    \t442.882    \n",
            "3  \t40    \t1743.82\t341.732    \t2617.77    \t1027.9     \n",
            "4  \t37    \t1795.99\t284.621    \t2617.77    \t1042.91    \n",
            "5  \t35    \t1784.17\t226.964    \t2032.84    \t1042.91    \n",
            "6  \t30    \t1821.35\t210.061    \t2032.84    \t953.022    \n",
            "7  \t38    \t1924.66\t114.967    \t2043.18    \t1788.07    \n",
            "8  \t40    \t1831.98\t420.236    \t2297.89    \t593.047    \n",
            "9  \t39    \t1975.63\t424.57     \t2727.93    \t517.934    \n",
            "10 \t34    \t2087.27\t436.791    \t2727.93    \t322.977    \n",
            "11 \t34    \t2258.44\t282.008    \t2727.93    \t1733.08    \n",
            "12 \t34    \t2411.76\t273.151    \t2727.93    \t1822.83    \n",
            "13 \t33    \t2515.74\t193.108    \t2727.93    \t2168.04    \n",
            "14 \t31    \t2661.25\t136.959    \t2727.93    \t2297.89    \n",
            "15 \t36    \t2596.09\t451.534    \t2962.99    \t1252.87    \n",
            "16 \t36    \t2707.43\t251.387    \t2962.99    \t1407.81    \n",
            "17 \t35    \t2566.44\t602.575    \t2962.99    \t-31.9252   \n",
            "18 \t31    \t2752.62\t281.036    \t2962.99    \t1352.99    \n",
            "19 \t38    \t2703.47\t666.696    \t2962.99    \t117.897    \n",
            "20 \t39    \t2767.32\t567.128    \t2962.99    \t817.804    \n",
            "21 \t34    \t2738.64\t666.88     \t2962.99    \t267.93     \n",
            "22 \t35    \t2955.15\t42.1941    \t2962.99    \t2727.93    \n",
            "23 \t37    \t2792.65\t523.462    \t2962.99    \t772.997    \n",
            "24 \t41    \t2962.99\t0          \t2962.99    \t2962.99    \n",
            "25 \t34    \t2830.98\t501.441    \t2962.99    \t647.842    \n",
            "26 \t29    \t2890.81\t388.664    \t2962.99    \t797.792    \n",
            "27 \t34    \t2962.99\t0          \t2962.99    \t2962.99    \n",
            "28 \t34    \t2900.98\t333.894    \t2962.99    \t1102.91    \n",
            "29 \t37    \t2841.82\t455.06     \t2962.99    \t993.1      \n",
            "30 \t33    \t2962.99\t0          \t2962.99    \t2962.99    \n",
            "31 \t27    \t2962.99\t0          \t2962.99    \t2962.99    \n",
            "32 \t37    \t2851.98\t415.336    \t2962.99    \t1297.94    \n",
            "33 \t45    \t2701.99\t614.834    \t2962.99    \t738        \n",
            "34 \t33    \t2691.15\t564.404    \t2962.99    \t973.072    \n",
            "35 \t24    \t2876.15\t330.655    \t2962.99    \t1422.85    \n",
            "36 \t38    \t2730.16\t629.266    \t2962.99    \t452.937    \n",
            "37 \t35    \t2818.83\t553.883    \t2962.99    \t313.005    \n",
            "38 \t40    \t2886.49\t313.895    \t3072.9     \t1423.17    \n",
            "39 \t39    \t2664.15\t675.484    \t2962.99    \t753.06     \n",
            "40 \t30    \t2744.82\t655.573    \t2962.99    \t652.862    \n",
            "41 \t40    \t2749.33\t648.094    \t2963.02    \t497.869    \n",
            "42 \t42    \t2705.49\t664.596    \t2963.02    \t553.09     \n",
            "43 \t27    \t2848.65\t427.884    \t2963.02    \t1222.79    \n",
            "44 \t32    \t2962.99\t0          \t2962.99    \t2962.99    \n",
            "45 \t36    \t2962.99\t0          \t2962.99    \t2962.99    \n",
            "46 \t36    \t2865.47\t372.6      \t2962.99    \t1207.81    \n",
            "47 \t21    \t2962.99\t0          \t2962.99    \t2962.99    \n",
            "48 \t36    \t2962.99\t0          \t2962.99    \t2962.99    \n",
            "49 \t37    \t2812.99\t580.225    \t2962.99    \t143.02     \n",
            "50 \t39    \t2962.99\t0          \t2962.99    \t2962.99    \n",
            "51 \t40    \t2908.15\t295.297    \t2962.99    \t1317.93    \n",
            "52 \t33    \t2857.66\t394.103    \t2962.99    \t1383.06    \n",
            "53 \t37    \t2962.99\t0          \t2962.99    \t2962.99    \n",
            "54 \t36    \t2962.99\t0          \t2962.99    \t2962.99    \n",
            "55 \t35    \t2878.81\t453.286    \t2962.99    \t437.794    \n",
            "56 \t34    \t2962.99\t0          \t2962.99    \t2962.99    \n",
            "57 \t39    \t2962.99\t0          \t2962.99    \t2962.99    \n",
            "58 \t41    \t2779.32\t554.595    \t2962.99    \t897.847    \n",
            "59 \t30    \t2942.65\t109.497    \t2962.99    \t2352.99    \n",
            "60 \t38    \t2763.15\t648.776    \t2962.99    \t-67.0054   \n",
            "61 \t38    \t2784.32\t541.285    \t2962.99    \t988.117    \n",
            "62 \t31    \t2833.32\t485.735    \t2962.99    \t927.937    \n",
            "63 \t41    \t2850.65\t422.627    \t2962.99    \t1107.95    \n",
            "64 \t36    \t2901.48\t331.201    \t2962.99    \t1117.91    \n",
            "65 \t33    \t2962.99\t0          \t2962.99    \t2962.99    \n",
            "66 \t36    \t2962.99\t0          \t2962.99    \t2962.99    \n",
            "67 \t38    \t2759.32\t636.169    \t2962.99    \t333.047    \n",
            "68 \t43    \t2814.65\t454.627    \t2962.99    \t1122.95    \n",
            "69 \t34    \t2946.65\t87.986     \t2962.99    \t2472.83    \n",
            "70 \t38    \t2962.99\t0          \t2962.99    \t2962.99    \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected features: [ True False False False False False  True False False  True  True False\n",
            " False False  True False False  True False False  True False  True False\n",
            " False False False  True False False False False False  True False False\n",
            " False False False False False  True False False False False False False\n",
            " False]\n",
            "Accuracy with selected features: 0.553\n",
            "Selecting for params: {'population_size': 30, 'mutation_probability': 0.5, 'crossover_probability': 0.2} and scoring: functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200)\n",
            "gen\tnevals\tfitness\tfitness_std\tfitness_max\tfitness_min\n",
            "0  \t30    \t314.966\t587.901    \t1687.92    \t-1057.07   \n",
            "1  \t43    \t786.462\t609.755    \t1783.1     \t-711.82    \n",
            "2  \t43    \t1303.17\t535.194    \t2128.03    \t427.912    \n",
            "3  \t45    \t1573   \t370.648    \t2128.03    \t497.914    \n",
            "4  \t41    \t1813.67\t190.446    \t2128.03    \t1382.73    \n",
            "5  \t46    \t2013.14\t256.265    \t2632.87    \t1692.97    \n",
            "6  \t47    \t2233.6 \t328.851    \t2632.87    \t1462.83    \n",
            "7  \t37    \t2387.61\t485.616    \t2758.07    \t112.952    \n",
            "8  \t37    \t2509.92\t415.928    \t2897.95    \t862.819    \n",
            "9  \t47    \t2593.61\t405.339    \t2897.95    \t818.117    \n",
            "10 \t43    \t2577.12\t639.907    \t3132.99    \t93.1699    \n",
            "11 \t46    \t2823.95\t257.753    \t3352.72    \t1788       \n",
            "12 \t43    \t2896.41\t405.972    \t3352.72    \t977.734    \n",
            "13 \t38    \t2982.09\t268.107    \t3352.72    \t1867.9     \n",
            "14 \t40    \t3016.57\t369.722    \t3352.72    \t1297.96    \n",
            "15 \t43    \t3255.83\t205.96     \t3947.7     \t2897.95    \n",
            "16 \t40    \t3194.12\t476.488    \t3557.74    \t1077.84    \n",
            "17 \t46    \t3242.42\t521.906    \t3557.74    \t872.897    \n",
            "18 \t39    \t3323.42\t593.664    \t3557.74    \t998.052    \n",
            "19 \t33    \t3520.41\t77.0519    \t3557.74    \t3312.66    \n",
            "20 \t48    \t3372.92\t595.239    \t3557.74    \t717.965    \n",
            "21 \t41    \t3273.76\t784.4      \t3557.74    \t432.819    \n",
            "22 \t47    \t3231.95\t742.123    \t3582.78    \t1087.91    \n",
            "23 \t42    \t3290.29\t702.327    \t3582.78    \t1023.05    \n",
            "24 \t39    \t3392.44\t513.73     \t3582.78    \t1593.11    \n",
            "25 \t42    \t3510.6 \t359.486    \t3757.77    \t1593.11    \n",
            "26 \t41    \t3521.77\t413.786    \t3757.77    \t1328.04    \n",
            "27 \t43    \t3589.77\t336.506    \t3757.77    \t1842.86    \n",
            "28 \t39    \t3722.77\t69.999     \t3757.77    \t3582.78    \n",
            "29 \t41    \t3751.94\t31.413     \t3757.77    \t3582.78    \n",
            "30 \t47    \t3665.62\t496.272    \t3757.77    \t993.115    \n",
            "31 \t42    \t3675.61\t442.459    \t3757.77    \t1292.89    \n",
            "32 \t43    \t3368.62\t997.694    \t3757.77    \t502.889    \n",
            "33 \t43    \t3665.45\t497.169    \t3757.77    \t988.117    \n",
            "34 \t44    \t3757.77\t2.72848e-12\t3757.77    \t3757.77    \n",
            "35 \t40    \t3699.79\t312.274    \t3757.77    \t2018.14    \n",
            "36 \t45    \t3538.12\t822.571    \t3757.77    \t332.995    \n",
            "37 \t40    \t3647.95\t591.404    \t3757.77    \t463.147    \n",
            "38 \t47    \t3587.61\t637.833    \t3757.77    \t1057.88    \n",
            "39 \t40    \t3671.28\t465.78     \t3757.77    \t1162.98    \n",
            "40 \t41    \t3757.77\t2.72848e-12\t3757.77    \t3757.77    \n",
            "41 \t42    \t3599.12\t593.706    \t3757.77    \t1342.99    \n",
            "42 \t43    \t3569.29\t720.584    \t3757.77    \t357.944    \n",
            "43 \t41    \t3696.11\t332.049    \t3757.77    \t1907.98    \n",
            "44 \t31    \t3691.11\t358.975    \t3757.77    \t1757.97    \n",
            "45 \t45    \t3493.63\t798.206    \t3757.77    \t822.862    \n",
            "46 \t38    \t3589.11\t631.077    \t3757.77    \t1227.84    \n",
            "47 \t42    \t3318.14\t1134.76    \t3757.77    \t43.1725    \n",
            "48 \t38    \t3671.95\t462.178    \t3757.77    \t1183.04    \n",
            "49 \t43    \t3757.77\t2.72848e-12\t3757.77    \t3757.77    \n",
            "50 \t45    \t3513.62\t743.217    \t3757.77    \t752.992    \n",
            "51 \t41    \t3757.77\t2.72848e-12\t3757.77    \t3757.77    \n",
            "52 \t41    \t3493.96\t795.67     \t3757.77    \t758.042    \n",
            "53 \t43    \t3504.79\t761.269    \t3757.77    \t962.734    \n",
            "54 \t42    \t3491.45\t762.005    \t3757.77    \t962.734    \n",
            "55 \t43    \t3675.45\t443.351    \t3757.77    \t1287.93    \n",
            "56 \t40    \t3624.45\t717.981    \t3757.77    \t-241.995   \n",
            "57 \t44    \t3700.27\t309.65     \t3757.77    \t2032.76    \n",
            "58 \t41    \t3578.12\t675.047    \t3757.77    \t823.1      \n",
            "59 \t36    \t3674.28\t449.639    \t3757.77    \t1252.9     \n",
            "60 \t43    \t3664.45\t502.562    \t3757.77    \t958.072    \n",
            "61 \t42    \t3437.13\t817.667    \t3757.77    \t1322.95    \n",
            "62 \t43    \t3595.79\t606.101    \t3757.77    \t1322.95    \n",
            "63 \t41    \t3511.8 \t738.095    \t3757.77    \t1228.04    \n",
            "64 \t44    \t3613.95\t539.213    \t3757.77    \t1467.94    \n",
            "65 \t45    \t3751.29\t48.4673    \t3822.91    \t3497.98    \n",
            "66 \t41    \t3653.45\t574        \t3822.91    \t563.01     \n",
            "67 \t40    \t3546.12\t824.423    \t3992.85    \t448.095    \n",
            "68 \t36    \t3677.79\t542.817    \t3992.85    \t783.09     \n",
            "69 \t39    \t3571.46\t761.049    \t3992.85    \t677.857    \n",
            "70 \t45    \t3318.5 \t1042.47    \t3992.85    \t208.122    \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected features: [False False False False False  True False False False False  True False\n",
            " False False False False False False False False False False False False\n",
            " False False  True  True False  True False False False False False False\n",
            " False  True False False False False False False False False False False\n",
            " False]\n",
            "Accuracy with selected features: 0.512\n",
            "Selecting for params: {'population_size': 30, 'mutation_probability': 0.5, 'crossover_probability': 0.3} and scoring: functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200)\n",
            "gen\tnevals\tfitness\tfitness_std\tfitness_max\tfitness_min\n",
            "0  \t30    \t256.498\t648.197    \t1662.93    \t-821.908   \n",
            "1  \t53    \t774.457\t405.386    \t1662.93    \t-156.88    \n",
            "2  \t48    \t1171.76\t275.939    \t1662.93    \t603.072    \n",
            "3  \t52    \t1316.74\t340.268    \t1877.91    \t262.872    \n",
            "4  \t51    \t1484.22\t475.363    \t2647.85    \t262.872    \n",
            "5  \t43    \t1962.89\t433.686    \t2922.94    \t1287.82    \n",
            "6  \t49    \t2198.89\t432.799    \t3172.79    \t1168.07    \n",
            "7  \t55    \t2209.87\t518.967    \t3077.84    \t188.057    \n",
            "8  \t54    \t2559.53\t361.328    \t3097.86    \t2083.01    \n",
            "9  \t45    \t2796.35\t309.404    \t3097.86    \t2257.84    \n",
            "10 \t47    \t2905.2 \t284.637    \t3208.05    \t1897.97    \n",
            "11 \t43    \t3019.54\t200.491    \t3472.81    \t2462.84    \n",
            "12 \t45    \t3116.71\t176.353    \t3472.81    \t2598.08    \n",
            "13 \t49    \t3007.94\t662.525    \t3517.88    \t608.107    \n",
            "14 \t42    \t3193.9 \t507.925    \t3622.72    \t1197.83    \n",
            "15 \t54    \t3074.56\t794.248    \t3622.72    \t48.17      \n",
            "16 \t47    \t3121.83\t839.712    \t3622.72    \t537.939    \n",
            "17 \t45    \t2955.66\t1061.76    \t3793.07    \t492.864    \n",
            "18 \t50    \t2655.37\t1280.43    \t3793.07    \t233.147    \n",
            "19 \t48    \t3302.81\t695.361    \t3793.07    \t953.097    \n",
            "20 \t46    \t3355.63\t763.525    \t3793.07    \t482.93     \n",
            "21 \t47    \t3531.34\t545.615    \t3837.86    \t907.947    \n",
            "22 \t47    \t3628.91\t629.642    \t3847.8     \t267.869    \n",
            "23 \t51    \t3613.75\t588.002    \t4077.86    \t1332.96    \n",
            "24 \t44    \t3736.6 \t562.156    \t4082.91    \t797.777    \n",
            "25 \t45    \t3850.26\t583.812    \t4092.93    \t797.777    \n",
            "26 \t46    \t3897.56\t584.675    \t4082.91    \t807.832    \n",
            "27 \t46    \t3944.4 \t614.896    \t4312.95    \t717.882    \n",
            "28 \t48    \t4114.25\t95.4424    \t4312.95    \t3887.93    \n",
            "29 \t54    \t4117.42\t386.235    \t4312.95    \t2127.64    \n",
            "30 \t54    \t4147.95\t473.766    \t4312.95    \t1663.19    \n",
            "31 \t52    \t4108.45\t646.984    \t4312.95    \t1663.19    \n",
            "32 \t46    \t4312.95\t0          \t4312.95    \t4312.95    \n",
            "33 \t55    \t4225.45\t471.221    \t4312.95    \t1687.84    \n",
            "34 \t50    \t4199.62\t610.319    \t4312.95    \t912.952    \n",
            "35 \t52    \t4095.29\t823.991    \t4312.95    \t563.07     \n",
            "36 \t52    \t4137.28\t657.29     \t4312.95    \t1677.92    \n",
            "37 \t55    \t4165.28\t575.971    \t4312.95    \t1467.89    \n",
            "38 \t49    \t3952.96\t1087.26    \t4312.95    \t193.032    \n",
            "39 \t47    \t3948.3 \t946.159    \t4312.95    \t788.057    \n",
            "40 \t51    \t4123.95\t715.853    \t4312.95    \t1047.8     \n",
            "41 \t50    \t3971.44\t926.261    \t4312.95    \t1047.8     \n",
            "42 \t53    \t3754.78\t1233.21    \t4312.95    \t522.909    \n",
            "43 \t48    \t4013.79\t968.758    \t4312.95    \t388.095    \n",
            "44 \t53    \t3982.29\t983.68     \t4312.95    \t568.022    \n",
            "45 \t46    \t4077.46\t742.301    \t4457.9     \t1297.97    \n",
            "46 \t48    \t4312.95\t0          \t4312.95    \t4312.95    \n",
            "47 \t48    \t3937.29\t1131.42    \t4312.95    \t327.877    \n",
            "48 \t47    \t4194.44\t638.182    \t4312.95    \t757.729    \n",
            "49 \t45    \t4312.95\t0          \t4312.95    \t4312.95    \n",
            "50 \t52    \t3936.46\t1129.55    \t4312.95    \t507.97     \n",
            "51 \t48    \t4207.29\t569.019    \t4312.95    \t1143.03    \n",
            "52 \t48    \t4220.12\t499.921    \t4312.95    \t1527.96    \n",
            "53 \t49    \t4106.45\t778.463    \t4312.95    \t848.117    \n",
            "54 \t48    \t3885.93\t1004.36    \t4312.95    \t737.947    \n",
            "55 \t49    \t4198.62\t615.697    \t4312.95    \t882.99     \n",
            "56 \t48    \t4117.12\t767.737    \t4312.95    \t487.965    \n",
            "57 \t49    \t4150.28\t616.927    \t4312.95    \t1482.9     \n",
            "58 \t50    \t4162.61\t564.999    \t4312.95    \t1852.84    \n",
            "59 \t51    \t4312.95\t0          \t4312.95    \t4312.95    \n",
            "60 \t52    \t4202.29\t595.945    \t4312.95    \t993.025    \n",
            "61 \t45    \t4312.95\t0          \t4312.95    \t4312.95    \n",
            "62 \t48    \t4312.95\t0          \t4312.95    \t4312.95    \n",
            "63 \t46    \t4208.78\t560.985    \t4312.95    \t1187.78    \n",
            "64 \t46    \t4153.45\t718.191    \t4312.95    \t373.035    \n",
            "65 \t55    \t3943.29\t1120.42    \t4312.95    \t132.927    \n",
            "66 \t44    \t3888.61\t1083.27    \t4312.95    \t927.982    \n",
            "67 \t47    \t4230.79\t442.482    \t4312.95    \t1847.95    \n",
            "68 \t42    \t4312.95\t0          \t4312.95    \t4312.95    \n",
            "69 \t46    \t4200.45\t605.829    \t4312.95    \t937.962    \n",
            "70 \t49    \t4164.78\t797.91     \t4312.95    \t-132.093   \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected features: [False False False False False False False False False False False  True\n",
            " False False False False False False False False False False False False\n",
            " False  True  True False False False False False False False  True False\n",
            " False False False False False False False False False False False False\n",
            " False]\n",
            "Accuracy with selected features: 0.493\n",
            "Selecting for params: {'population_size': 30, 'mutation_probability': 0.4, 'crossover_probability': 0.1} and scoring: functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200)\n",
            "gen\tnevals\tfitness\tfitness_std\tfitness_max\tfitness_min\n",
            "0  \t30    \t401.471\t625.071    \t1677.9     \t-1051.94   \n",
            "1  \t34    \t1044.47\t374.574    \t1677.9     \t228.157    \n",
            "2  \t26    \t1239.48\t230.888    \t1657.78    \t558.325    \n",
            "3  \t22    \t1414.25\t158.452    \t1657.78    \t1158       \n",
            "4  \t25    \t1603.88\t85.601     \t1672.97    \t1322.99    \n",
            "5  \t30    \t1593.83\t302.318    \t1892.8     \t162.927    \n",
            "6  \t34    \t1702.82\t165.838    \t2562.96    \t1657.78    \n",
            "7  \t29    \t1767.54\t225.693    \t2562.96    \t1657.78    \n",
            "8  \t36    \t1791.9 \t272.827    \t2562.96    \t728.012    \n",
            "9  \t32    \t1838.23\t256.291    \t2107.89    \t728.012    \n",
            "10 \t36    \t2034.03\t214.867    \t2717.92    \t1717.99    \n",
            "11 \t24    \t2171.88\t228.895    \t2717.92    \t1892.8     \n",
            "12 \t30    \t2228.74\t221.55     \t2717.92    \t1892.8     \n",
            "13 \t28    \t2307.42\t230.376    \t2717.92    \t2107.89    \n",
            "14 \t32    \t2336.97\t397.81     \t2623.08    \t537.909    \n",
            "15 \t30    \t2419.83\t272.229    \t2623.08    \t1482.88    \n",
            "16 \t30    \t2568.38\t134.942    \t2623.08    \t2128       \n",
            "17 \t27    \t2602.74\t93.45      \t2623.08    \t2107.97    \n",
            "18 \t29    \t2639.58\t88.8242    \t3117.91    \t2623.08    \n",
            "19 \t32    \t2582.91\t320.763    \t3117.91    \t923.015    \n",
            "20 \t27    \t2689.06\t168.209    \t3117.91    \t2623.08    \n",
            "21 \t33    \t2804.01\t231.17     \t3117.91    \t2623.08    \n",
            "22 \t33    \t2945.97\t226.744    \t3117.91    \t2623.08    \n",
            "23 \t31    \t3021.93\t338.892    \t3117.91    \t1272.87    \n",
            "24 \t24    \t3036.25\t306.444    \t3117.91    \t1803.05    \n",
            "25 \t30    \t3074.08\t236.025    \t3117.91    \t1803.05    \n",
            "26 \t27    \t3038.25\t299.677    \t3117.91    \t1803.05    \n",
            "27 \t31    \t2998.58\t359.319    \t3117.91    \t1803.05    \n",
            "28 \t32    \t3042.58\t301.55     \t3117.91    \t1572.97    \n",
            "29 \t29    \t3042.58\t301.55     \t3117.91    \t1572.97    \n",
            "30 \t27    \t3019.41\t320.948    \t3117.91    \t1572.97    \n",
            "31 \t27    \t3014.92\t385.377    \t3117.91    \t1572.97    \n",
            "32 \t30    \t3117.91\t1.36424e-12\t3117.91    \t3117.91    \n",
            "33 \t23    \t3117.91\t1.36424e-12\t3117.91    \t3117.91    \n",
            "34 \t30    \t3117.91\t1.36424e-12\t3117.91    \t3117.91    \n",
            "35 \t23    \t3039.41\t422.726    \t3117.91    \t762.964    \n",
            "36 \t35    \t3039.41\t422.726    \t3117.91    \t762.964    \n",
            "37 \t34    \t3058.25\t321.306    \t3117.91    \t1327.96    \n",
            "38 \t33    \t3117.91\t1.36424e-12\t3117.91    \t3117.91    \n",
            "39 \t24    \t3117.91\t1.36424e-12\t3117.91    \t3117.91    \n",
            "40 \t28    \t3117.91\t1.36424e-12\t3117.91    \t3117.91    \n",
            "41 \t25    \t3078.9 \t210.063    \t3117.91    \t1947.68    \n",
            "42 \t35    \t3083.24\t186.692    \t3117.91    \t2077.88    \n",
            "43 \t30    \t3117.91\t1.36424e-12\t3117.91    \t3117.91    \n",
            "44 \t30    \t3117.91\t1.36424e-12\t3117.91    \t3117.91    \n",
            "45 \t33    \t3117.91\t1.36424e-12\t3117.91    \t3117.91    \n",
            "46 \t33    \t3117.91\t1.36424e-12\t3117.91    \t3117.91    \n",
            "47 \t37    \t3061.91\t301.584    \t3117.91    \t1437.83    \n",
            "48 \t25    \t3056.41\t331.185    \t3117.91    \t1272.93    \n",
            "49 \t31    \t3117.91\t1.36424e-12\t3117.91    \t3117.91    \n",
            "50 \t37    \t2846.4 \t659.215    \t3117.91    \t678.08     \n",
            "51 \t33    \t2881.75\t620.408    \t3117.91    \t678.08     \n",
            "52 \t34    \t2886.92\t602.411    \t3117.91    \t833.155    \n",
            "53 \t27    \t3018.75\t371.802    \t3117.91    \t1538       \n",
            "54 \t30    \t2976.25\t426.84     \t3117.91    \t1538       \n",
            "55 \t32    \t3058.91\t317.714    \t3117.91    \t1347.97    \n",
            "56 \t25    \t3022.08\t358.575    \t3117.91    \t1677.9     \n",
            "57 \t31    \t3117.91\t1.36424e-12\t3117.91    \t3117.91    \n",
            "58 \t29    \t3044.25\t396.699    \t3117.91    \t907.955    \n",
            "59 \t27    \t3092.25\t138.204    \t3117.91    \t2347.99    \n",
            "60 \t34    \t3092.25\t138.204    \t3117.91    \t2347.99    \n",
            "61 \t31    \t3022.42\t396.128    \t3117.91    \t1023.08    \n",
            "62 \t33    \t3064.41\t288.103    \t3117.91    \t1512.93    \n",
            "63 \t38    \t3058.07\t246.837    \t3117.91    \t1817.85    \n",
            "64 \t30    \t3061.07\t306.089    \t3117.91    \t1412.73    \n",
            "65 \t27    \t3117.91\t1.36424e-12\t3117.91    \t3117.91    \n",
            "66 \t32    \t3117.91\t1.36424e-12\t3117.91    \t3117.91    \n",
            "67 \t34    \t3077.75\t216.286    \t3117.91    \t1913.01    \n",
            "68 \t27    \t3117.91\t1.36424e-12\t3117.91    \t3117.91    \n",
            "69 \t29    \t3117.91\t1.36424e-12\t3117.91    \t3117.91    \n",
            "70 \t33    \t3117.91\t1.36424e-12\t3117.91    \t3117.91    \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected features: [False  True False False False False False  True False False  True False\n",
            " False False False  True  True False False  True False False False False\n",
            " False  True False False False False  True False False False False  True\n",
            " False False False False  True False False False False False False False\n",
            " False]\n",
            "Accuracy with selected features: 0.52\n",
            "Selecting for params: {'population_size': 30, 'mutation_probability': 0.4, 'crossover_probability': 0.2} and scoring: functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200)\n",
            "gen\tnevals\tfitness\tfitness_std\tfitness_max\tfitness_min\n",
            "0  \t30    \t212.99 \t626.917    \t1583.01    \t-1166.82   \n",
            "1  \t34    \t813.3  \t440.769    \t1583.01    \t-401.955   \n",
            "2  \t36    \t1180.96\t403.894    \t2027.81    \t512.975    \n",
            "3  \t37    \t1357.61\t380.182    \t2027.81    \t537.924    \n",
            "4  \t37    \t1608.61\t304.566    \t2027.81    \t912.967    \n",
            "5  \t39    \t1691.93\t291.679    \t2027.81    \t652.884    \n",
            "6  \t31    \t1734.07\t395.82     \t2292.83    \t652.884    \n",
            "7  \t36    \t1973.36\t434.296    \t2662.83    \t852.952    \n",
            "8  \t34    \t2181.33\t311.8      \t2662.83    \t1652.81    \n",
            "9  \t39    \t2231.15\t265.827    \t2677.89    \t1867.78    \n",
            "10 \t39    \t2389.33\t242.605    \t2677.89    \t2027.81    \n",
            "11 \t42    \t2213.01\t540.024    \t2677.89    \t907.977    \n",
            "12 \t35    \t2540.34\t318.56     \t2913       \t1377.99    \n",
            "13 \t40    \t2675.16\t153.727    \t3232.85    \t2427.78    \n",
            "14 \t34    \t2764.51\t369.475    \t3282.68    \t1132.96    \n",
            "15 \t35    \t2817.67\t475.76     \t3282.68    \t1132.96    \n",
            "16 \t38    \t3080.64\t176.561    \t3282.68    \t2837.83    \n",
            "17 \t42    \t2914.12\t721.106    \t3282.68    \t712.952    \n",
            "18 \t32    \t3235.55\t123.02     \t3497.71    \t2837.83    \n",
            "19 \t32    \t3291.69\t83.1167    \t3497.71    \t3047.84    \n",
            "20 \t42    \t3263.87\t336.538    \t3497.71    \t1518       \n",
            "21 \t40    \t3376.36\t109.286    \t3497.71    \t3157.8     \n",
            "22 \t46    \t3465.21\t111.079    \t3727.74    \t3282.68    \n",
            "23 \t41    \t3469.71\t278.608    \t3727.74    \t2137.66    \n",
            "24 \t37    \t3444.9 \t491.222    \t3727.74    \t1112.87    \n",
            "25 \t38    \t3612.73\t115.015    \t3727.74    \t3497.71    \n",
            "26 \t35    \t3689.41\t85.7271    \t3727.74    \t3497.71    \n",
            "27 \t36    \t3720.08\t41.2917    \t3727.74    \t3497.71    \n",
            "28 \t43    \t3564.92\t611.462    \t3727.74    \t1082.93    \n",
            "29 \t40    \t3594.26\t500.533    \t3727.74    \t1598.08    \n",
            "30 \t37    \t3603.59\t469.285    \t3727.74    \t1607.8     \n",
            "31 \t40    \t3592.26\t507.455    \t3727.74    \t1607.8     \n",
            "32 \t31    \t3597.76\t486.345    \t3727.74    \t1773.02    \n",
            "33 \t33    \t3667.58\t323.974    \t3727.74    \t1922.93    \n",
            "34 \t31    \t3727.74\t9.09495e-13\t3727.74    \t3727.74    \n",
            "35 \t37    \t3727.74\t9.09495e-13\t3727.74    \t3727.74    \n",
            "36 \t42    \t3727.74\t9.09495e-13\t3727.74    \t3727.74    \n",
            "37 \t35    \t3727.74\t9.09495e-13\t3727.74    \t3727.74    \n",
            "38 \t35    \t3727.74\t9.09495e-13\t3727.74    \t3727.74    \n",
            "39 \t33    \t3727.74\t9.09495e-13\t3727.74    \t3727.74    \n",
            "40 \t41    \t3677.25\t271.933    \t3727.74    \t2212.84    \n",
            "41 \t38    \t3727.74\t9.09495e-13\t3727.74    \t3727.74    \n",
            "42 \t35    \t3727.74\t9.09495e-13\t3727.74    \t3727.74    \n",
            "43 \t38    \t3727.74\t9.09495e-13\t3727.74    \t3727.74    \n",
            "44 \t34    \t3727.74\t9.09495e-13\t3727.74    \t3727.74    \n",
            "45 \t35    \t3646.92\t435.269    \t3727.74    \t1302.92    \n",
            "46 \t38    \t3652.58\t404.756    \t3727.74    \t1472.9     \n",
            "47 \t34    \t3727.74\t9.09495e-13\t3727.74    \t3727.74    \n",
            "48 \t40    \t3727.74\t9.09495e-13\t3727.74    \t3727.74    \n",
            "49 \t48    \t3557.26\t570.015    \t3727.74    \t907.872    \n",
            "50 \t38    \t3727.74\t9.09495e-13\t3727.74    \t3727.74    \n",
            "51 \t33    \t3727.74\t9.09495e-13\t3727.74    \t3727.74    \n",
            "52 \t38    \t3665.58\t334.755    \t3727.74    \t1862.87    \n",
            "53 \t36    \t3727.74\t9.09495e-13\t3727.74    \t3727.74    \n",
            "54 \t39    \t3660.92\t359.872    \t3727.74    \t1722.95    \n",
            "55 \t36    \t3727.74\t9.09495e-13\t3727.74    \t3727.74    \n",
            "56 \t42    \t3727.74\t9.09495e-13\t3727.74    \t3727.74    \n",
            "57 \t34    \t3727.74\t9.09495e-13\t3727.74    \t3727.74    \n",
            "58 \t34    \t3727.74\t9.09495e-13\t3727.74    \t3727.74    \n",
            "59 \t36    \t3727.74\t9.09495e-13\t3727.74    \t3727.74    \n",
            "60 \t41    \t3647.74\t430.812    \t3727.74    \t1327.75    \n",
            "61 \t38    \t3584.75\t539.08     \t3727.74    \t1327.75    \n",
            "62 \t41    \t3657.08\t380.526    \t3727.74    \t1607.88    \n",
            "63 \t36    \t3698.75\t156.157    \t3727.74    \t2857.81    \n",
            "64 \t37    \t3727.74\t9.09495e-13\t3727.74    \t3727.74    \n",
            "65 \t38    \t3683.08\t240.537    \t3727.74    \t2387.74    \n",
            "66 \t37    \t3715.24\t67.3128    \t3727.74    \t3352.75    \n",
            "67 \t33    \t3647.75\t430.777    \t3727.74    \t1327.94    \n",
            "68 \t42    \t3727.74\t9.09495e-13\t3727.74    \t3727.74    \n",
            "69 \t33    \t3727.74\t9.09495e-13\t3727.74    \t3727.74    \n",
            "70 \t42    \t3642.09\t461.266    \t3727.74    \t1158.09    \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected features: [False False False False False  True False False False False False False\n",
            " False False False False False False False False False False  True False\n",
            " False False False False  True  True False False False False False False\n",
            " False False False False False False False False False  True False  True\n",
            "  True]\n",
            "Accuracy with selected features: 0.547\n",
            "Selecting for params: {'population_size': 30, 'mutation_probability': 0.4, 'crossover_probability': 0.3} and scoring: functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200)\n",
            "gen\tnevals\tfitness\tfitness_std\tfitness_max\tfitness_min\n",
            "0  \t30    \t359.482\t735.306    \t1707.96    \t-1526.84   \n",
            "1  \t41    \t773.486\t535.294    \t2052.85    \t-342.133   \n",
            "2  \t43    \t1170.47\t378.982    \t2052.85    \t618.095    \n",
            "3  \t43    \t1395.61\t358.018    \t2052.85    \t887.942    \n",
            "4  \t43    \t1583.12\t288.708    \t2122.97    \t1143       \n",
            "5  \t43    \t1901.21\t210.527    \t2122.97    \t1487.87    \n",
            "6  \t46    \t2053.72\t223.704    \t2682.92    \t1598.1     \n",
            "7  \t41    \t2168.05\t218.609    \t2713.01    \t2052.85    \n",
            "8  \t38    \t2297.39\t349.29     \t2713.01    \t1232.82    \n",
            "9  \t40    \t2444.25\t256.809    \t2807.78    \t2052.85    \n",
            "10 \t43    \t2577.6 \t204.661    \t2948.02    \t2052.85    \n",
            "11 \t47    \t2672.58\t140.81     \t2812.81    \t2052.85    \n",
            "12 \t43    \t2694.09\t325.738    \t3182.76    \t1053.11    \n",
            "13 \t30    \t2930.06\t206.052    \t3417.89    \t2677.95    \n",
            "14 \t40    \t2995.73\t243.493    \t3417.89    \t2282.93    \n",
            "15 \t36    \t3065.39\t399.747    \t3417.89    \t1383.04    \n",
            "16 \t35    \t3264.71\t154.647    \t3627.92    \t2972.78    \n",
            "17 \t40    \t3226.54\t457.538    \t3627.92    \t1622.88    \n",
            "18 \t41    \t3482.39\t171.422    \t3627.92    \t3182.76    \n",
            "19 \t44    \t3467.4 \t482.143    \t3627.92    \t958.035    \n",
            "20 \t47    \t3597.74\t73.2567    \t3627.92    \t3392.8     \n",
            "21 \t38    \t3510.92\t438.27     \t3627.92    \t1792.77    \n",
            "22 \t38    \t3627.92\t9.09495e-13\t3627.92    \t3627.92    \n",
            "23 \t39    \t3627.92\t9.09495e-13\t3627.92    \t3627.92    \n",
            "24 \t47    \t3559.93\t366.16     \t3627.92    \t1588.09    \n",
            "25 \t43    \t3568.75\t318.635    \t3627.92    \t1852.85    \n",
            "26 \t39    \t3454.41\t522.573    \t3627.92    \t1737.64    \n",
            "27 \t39    \t3565.08\t444.851    \t3767.8     \t1183.14    \n",
            "28 \t44    \t3665.22\t61.8571    \t3767.8     \t3627.92    \n",
            "29 \t33    \t3717.7 \t118.118    \t4022.91    \t3627.92    \n",
            "30 \t43    \t3778.86\t126.047    \t4022.91    \t3627.92    \n",
            "31 \t47    \t3763.68\t549.621    \t4022.91    \t918.002    \n",
            "32 \t43    \t3970.02\t115.005    \t4177.79    \t3767.8     \n",
            "33 \t38    \t4007.38\t63.6057    \t4177.79    \t3777.84    \n",
            "34 \t42    \t4032.23\t39.0822    \t4177.79    \t4007.85    \n",
            "35 \t47    \t4037.89\t46.7092    \t4177.79    \t4007.85    \n",
            "36 \t42    \t4053.88\t61.9519    \t4177.79    \t4022.91    \n",
            "37 \t43    \t4069.37\t70.9748    \t4177.79    \t4022.91    \n",
            "38 \t46    \t4084.86\t75.8752    \t4177.79    \t4022.91    \n",
            "39 \t46    \t4100.35\t77.4398    \t4177.79    \t4022.91    \n",
            "40 \t43    \t4126.16\t73.011     \t4177.79    \t4022.91    \n",
            "41 \t38    \t4085.47\t385.69     \t4177.79    \t2027.81    \n",
            "42 \t41    \t4177.79\t2.72848e-12\t4177.79    \t4177.79    \n",
            "43 \t43    \t4177.79\t2.72848e-12\t4177.79    \t4177.79    \n",
            "44 \t51    \t3934.97\t768.748    \t4177.79    \t1193.05    \n",
            "45 \t43    \t4089.13\t331.73     \t4177.79    \t2847.91    \n",
            "46 \t41    \t4067.46\t459.868    \t4177.79    \t1738.02    \n",
            "47 \t41    \t4082.46\t513.354    \t4177.79    \t1317.96    \n",
            "48 \t46    \t4177.79\t2.72848e-12\t4177.79    \t4177.79    \n",
            "49 \t42    \t4177.79\t2.72848e-12\t4177.79    \t4177.79    \n",
            "50 \t37    \t4083.29\t508.862    \t4177.79    \t1342.99    \n",
            "51 \t36    \t4177.79\t2.72848e-12\t4177.79    \t4177.79    \n",
            "52 \t44    \t4177.79\t2.72848e-12\t4177.79    \t4177.79    \n",
            "53 \t39    \t4177.79\t2.72848e-12\t4177.79    \t4177.79    \n",
            "54 \t39    \t4177.79\t2.72848e-12\t4177.79    \t4177.79    \n",
            "55 \t43    \t4177.79\t2.72848e-12\t4177.79    \t4177.79    \n",
            "56 \t45    \t4177.79\t2.72848e-12\t4177.79    \t4177.79    \n",
            "57 \t42    \t4177.79\t2.72848e-12\t4177.79    \t4177.79    \n",
            "58 \t42    \t4177.79\t2.72848e-12\t4177.79    \t4177.79    \n",
            "59 \t37    \t4177.79\t2.72848e-12\t4177.79    \t4177.79    \n",
            "60 \t41    \t4074.96\t553.754    \t4177.79    \t1092.9     \n",
            "61 \t47    \t4012.79\t637.456    \t4177.79    \t1087.95    \n",
            "62 \t44    \t4177.79\t2.72848e-12\t4177.79    \t4177.79    \n",
            "63 \t39    \t4109.79\t366.157    \t4177.79    \t2137.98    \n",
            "64 \t42    \t4079.8 \t527.687    \t4177.79    \t1238.11    \n",
            "65 \t44    \t4079.8 \t527.687    \t4177.79    \t1238.11    \n",
            "66 \t32    \t4177.79\t2.72848e-12\t4177.79    \t4177.79    \n",
            "67 \t43    \t4014.63\t612.951    \t4177.79    \t1518.02    \n",
            "68 \t37    \t4105.96\t386.777    \t4177.79    \t2023.11    \n",
            "69 \t41    \t4177.79\t2.72848e-12\t4177.79    \t4177.79    \n",
            "70 \t44    \t4010.45\t636.329    \t4177.79    \t1227.87    \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected features: [False  True False False False False False False False False False False\n",
            " False False False False False False False  True False False False False\n",
            "  True False  True False False False False False False False False False\n",
            " False  True False False False False False False False False False False\n",
            " False]\n",
            "Accuracy with selected features: 0.504\n",
            "Selecting for params: {'population_size': 40, 'mutation_probability': 0.6, 'crossover_probability': 0.1} and scoring: functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200)\n",
            "gen\tnevals\tfitness\tfitness_std\tfitness_max\tfitness_min\n",
            "0  \t40    \t409.852\t736.947    \t1528.03    \t-1761.96   \n",
            "1  \t56    \t1075.17\t508.845    \t2102.9     \t-342.126   \n",
            "2  \t54    \t1430.18\t443.994    \t2102.9     \t332.987    \n",
            "3  \t47    \t1720.08\t345.674    \t2102.93    \t892.985    \n",
            "4  \t60    \t1754.49\t489.195    \t2102.93    \t183.052    \n",
            "5  \t59    \t1906.14\t432.464    \t2102.93    \t282.877    \n",
            "6  \t54    \t1976.94\t389.768    \t2102.93    \t388.057    \n",
            "7  \t49    \t1930.58\t518.392    \t2102.93    \t-477.038   \n",
            "8  \t53    \t1975.79\t436.585    \t2102.93    \t272.889    \n",
            "9  \t64    \t2089.15\t73.0619    \t2183.08    \t1697.92    \n",
            "10 \t58    \t2043.17\t281.28     \t2358.03    \t768.015    \n",
            "11 \t53    \t2140.06\t74.7354    \t2358.03    \t2102.9     \n",
            "12 \t58    \t2062.83\t393.967    \t2358.03    \t368.022    \n",
            "13 \t56    \t2030.76\t489.437    \t2568       \t697.96     \n",
            "14 \t64    \t2149.13\t417.341    \t2568       \t418.065    \n",
            "15 \t57    \t2230.64\t564.389    \t2827.84    \t133.032    \n",
            "16 \t58    \t2336.46\t545.545    \t2827.84    \t418.065    \n",
            "17 \t54    \t2469.54\t498.199    \t2827.84    \t682.93     \n",
            "18 \t56    \t2653.38\t403.666    \t2827.84    \t482.952    \n",
            "19 \t54    \t2426.98\t889.821    \t2827.84    \t-52.2959   \n",
            "20 \t52    \t2462.23\t778.198    \t2827.84    \t218.05     \n",
            "21 \t55    \t2549.97\t694.073    \t2827.84    \t177.995    \n",
            "22 \t60    \t2636.47\t539.905    \t2827.84    \t482.862    \n",
            "23 \t58    \t2655.86\t607.517    \t2827.84    \t203.14     \n",
            "24 \t60    \t2666.35\t486.162    \t2827.84    \t1097.95    \n",
            "25 \t60    \t2748.72\t349.857    \t2827.84    \t982.784    \n",
            "26 \t53    \t2690.11\t493.961    \t2827.84    \t733.017    \n",
            "27 \t52    \t2615.36\t756.947    \t2827.84    \t-586.873   \n",
            "28 \t53    \t2483.88\t853.008    \t2827.84    \t-201.933   \n",
            "29 \t56    \t2440.24\t886.813    \t2827.84    \t-282.096   \n",
            "30 \t53    \t2551.49\t721.761    \t3027.85    \t342.967    \n",
            "31 \t51    \t2567.86\t752.207    \t3207.83    \t-112.058   \n",
            "32 \t53    \t2636.11\t751.64     \t3207.83    \t52.8995    \n",
            "33 \t51    \t2794.97\t591.465    \t3207.83    \t-277.038   \n",
            "34 \t57    \t2566.24\t909.617    \t3207.83    \t-77.0605   \n",
            "35 \t54    \t2717.86\t797.13     \t3207.83    \t728.042    \n",
            "36 \t53    \t2738.87\t965.507    \t3207.83    \t-456.98    \n",
            "37 \t59    \t2823.11\t878.333    \t3207.83    \t208.13     \n",
            "38 \t62    \t2586.01\t1128.93    \t3207.83    \t-522.181   \n",
            "39 \t59    \t2608.12\t1038.6     \t3207.83    \t32.8644    \n",
            "40 \t60    \t2854.35\t800.299    \t3207.83    \t453.175    \n",
            "41 \t54    \t2798.5 \t967.351    \t3207.83    \t-181.868   \n",
            "42 \t56    \t3126.96\t477.039    \t3207.83    \t153.022    \n",
            "43 \t51    \t3002.96\t747.552    \t3472.87    \t153.022    \n",
            "44 \t59    \t2916.72\t912.882    \t3472.87    \t-666.983   \n",
            "45 \t52    \t3059.34\t604.846    \t3472.87    \t612.867    \n",
            "46 \t57    \t3063.84\t561.827    \t3472.87    \t832.857    \n",
            "47 \t61    \t3024.35\t783.53     \t3472.87    \t-122.038   \n",
            "48 \t54    \t3024.97\t810.036    \t3472.87    \t-317.168   \n",
            "49 \t52    \t3136.35\t546.765    \t3472.87    \t322.947    \n",
            "50 \t51    \t2884.11\t967.648    \t3472.87    \t-641.965   \n",
            "51 \t48    \t3113.23\t652.753    \t3472.87    \t522.932    \n",
            "52 \t58    \t3050.6 \t812.03     \t3472.87    \t112.907    \n",
            "53 \t56    \t3075.98\t799.474    \t3472.87    \t247.955    \n",
            "54 \t52    \t3034.11\t870.15     \t3472.87    \t-71.9803   \n",
            "55 \t54    \t2972.88\t931.046    \t3472.87    \t492.909    \n",
            "56 \t55    \t2936.5 \t1090.04    \t3552.75    \t-41.9577   \n",
            "57 \t60    \t2997.01\t1055.99    \t3472.87    \t8.15251    \n",
            "58 \t53    \t3168.99\t827.841    \t3472.87    \t627.799    \n",
            "59 \t55    \t2872.77\t1175.03    \t3472.87    \t-166.838   \n",
            "60 \t59    \t2853.77\t1175.59    \t3472.87    \t-497.065   \n",
            "61 \t51    \t2931.39\t1109.61    \t3472.87    \t263.155    \n",
            "62 \t53    \t3274.88\t697.708    \t3482.91    \t697.884    \n",
            "63 \t60    \t3259.13\t753.514    \t3482.91    \t343.02     \n",
            "64 \t55    \t3351.88\t576.806    \t3687.86    \t772.744    \n",
            "65 \t52    \t3142.52\t983.789    \t3642.9     \t-21.9227   \n",
            "66 \t63    \t2848.4 \t1295.03    \t3642.9     \t-771.88    \n",
            "67 \t56    \t3115.66\t1114.05    \t3642.9     \t-617.038   \n",
            "68 \t58    \t3346.28\t873.502    \t3642.9     \t-152.113   \n",
            "69 \t48    \t3538.53\t518.738    \t3642.9     \t318.032    \n",
            "70 \t56    \t3175.92\t1102.07    \t3642.9     \t-581.875   \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected features: [False False False False False False False False False  True False False\n",
            " False False False False False  True False False  True False False False\n",
            " False False False  True False False  True False False False False False\n",
            " False  True False  True False False False False False False False False\n",
            " False]\n",
            "Accuracy with selected features: 0.539\n",
            "Selecting for params: {'population_size': 40, 'mutation_probability': 0.6, 'crossover_probability': 0.2} and scoring: functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200)\n",
            "gen\tnevals\tfitness\tfitness_std\tfitness_max\tfitness_min\n",
            "0  \t40    \t367.23 \t692.577    \t1593.05    \t-1247.02   \n",
            "1  \t65    \t1018.45\t496.217    \t1897.85    \t-291.993   \n",
            "2  \t63    \t1179.16\t387.294    \t1692.97    \t42.8368    \n",
            "3  \t70    \t1231.13\t425.058    \t1798.07    \t107.947    \n",
            "4  \t67    \t1416.26\t369.626    \t1877.92    \t337.985    \n",
            "5  \t62    \t1670.22\t304.596    \t2257.85    \t502.965    \n",
            "6  \t61    \t1859.46\t549.112    \t2817.71    \t168.015    \n",
            "7  \t67    \t1965.67\t594.624    \t2817.71    \t188.027    \n",
            "8  \t64    \t2149.91\t604.756    \t3187.74    \t158.012    \n",
            "9  \t66    \t2356.92\t646.27     \t3187.74    \t-322.121   \n",
            "10 \t65    \t2161.21\t952.501    \t3187.74    \t-186.865   \n",
            "11 \t61    \t2340.3 \t866.231    \t3187.74    \t-31.9854   \n",
            "12 \t63    \t2401.14\t961.434    \t3187.74    \t123.03     \n",
            "13 \t66    \t2775.38\t763.398    \t3187.74    \t-142.156   \n",
            "14 \t65    \t2942.24\t486.409    \t3382.74    \t542.96     \n",
            "15 \t65    \t2931.28\t744.462    \t3387.74    \t-67.0279   \n",
            "16 \t69    \t2884.27\t936.885    \t3527.64    \t177.995    \n",
            "17 \t59    \t3091.63\t802.259    \t3527.64    \t128.258    \n",
            "18 \t63    \t3194.59\t778.799    \t3802.75    \t332.942    \n",
            "19 \t62    \t3166.35\t1017.52    \t3802.75    \t-421.99    \n",
            "20 \t64    \t3050.87\t1175.02    \t3802.75    \t-646.948   \n",
            "21 \t72    \t3256.48\t975.092    \t3802.75    \t-206.915   \n",
            "22 \t68    \t3167.87\t972.138    \t3802.75    \t357.997    \n",
            "23 \t57    \t3387.5 \t765.13     \t3802.75    \t357.997    \n",
            "24 \t64    \t3426.78\t826.191    \t3802.75    \t357.997    \n",
            "25 \t69    \t3265.81\t1142.51    \t3802.75    \t-31.9327   \n",
            "26 \t53    \t3284.69\t1147.02    \t3802.75    \t357.997    \n",
            "27 \t65    \t3530.53\t742.358    \t3802.75    \t353        \n",
            "28 \t63    \t3329.42\t1223.69    \t3802.75    \t-591.945   \n",
            "29 \t66    \t3653.27\t532.407    \t3802.75    \t1403.12    \n",
            "30 \t60    \t3511.14\t883.909    \t3802.75    \t312.616    \n",
            "31 \t63    \t3612.64\t783.041    \t3802.75    \t57.8594    \n",
            "32 \t66    \t3530.02\t972.748    \t3802.75    \t-297.126   \n",
            "33 \t62    \t3560.52\t863.26     \t3802.75    \t-151.883   \n",
            "34 \t66    \t3747.76\t343.412    \t3802.75    \t1603.16    \n",
            "35 \t61    \t3554.9 \t891.929    \t3802.75    \t-26.9126   \n",
            "36 \t66    \t3484.02\t938.393    \t3802.75    \t208.055    \n",
            "37 \t63    \t3446.28\t1074.27    \t3802.75    \t-941.835   \n",
            "38 \t65    \t3539.39\t931.062    \t3802.75    \t-241.958   \n",
            "39 \t66    \t3714.64\t550.283    \t3802.75    \t278.117    \n",
            "40 \t67    \t3630.77\t749.895    \t3802.75    \t278.117    \n",
            "41 \t63    \t3639.64\t711.193    \t3802.75    \t467.847    \n",
            "42 \t60    \t3398.15\t1076.69    \t3802.75    \t148.01     \n",
            "43 \t70    \t3462.03\t1093.54    \t4147.67    \t-401.933   \n",
            "44 \t63    \t3397.4 \t1075.82    \t3802.75    \t322.962    \n",
            "45 \t66    \t3353.28\t1083.18    \t3802.75    \t352.962    \n",
            "46 \t63    \t3193.54\t1337.09    \t3872.87    \t-191.93    \n",
            "47 \t59    \t3179.04\t1252.59    \t3872.87    \t523.037    \n",
            "48 \t69    \t3326.9 \t1145.77    \t3802.75    \t-216.978   \n",
            "49 \t68    \t3428.14\t922.705    \t3802.75    \t703.017    \n",
            "50 \t62    \t3365.53\t1067.04    \t3802.75    \t338.015    \n",
            "51 \t59    \t3342.91\t1123.39    \t3802.75    \t-431.978   \n",
            "52 \t62    \t3451.65\t952.134    \t3802.75    \t223.077    \n",
            "53 \t64    \t3347.4 \t1153.5     \t3802.75    \t-586.933   \n",
            "54 \t63    \t3260.66\t1192.76    \t3802.75    \t-297.05    \n",
            "55 \t69    \t3407.28\t969.908    \t3802.75    \t553.097    \n",
            "56 \t67    \t3361.53\t1171.09    \t3802.75    \t-16.9779   \n",
            "57 \t71    \t3466.77\t933.907    \t3802.75    \t-16.9779   \n",
            "58 \t62    \t3711.13\t572.158    \t3802.75    \t138.007    \n",
            "59 \t60    \t3456.4 \t1054.03    \t3802.75    \t-571.85    \n",
            "60 \t63    \t3387.53\t1087.16    \t3802.75    \t203.012    \n",
            "61 \t66    \t3239.92\t1226.13    \t3802.75    \t317.942    \n",
            "62 \t63    \t3380.28\t1027.97    \t3802.75    \t453.152    \n",
            "63 \t61    \t3471.9 \t972.492    \t3802.75    \t-241.98    \n",
            "64 \t61    \t3435.4 \t961.144    \t3802.75    \t-241.98    \n",
            "65 \t67    \t3587.64\t802.802    \t3802.75    \t-166.905   \n",
            "66 \t65    \t3413.66\t1082.59    \t3802.75    \t-236.96    \n",
            "67 \t64    \t3343.03\t1095.94    \t3802.75    \t297.945    \n",
            "68 \t63    \t3374.41\t1055.84    \t3802.75    \t-257.048   \n",
            "69 \t68    \t3516.4 \t721.524    \t3802.75    \t558.012    \n",
            "70 \t65    \t3614.9 \t664.809    \t3802.75    \t383        \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected features: [False False False  True False False False False False False False False\n",
            " False False False  True False False False False False False False False\n",
            " False False False False False False  True False False  True False False\n",
            " False False False False False False  True False False False False False\n",
            " False]\n",
            "Accuracy with selected features: 0.523\n",
            "Selecting for params: {'population_size': 40, 'mutation_probability': 0.6, 'crossover_probability': 0.3} and scoring: functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200)\n",
            "gen\tnevals\tfitness\tfitness_std\tfitness_max\tfitness_min\n",
            "0  \t40    \t447.457\t638.924    \t1782.84    \t-796.852   \n",
            "1  \t70    \t918.199\t445.373    \t1782.84    \t-37.013    \n",
            "2  \t75    \t1143.17\t454.276    \t2002.81    \t72.8593    \n",
            "3  \t72    \t1434.15\t474.663    \t2377.82    \t408.055    \n",
            "4  \t74    \t1800.24\t322.992    \t2477.57    \t852.832    \n",
            "5  \t75    \t2020.98\t359.372    \t3417.72    \t912.96     \n",
            "6  \t71    \t2290.37\t515.615    \t3417.72    \t1318.01    \n",
            "7  \t75    \t2269.62\t802.049    \t3882.87    \t-206.908   \n",
            "8  \t71    \t2632.25\t719.66     \t3882.87    \t492.977    \n",
            "9  \t75    \t2510.42\t966.754    \t3882.87    \t-372.095   \n",
            "10 \t73    \t2487.18\t1003.96    \t3487.94    \t32.8118    \n",
            "11 \t75    \t2688.18\t842.951    \t3487.94    \t697.93     \n",
            "12 \t70    \t2966.38\t781.738    \t3502.93    \t512.937    \n",
            "13 \t74    \t3207.61\t661.245    \t3737.95    \t547.957    \n",
            "14 \t75    \t3028.73\t1072.87    \t4092.86    \t-591.99    \n",
            "15 \t73    \t3136.22\t1107.52    \t4092.86    \t-436.877   \n",
            "16 \t73    \t3177.24\t1080.4     \t4092.86    \t242.837    \n",
            "17 \t75    \t3629.63\t461.5      \t4092.86    \t1202.88    \n",
            "18 \t70    \t3625.63\t731.623    \t4277.89    \t198.067    \n",
            "19 \t74    \t3790.12\t474.189    \t4277.89    \t1168.04    \n",
            "20 \t71    \t3375.77\t1312.56    \t4128       \t-946.885   \n",
            "21 \t67    \t3339.16\t1395.38    \t4128       \t-677.008   \n",
            "22 \t75    \t3609.66\t1168.98    \t4128       \t337.977    \n",
            "23 \t73    \t3668.14\t1137.19    \t4128       \t-557.231   \n",
            "24 \t70    \t3963.92\t529.512    \t4128       \t932.972    \n",
            "25 \t69    \t3818.96\t988.239    \t4128       \t-136.815   \n",
            "26 \t70    \t3562.83\t1321.27    \t4128       \t-692.113   \n",
            "27 \t62    \t3657.58\t1179.54    \t4277.89    \t-517.086   \n",
            "28 \t75    \t3866.7 \t1013.52    \t4277.89    \t-682.096   \n",
            "29 \t73    \t3959.33\t828.454    \t4277.89    \t-21.9603   \n",
            "30 \t73    \t3836.45\t1125.36    \t4277.89    \t-61.9552   \n",
            "31 \t70    \t4224.55\t72.8834    \t4277.89    \t4092.86    \n",
            "32 \t77    \t4059.41\t869.351    \t4277.89    \t48.1549    \n",
            "33 \t71    \t4108.4 \t644.085    \t4277.89    \t993.025    \n",
            "34 \t71    \t3604.52\t1483.65    \t4277.89    \t-221.975   \n",
            "35 \t77    \t3183.04\t1650.73    \t4277.89    \t23.1224    \n",
            "36 \t75    \t3316.29\t1531.58    \t4277.89    \t78.1699    \n",
            "37 \t68    \t3702.9 \t1125.84    \t4277.89    \t78.1699    \n",
            "38 \t72    \t3991.01\t921.075    \t4277.89    \t-457.048   \n",
            "39 \t68    \t3980.76\t1002.23    \t4277.89    \t-377.168   \n",
            "40 \t69    \t4248.76\t96.342     \t4277.89    \t3852.88    \n",
            "41 \t70    \t4172.89\t636.755    \t4277.89    \t198.082    \n",
            "42 \t74    \t3788.77\t1305.75    \t4277.89    \t-102.086   \n",
            "43 \t78    \t3866.4 \t1136.75    \t4277.89    \t-31.9854   \n",
            "44 \t71    \t3939.9 \t886.403    \t4277.89    \t763.017    \n",
            "45 \t70    \t4188.39\t293.637    \t4277.89    \t2537.92    \n",
            "46 \t74    \t3871.9 \t1170.3     \t4297.91    \t137.939    \n",
            "47 \t71    \t3864.65\t1132.8     \t4297.91    \t-11.9879   \n",
            "48 \t76    \t3900.78\t1050.45    \t4297.91    \t23.0772    \n",
            "49 \t70    \t4066.02\t782.569    \t4297.91    \t547.965    \n",
            "50 \t67    \t4187.78\t656.076    \t4417.85    \t92.9396    \n",
            "51 \t73    \t3800.42\t1334.72    \t4417.85    \t-557.133   \n",
            "52 \t70    \t3681.54\t1334.98    \t4417.85    \t-87.0404   \n",
            "53 \t75    \t4066.51\t717.64     \t4417.85    \t1562.96    \n",
            "54 \t71    \t4159.63\t801.28     \t4417.85    \t692.894    \n",
            "55 \t73    \t4025.5 \t1112.93    \t4417.85    \t168.022    \n",
            "56 \t71    \t4110.5 \t873.526    \t4417.85    \t713.02     \n",
            "57 \t73    \t3316.26\t1730.03    \t4417.85    \t-296.998   \n",
            "58 \t76    \t3639.26\t1399.98    \t4417.85    \t132.972    \n",
            "59 \t75    \t3264.53\t1741.73    \t4417.85    \t-986.918   \n",
            "60 \t70    \t3776.74\t1324.82    \t4417.85    \t-577.131   \n",
            "61 \t72    \t4098.11\t965.026    \t4417.85    \t652.952    \n",
            "62 \t72    \t4153.1 \t985.113    \t4417.85    \t-2.27596   \n",
            "63 \t71    \t3896.75\t1387.59    \t4417.85    \t-51.9226   \n",
            "64 \t74    \t4334.6 \t519.889    \t4417.85    \t1087.9     \n",
            "65 \t71    \t3861.74\t1497.5     \t4417.85    \t-912.133   \n",
            "66 \t75    \t4001.73\t1193.22    \t4417.85    \t27.7616    \n",
            "67 \t73    \t3959.99\t1387.53    \t4417.85    \t-852.02    \n",
            "68 \t78    \t4136.61\t1005       \t4417.85    \t-26.9578   \n",
            "69 \t74    \t3947.62\t1248.14    \t4417.85    \t193.07     \n",
            "70 \t70    \t4065.86\t1072       \t4417.85    \t462.887    \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected features: [False False  True False False False False False False False False  True\n",
            " False False False False False False False False False False False False\n",
            " False  True False False False False False False False  True False False\n",
            " False False False False False False False False False False False False\n",
            " False]\n",
            "Accuracy with selected features: 0.52\n",
            "Selecting for params: {'population_size': 40, 'mutation_probability': 0.5, 'crossover_probability': 0.1} and scoring: functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200)\n",
            "gen\tnevals\tfitness\tfitness_std\tfitness_max\tfitness_min\n",
            "0  \t40    \t333.755\t571.275    \t1502.87    \t-757.141   \n",
            "1  \t55    \t739.748\t509.586    \t2232.81    \t-531.855   \n",
            "2  \t58    \t1024.57\t399.336    \t2232.81    \t203.087    \n",
            "3  \t33    \t1369.52\t381.825    \t2232.81    \t627.852    \n",
            "4  \t44    \t1729.38\t386.146    \t2232.81    \t1187.75    \n",
            "5  \t53    \t1866.49\t402.445    \t2232.81    \t847.804    \n",
            "6  \t53    \t1843.37\t498.972    \t2232.81    \t758.05     \n",
            "7  \t53    \t1851.6 \t520.857    \t2232.81    \t17.7892    \n",
            "8  \t45    \t2111.2 \t264.971    \t2232.81    \t1332.99    \n",
            "9  \t49    \t2190.19\t185.208    \t2232.81    \t1112.97    \n",
            "10 \t49    \t2153.57\t347.883    \t2252.85    \t507.955    \n",
            "11 \t56    \t2233.31\t3.12797    \t2252.85    \t2232.81    \n",
            "12 \t49    \t2233.81\t4.36654    \t2252.85    \t2232.81    \n",
            "13 \t53    \t2173.44\t271.518    \t2232.81    \t677.857    \n",
            "14 \t55    \t2232.81\t1.81899e-12\t2232.81    \t2232.81    \n",
            "15 \t47    \t2206.56\t163.927    \t2232.81    \t1182.84    \n",
            "16 \t48    \t2212.07\t129.554    \t2232.81    \t1403       \n",
            "17 \t53    \t2058.47\t395.854    \t2232.81    \t758.012    \n",
            "18 \t48    \t2112.45\t328.574    \t2232.81    \t847.887    \n",
            "19 \t47    \t2032.96\t505.521    \t2232.81    \t363.047    \n",
            "20 \t45    \t2173.07\t311.732    \t2422.82    \t288.172    \n",
            "21 \t50    \t2196.44\t223.338    \t2422.82    \t932.927    \n",
            "22 \t45    \t2209.32\t274.394    \t2422.82    \t533.002    \n",
            "23 \t43    \t2152.08\t478.371    \t2422.82    \t-77.0379   \n",
            "24 \t43    \t2285.7 \t267.011    \t2422.82    \t1062.93    \n",
            "25 \t48    \t2368.57\t187.909    \t2682.85    \t1322.92    \n",
            "26 \t48    \t2394.07\t188.432    \t2682.85    \t1322.92    \n",
            "27 \t56    \t2358.83\t304.527    \t2682.85    \t967.992    \n",
            "28 \t51    \t2383.58\t216.282    \t2507.9     \t1082.88    \n",
            "29 \t56    \t2436.33\t80.9572    \t2712.91    \t2232.81    \n",
            "30 \t49    \t2410.59\t336.732    \t2712.91    \t817.797    \n",
            "31 \t47    \t2491.22\t288.145    \t2772.69    \t1232.86    \n",
            "32 \t52    \t2500.74\t361.453    \t2772.69    \t398.052    \n",
            "33 \t49    \t2524.87\t450.146    \t3122.94    \t398.052    \n",
            "34 \t43    \t2737.95\t129.897    \t3122.94    \t2422.82    \n",
            "35 \t52    \t2802.8 \t162.166    \t3282.89    \t2422.82    \n",
            "36 \t52    \t2733.94\t511.451    \t3282.89    \t652.907    \n",
            "37 \t53    \t2953.6 \t183.477    \t3282.89    \t2712.91    \n",
            "38 \t47    \t2924.12\t399.72     \t3282.89    \t1067.87    \n",
            "39 \t58    \t3119.78\t221.781    \t3672.85    \t2317.97    \n",
            "40 \t52    \t2983.79\t732.772    \t3672.85    \t543.012    \n",
            "41 \t48    \t3108.41\t483.489    \t3292.82    \t692.917    \n",
            "42 \t43    \t3133.4 \t544.564    \t3292.82    \t373.012    \n",
            "43 \t49    \t3105.64\t587.989    \t3292.82    \t532.987    \n",
            "44 \t47    \t2976.01\t836.581    \t3292.82    \t532.987    \n",
            "45 \t48    \t3211.5 \t430.149    \t3292.82    \t532.987    \n",
            "46 \t52    \t3125.49\t558.481    \t3292.82    \t1028.04    \n",
            "47 \t47    \t3062.73\t681.529    \t3292.82    \t752.947    \n",
            "48 \t49    \t3012.98\t726.184    \t3292.82    \t847.895    \n",
            "49 \t53    \t3296.96\t58.2372    \t3637.77    \t3162.96    \n",
            "50 \t42    \t3253.95\t398.865    \t3637.77    \t833.08     \n",
            "51 \t56    \t3181.69\t506.873    \t3292.82    \t402.752    \n",
            "52 \t50    \t3112.2 \t638.98     \t3292.82    \t487.919    \n",
            "53 \t49    \t3094.2 \t601.697    \t3292.82    \t1118.02    \n",
            "54 \t53    \t3199.69\t409.991    \t3292.82    \t1172.72    \n",
            "55 \t51    \t3239.82\t331        \t3292.82    \t1172.72    \n",
            "56 \t51    \t3233.32\t371.563    \t3292.82    \t912.914    \n",
            "57 \t44    \t3292.82\t1.36424e-12\t3292.82    \t3292.82    \n",
            "58 \t53    \t3043.96\t748.303    \t3292.82    \t532.697    \n",
            "59 \t42    \t3091.96\t613.804    \t3292.82    \t882.96     \n",
            "60 \t53    \t3199.58\t418.442    \t3292.82    \t982.977    \n",
            "61 \t53    \t3244.2 \t303.63     \t3292.82    \t1348.03    \n",
            "62 \t47    \t3188.95\t453.72     \t3292.82    \t1082.88    \n",
            "63 \t54    \t3083.21\t765.645    \t3557.67    \t158.027    \n",
            "64 \t48    \t3183.19\t523.501    \t3557.67    \t437.832    \n",
            "65 \t47    \t3247.7 \t326.819    \t3557.67    \t1223.11    \n",
            "66 \t45    \t3180.68\t584.097    \t3557.67    \t652.809    \n",
            "67 \t47    \t3036.2 \t767.708    \t3557.67    \t652.809    \n",
            "68 \t47    \t3139.17\t762.957    \t3557.67    \t282.839    \n",
            "69 \t43    \t3330.12\t494.888    \t3557.67    \t1067.85    \n",
            "70 \t57    \t3397.97\t462.525    \t3557.67    \t607.787    \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected features: [False False False False False False False False False False False False\n",
            " False False False False False  True False False False  True False  True\n",
            " False False False False  True False False False False False False False\n",
            "  True False False  True False False  True False False False False False\n",
            "  True]\n",
            "Accuracy with selected features: 0.539\n",
            "Selecting for params: {'population_size': 40, 'mutation_probability': 0.5, 'crossover_probability': 0.2} and scoring: functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200)\n",
            "gen\tnevals\tfitness\tfitness_std\tfitness_max\tfitness_min\n",
            "0  \t40    \t303.625\t639.406    \t1757.97    \t-1181.85   \n",
            "1  \t57    \t796.085\t403.696    \t1757.97    \t208.115    \n",
            "2  \t56    \t1125.44\t384.049    \t1757.97    \t122.992    \n",
            "3  \t53    \t1438.64\t345.517    \t2607.78    \t257.912    \n",
            "4  \t51    \t1614.28\t379.198    \t2607.78    \t273.195    \n",
            "5  \t58    \t1760.15\t323.665    \t2447.8     \t822.824    \n",
            "6  \t56    \t1851.53\t248.616    \t2447.8     \t958.02     \n",
            "7  \t56    \t1911.03\t307.706    \t2447.8     \t862.842    \n",
            "8  \t59    \t2054.23\t431.466    \t2687.54    \t537.932    \n",
            "9  \t58    \t2177.16\t510.915    \t2962.67    \t337.664    \n",
            "10 \t55    \t2426.46\t599.318    \t2962.67    \t337.664    \n",
            "11 \t61    \t2616.05\t399.101    \t2962.67    \t958.027    \n",
            "12 \t55    \t2690.88\t451.76     \t2962.67    \t442.852    \n",
            "13 \t58    \t2884.65\t173.434    \t3137.62    \t2173       \n",
            "14 \t46    \t2970.03\t101.673    \t3137.62    \t2687.54    \n",
            "15 \t56    \t2867.79\t508.462    \t3137.62    \t647.887    \n",
            "16 \t54    \t2978.16\t362.372    \t3222.74    \t788.05     \n",
            "17 \t61    \t2866.07\t669.375    \t3222.74    \t-21.9754   \n",
            "18 \t54    \t3010.3 \t461.473    \t3452.81    \t1032.81    \n",
            "19 \t56    \t3075.69\t373.669    \t3452.81    \t922.754    \n",
            "20 \t57    \t3160.12\t362.789    \t3452.81    \t1418.08    \n",
            "21 \t63    \t2979.54\t759.107    \t3452.81    \t607.862    \n",
            "22 \t58    \t3168.31\t604.006    \t3452.81    \t607.862    \n",
            "23 \t58    \t3252.82\t525.135    \t3452.81    \t1477.81    \n",
            "24 \t58    \t3332.06\t424.045    \t3452.81    \t1842.87    \n",
            "25 \t53    \t3342.19\t413.123    \t3452.81    \t1427.87    \n",
            "26 \t53    \t3433.06\t123.329    \t3452.81    \t2662.87    \n",
            "27 \t50    \t3404.81\t299.776    \t3452.81    \t1532.7     \n",
            "28 \t57    \t3377.81\t468.369    \t3452.81    \t452.847    \n",
            "29 \t51    \t3305.2 \t526.862    \t3452.81    \t1078.15    \n",
            "30 \t55    \t3452.81\t1.81899e-12\t3452.81    \t3452.81    \n",
            "31 \t54    \t3369.19\t364.72     \t3452.81    \t1722.9     \n",
            "32 \t61    \t3409.56\t270.082    \t3452.81    \t1722.9     \n",
            "33 \t55    \t3392.69\t375.46     \t3452.81    \t1047.94    \n",
            "34 \t59    \t3452.81\t1.81899e-12\t3452.81    \t3452.81    \n",
            "35 \t53    \t3341.32\t508.218    \t3452.81    \t558.027    \n",
            "36 \t59    \t3243.33\t650.352    \t3452.81    \t798.052    \n",
            "37 \t55    \t3155.46\t899.384    \t3452.81    \t-47.0079   \n",
            "38 \t53    \t3452.81\t1.81899e-12\t3452.81    \t3452.81    \n",
            "39 \t59    \t3452.81\t1.81899e-12\t3452.81    \t3452.81    \n",
            "40 \t60    \t3279.07\t631.176    \t3452.81    \t403.08     \n",
            "41 \t66    \t3301.94\t677.166    \t3452.81    \t-287.033   \n",
            "42 \t52    \t3322.69\t571.313    \t3452.81    \t543.005    \n",
            "43 \t52    \t3298.32\t673.448    \t3452.81    \t327.99     \n",
            "44 \t57    \t3333.19\t521.411    \t3452.81    \t1057.9     \n",
            "45 \t61    \t3282.7 \t608.456    \t3457.84    \t722.977    \n",
            "46 \t59    \t3341.2 \t460.443    \t3562.72    \t1137.97    \n",
            "47 \t57    \t3395.8 \t429.271    \t3562.72    \t722.872    \n",
            "48 \t51    \t3476.17\t47.4211    \t3617.77    \t3452.81    \n",
            "49 \t53    \t3360.42\t572.282    \t3617.77    \t267.907    \n",
            "50 \t56    \t3291.79\t625.24     \t3617.77    \t822.757    \n",
            "51 \t59    \t3382.65\t577.267    \t3617.77    \t773.02     \n",
            "52 \t58    \t3521.13\t341.261    \t3617.77    \t1403.09    \n",
            "53 \t55    \t3528.25\t437.583    \t3752.64    \t812.837    \n",
            "54 \t53    \t3571.75\t368.244    \t3752.64    \t1297.96    \n",
            "55 \t62    \t3354.36\t984.622    \t3752.64    \t-122.03    \n",
            "56 \t42    \t3572.06\t632.12     \t3752.64    \t327.914    \n",
            "57 \t63    \t3619.16\t521.841    \t3752.64    \t1267.88    \n",
            "58 \t59    \t3608.54\t571.57     \t3752.64    \t842.829    \n",
            "59 \t54    \t3752.64\t4.54747e-13\t3752.64    \t3752.64    \n",
            "60 \t60    \t3752.64\t4.54747e-13\t3752.64    \t3752.64    \n",
            "61 \t51    \t3635.78\t509.445    \t3752.64    \t1387.76    \n",
            "62 \t51    \t3485.67\t826.227    \t3752.64    \t58.1649    \n",
            "63 \t53    \t3684.28\t426.925    \t3752.64    \t1018.13    \n",
            "64 \t63    \t3570.29\t642.825    \t3752.64    \t1112.87    \n",
            "65 \t57    \t3625.53\t583.781    \t3752.64    \t388.042    \n",
            "66 \t59    \t3611.78\t614.531    \t3752.64    \t822.809    \n",
            "67 \t54    \t3672.4 \t501.102    \t3752.64    \t543.02     \n",
            "68 \t58    \t3676.9 \t473.003    \t3752.64    \t723        \n",
            "69 \t56    \t3685.15\t421.491    \t3752.64    \t1052.94    \n",
            "70 \t56    \t3488.17\t820.765    \t3752.64    \t292.894    \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected features: [False False  True False False  True False False False False  True False\n",
            " False False False False False  True False False  True False  True False\n",
            " False False False False False False False False False False False  True\n",
            " False False False False False False False False False False False False\n",
            " False]\n",
            "Accuracy with selected features: 0.555\n",
            "Selecting for params: {'population_size': 40, 'mutation_probability': 0.5, 'crossover_probability': 0.3} and scoring: functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200)\n",
            "gen\tnevals\tfitness\tfitness_std\tfitness_max\tfitness_min\n",
            "0  \t40    \t470.847\t610.856    \t2142.94    \t-666.99    \n",
            "1  \t60    \t926.205\t458.255    \t2142.94    \t-206.968   \n",
            "2  \t61    \t1248.11\t310.818    \t1757.97    \t537.977    \n",
            "3  \t67    \t1418.03\t326.611    \t2037.85    \t398.037    \n",
            "4  \t65    \t1680.35\t317.456    \t2452.85    \t973.072    \n",
            "5  \t68    \t1874.58\t293.944    \t2622.78    \t1097.86    \n",
            "6  \t62    \t2074.16\t332.523    \t2747.7     \t1237.84    \n",
            "7  \t65    \t2194.63\t421.635    \t2747.7     \t832.827    \n",
            "8  \t60    \t2480.74\t296.337    \t2908.01    \t1837.78    \n",
            "9  \t66    \t2581.44\t295.862    \t2967.74    \t1822.77    \n",
            "10 \t59    \t2715.08\t389.702    \t3052.87    \t978.077    \n",
            "11 \t65    \t2845.07\t325.31     \t3362.76    \t1642.86    \n",
            "12 \t60    \t3025.18\t256.624    \t3472.89    \t2037.87    \n",
            "13 \t63    \t3068.52\t365.642    \t3412.83    \t1328.03    \n",
            "14 \t69    \t3170.54\t442.084    \t3607.76    \t1328.03    \n",
            "15 \t63    \t3257.92\t606.964    \t3742.71    \t922.97     \n",
            "16 \t67    \t3461.94\t438.16     \t3902.91    \t1652.89    \n",
            "17 \t63    \t3476.74\t672.706    \t4062.83    \t533.33     \n",
            "18 \t62    \t3495.61\t797.016    \t4102.87    \t312.899    \n",
            "19 \t64    \t3669.49\t453.199    \t4102.87    \t1452.85    \n",
            "20 \t64    \t3618.12\t669.099    \t4237.85    \t1452.85    \n",
            "21 \t60    \t3682.25\t763.479    \t4237.85    \t682.892    \n",
            "22 \t64    \t3896.99\t373.588    \t4237.85    \t1672.86    \n",
            "23 \t64    \t4004.34\t134.099    \t4402.78    \t3832.87    \n",
            "24 \t64    \t3911.56\t728.301    \t4407.87    \t293.022    \n",
            "25 \t67    \t4163.8 \t157.87     \t4502.67    \t3602.83    \n",
            "26 \t65    \t4059.69\t791.976    \t4637.82    \t142.975    \n",
            "27 \t65    \t3956.46\t1034.12    \t4637.82    \t567.724    \n",
            "28 \t60    \t4157.35\t822.662    \t4637.82    \t747.987    \n",
            "29 \t69    \t4300.09\t650.156    \t4637.82    \t747.987    \n",
            "30 \t61    \t4321.2 \t771.235    \t4637.82    \t897.877    \n",
            "31 \t61    \t4394.45\t652.306    \t4772.73    \t1143       \n",
            "32 \t65    \t4599.69\t104.204    \t4772.73    \t4327.72    \n",
            "33 \t65    \t4536.06\t740.823    \t4772.73    \t-61.9401   \n",
            "34 \t66    \t4501.42\t860.181    \t4772.73    \t722.97     \n",
            "35 \t67    \t4547.25\t758.352    \t4772.73    \t722.932    \n",
            "36 \t65    \t4632.75\t646.324    \t4772.73    \t722.932    \n",
            "37 \t64    \t4677.99\t570.445    \t4772.73    \t1117.99    \n",
            "38 \t62    \t4487.62\t996.769    \t4772.73    \t642.837    \n",
            "39 \t63    \t4616.37\t671.952    \t4772.73    \t1328.01    \n",
            "40 \t67    \t4772.73\t1.81899e-12\t4772.73    \t4772.73    \n",
            "41 \t68    \t4584.99\t825.198    \t4772.73    \t542.952    \n",
            "42 \t65    \t4566.99\t897.059    \t4772.73    \t558.057    \n",
            "43 \t68    \t4548.49\t978.05     \t4772.73    \t133.025    \n",
            "44 \t61    \t4571.37\t878.722    \t4772.73    \t558.012    \n",
            "45 \t66    \t4684.48\t551.114    \t4772.73    \t1242.78    \n",
            "46 \t62    \t4695.48\t482.401    \t4772.73    \t1682.89    \n",
            "47 \t67    \t4579.12\t843.942    \t4772.73    \t892.947    \n",
            "48 \t67    \t4551.74\t797.503    \t4772.73    \t1157.95    \n",
            "49 \t63    \t4563.61\t917.229    \t4772.73    \t133.002    \n",
            "50 \t67    \t4409.12\t1104.07    \t4772.73    \t347.965    \n",
            "51 \t61    \t4577.11\t854.767    \t4772.73    \t592.824    \n",
            "52 \t63    \t4650.49\t533.47     \t4772.73    \t2212.82    \n",
            "53 \t68    \t4473.25\t1064.87    \t4772.73    \t-11.9051   \n",
            "54 \t66    \t4675.23\t608.869    \t4772.73    \t872.844    \n",
            "55 \t66    \t4172.13\t1437.3     \t4772.73    \t317.987    \n",
            "56 \t64    \t4191.52\t1384.97    \t4772.73    \t778.07     \n",
            "57 \t62    \t4772.73\t1.81899e-12\t4772.73    \t4772.73    \n",
            "58 \t66    \t4498.25\t972.751    \t4772.73    \t773.035    \n",
            "59 \t61    \t4682.11\t565.907    \t4772.73    \t1148.02    \n",
            "60 \t65    \t4663.49\t682.217    \t4772.73    \t403.042    \n",
            "61 \t67    \t4615.37\t686.351    \t4772.73    \t1517.93    \n",
            "62 \t70    \t4543.36\t1000.96    \t4772.73    \t-31.9854   \n",
            "63 \t63    \t4395.13\t1147.18    \t4772.73    \t532.949    \n",
            "64 \t60    \t4313.64\t1226.66    \t4772.73    \t578.025    \n",
            "65 \t68    \t4187.52\t1302.73    \t4772.73    \t368.007    \n",
            "66 \t62    \t4501.76\t800.773    \t4787.81    \t1243.15    \n",
            "67 \t70    \t4722.25\t236.444    \t4787.81    \t3457.9     \n",
            "68 \t64    \t4598.26\t774.582    \t4787.81    \t1142.94    \n",
            "69 \t61    \t4532.25\t931.834    \t4942.67    \t338        \n",
            "70 \t60    \t4505.76\t879.918    \t4942.67    \t993.13     \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected features: [False False False False False False False False False False False False\n",
            " False False False False False False False False False False False False\n",
            " False False False False False False False False False False False False\n",
            "  True False False False False False False False False False False False\n",
            " False]\n",
            "Accuracy with selected features: 0.517\n",
            "Selecting for params: {'population_size': 40, 'mutation_probability': 0.4, 'crossover_probability': 0.1} and scoring: functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200)\n",
            "gen\tnevals\tfitness\tfitness_std\tfitness_max\tfitness_min\n",
            "0  \t40    \t291.839\t659.691    \t1968.04    \t-1061.98   \n",
            "1  \t41    \t874.955\t519.716    \t1968.04    \t117.919    \n",
            "2  \t53    \t1237.71\t412.493    \t1968.04    \t408.107    \n",
            "3  \t41    \t1614.43\t345.076    \t2352.74    \t912.96     \n",
            "4  \t35    \t1784.95\t282.216    \t2553.01    \t1107.9     \n",
            "5  \t46    \t1911.21\t306.441    \t2553.01    \t707.947    \n",
            "6  \t39    \t2093.93\t220.504    \t2553.01    \t1477.87    \n",
            "7  \t41    \t2170.66\t210.633    \t2553.01    \t1802.74    \n",
            "8  \t41    \t2206.11\t288.051    \t2353.03    \t1143.03    \n",
            "9  \t42    \t2278.32\t204.579    \t2372.78    \t1277.91    \n",
            "10 \t42    \t2307.69\t185.128    \t2372.78    \t1277.91    \n",
            "11 \t37    \t2352.08\t71.7047    \t2587.84    \t1968.04    \n",
            "12 \t49    \t2276.81\t318.972    \t2517.67    \t1173.03    \n",
            "13 \t43    \t2394.38\t89.2878    \t2517.67    \t2148.02    \n",
            "14 \t39    \t2413.61\t185.338    \t2517.67    \t1353.05    \n",
            "15 \t34    \t2485.07\t60.5013    \t2517.67    \t2372.78    \n",
            "16 \t47    \t2476.42\t235.456    \t2517.67    \t1012.8     \n",
            "17 \t39    \t2529.29\t72.6094    \t2982.74    \t2517.67    \n",
            "18 \t42    \t2535.67\t81.915     \t2982.74    \t2517.67    \n",
            "19 \t41    \t2500.43\t155.386    \t2982.74    \t1902.96    \n",
            "20 \t38    \t2562.8 \t127.113    \t2982.74    \t2517.67    \n",
            "21 \t32    \t2590.07\t140.647    \t2982.74    \t2517.67    \n",
            "22 \t37    \t2619.84\t224.315    \t2982.74    \t1717.99    \n",
            "23 \t40    \t2726.62\t315.268    \t2982.74    \t1143.05    \n",
            "24 \t35    \t2805.87\t416.948    \t2982.74    \t842.859    \n",
            "25 \t31    \t2920.12\t221.365    \t2982.74    \t1667.93    \n",
            "26 \t38    \t2999.37\t84.1747    \t3272.84    \t2777.77    \n",
            "27 \t37    \t3011.75\t87.0293    \t3272.84    \t2982.74    \n",
            "28 \t38    \t2859.51\t569.674    \t3272.84    \t168.067    \n",
            "29 \t40    \t2956.64\t312.465    \t3272.84    \t1287.93    \n",
            "30 \t43    \t3052.52\t120.301    \t3272.84    \t2982.74    \n",
            "31 \t45    \t3078.67\t230.185    \t3497.88    \t1973       \n",
            "32 \t40    \t3159.68\t148.389    \t3497.88    \t2982.74    \n",
            "33 \t47    \t3128.2 \t438.212    \t3497.88    \t1427.83    \n",
            "34 \t46    \t3195.71\t324.348    \t3497.88    \t1607.79    \n",
            "35 \t41    \t3191.6 \t476.418    \t3497.88    \t682.975    \n",
            "36 \t33    \t3261.97\t258.916    \t3497.88    \t1713.03    \n",
            "37 \t42    \t3208.47\t480.656    \t3497.88    \t727.944    \n",
            "38 \t41    \t3304.86\t322.635    \t3497.88    \t1403.15    \n",
            "39 \t43    \t3308.61\t314.137    \t3497.88    \t1707.98    \n",
            "40 \t36    \t3441.62\t97.4452    \t3497.88    \t3272.84    \n",
            "41 \t32    \t3475.37\t67.512     \t3497.88    \t3272.84    \n",
            "42 \t42    \t3461.38\t227.939    \t3497.88    \t2037.9     \n",
            "43 \t38    \t3497.88\t1.36424e-12\t3497.88    \t3497.88    \n",
            "44 \t39    \t3497.88\t1.36424e-12\t3497.88    \t3497.88    \n",
            "45 \t43    \t3497.88\t1.36424e-12\t3497.88    \t3497.88    \n",
            "46 \t37    \t3497.88\t1.36424e-12\t3497.88    \t3497.88    \n",
            "47 \t34    \t3450   \t298.998    \t3497.88    \t1582.75    \n",
            "48 \t41    \t3375.51\t548.288    \t3497.88    \t482.914    \n",
            "49 \t36    \t3497.88\t1.36424e-12\t3497.88    \t3497.88    \n",
            "50 \t33    \t3497.88\t1.36424e-12\t3497.88    \t3497.88    \n",
            "51 \t37    \t3497.88\t1.36424e-12\t3497.88    \t3497.88    \n",
            "52 \t34    \t3497.88\t1.36424e-12\t3497.88    \t3497.88    \n",
            "53 \t45    \t3250.38\t695.596    \t3497.88    \t652.877    \n",
            "54 \t39    \t3452.25\t284.925    \t3497.88    \t1672.9     \n",
            "55 \t50    \t3394.88\t449.007    \t3497.88    \t1412.78    \n",
            "56 \t43    \t3497.88\t1.36424e-12\t3497.88    \t3497.88    \n",
            "57 \t35    \t3497.88\t1.36424e-12\t3497.88    \t3497.88    \n",
            "58 \t41    \t3497.88\t1.36424e-12\t3497.88    \t3497.88    \n",
            "59 \t44    \t3368.75\t465.981    \t3497.88    \t1457.9     \n",
            "60 \t37    \t3395.13\t448.408    \t3497.88    \t1343.01    \n",
            "61 \t40    \t3455.75\t263.078    \t3497.88    \t1812.83    \n",
            "62 \t40    \t3497.88\t1.36424e-12\t3497.88    \t3497.88    \n",
            "63 \t49    \t3408.38\t390.133    \t3497.88    \t1687.9     \n",
            "64 \t44    \t3497.88\t1.36424e-12\t3497.88    \t3497.88    \n",
            "65 \t29    \t3497.88\t1.36424e-12\t3497.88    \t3497.88    \n",
            "66 \t48    \t3497.88\t1.36424e-12\t3497.88    \t3497.88    \n",
            "67 \t32    \t3497.88\t1.36424e-12\t3497.88    \t3497.88    \n",
            "68 \t46    \t3398.01\t435.329    \t3497.88    \t1492.99    \n",
            "69 \t37    \t3497.88\t1.36424e-12\t3497.88    \t3497.88    \n",
            "70 \t40    \t3497.88\t1.36424e-12\t3497.88    \t3497.88    \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected features: [False False  True False  True False False False  True False False False\n",
            " False False False False False  True False  True  True False False False\n",
            "  True False False  True False False False False False False False False\n",
            " False False False False False False False False False False False False\n",
            " False]\n",
            "Accuracy with selected features: 0.545\n",
            "Selecting for params: {'population_size': 40, 'mutation_probability': 0.4, 'crossover_probability': 0.2} and scoring: functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200)\n",
            "gen\tnevals\tfitness\tfitness_std\tfitness_max\tfitness_min\n",
            "0  \t40    \t541.981\t633.269    \t1907.98    \t-651.968   \n",
            "1  \t53    \t1107.34\t470.858    \t2177.83    \t142.99     \n",
            "2  \t43    \t1316.55\t342.798    \t1758.07    \t383.097    \n",
            "3  \t44    \t1590.12\t264.36     \t2112.97    \t1072.92    \n",
            "4  \t50    \t1746.22\t236.519    \t2112.97    \t1098       \n",
            "5  \t52    \t1872.75\t269.836    \t2192.79    \t623.077    \n",
            "6  \t53    \t2017.01\t182.841    \t2337.75    \t1732.7     \n",
            "7  \t49    \t2116.01\t214.251    \t2937.69    \t1408.08    \n",
            "8  \t42    \t2171.86\t65.9306    \t2337.75    \t2032.82    \n",
            "9  \t39    \t2177.57\t246.801    \t2552.71    \t737.97     \n",
            "10 \t48    \t2191.17\t218.234    \t2367.72    \t882.892    \n",
            "11 \t47    \t2279.89\t90.0962    \t2602.78    \t2192.79    \n",
            "12 \t49    \t2303.76\t312.864    \t2742.68    \t527.952    \n",
            "13 \t52    \t2377.51\t270.492    \t2602.78    \t887.927    \n",
            "14 \t45    \t2483.13\t137.952    \t2827.86    \t2192.79    \n",
            "15 \t51    \t2524.78\t357.731    \t2827.86    \t408.115    \n",
            "16 \t47    \t2664.04\t117.452    \t2827.86    \t2367.72    \n",
            "17 \t44    \t2732.93\t132.86     \t2967.75    \t2367.72    \n",
            "18 \t49    \t2775.32\t305.19     \t2967.75    \t932.757    \n",
            "19 \t43    \t2853.7 \t89.9031    \t2967.75    \t2538.01    \n",
            "20 \t44    \t2884.06\t68.3442    \t2967.75    \t2827.86    \n",
            "21 \t49    \t2885.04\t173.417    \t2967.75    \t1887.87    \n",
            "22 \t43    \t2920.27\t172.335    \t2967.75    \t1887.87    \n",
            "23 \t42    \t2964.5 \t20.2818    \t2967.75    \t2837.84    \n",
            "24 \t47    \t2967.75\t2.72848e-12\t2967.75    \t2967.75    \n",
            "25 \t47    \t2956.5 \t70.2293    \t2967.75    \t2517.92    \n",
            "26 \t46    \t2967.75\t2.72848e-12\t2967.75    \t2967.75    \n",
            "27 \t49    \t2926.63\t256.8      \t2967.75    \t1322.91    \n",
            "28 \t41    \t2967.75\t2.72848e-12\t2967.75    \t2967.75    \n",
            "29 \t50    \t2967.75\t2.72848e-12\t2967.75    \t2967.75    \n",
            "30 \t46    \t2967.75\t2.72848e-12\t2967.75    \t2967.75    \n",
            "31 \t47    \t2967.75\t2.72848e-12\t2967.75    \t2967.75    \n",
            "32 \t46    \t2961   \t42.1252    \t2967.75    \t2697.93    \n",
            "33 \t48    \t2967.75\t2.72848e-12\t2967.75    \t2967.75    \n",
            "34 \t50    \t2933.37\t214.669    \t2967.75    \t1592.76    \n",
            "35 \t46    \t2937.37\t189.694    \t2967.75    \t1752.73    \n",
            "36 \t55    \t2930.5 \t163.208    \t2967.75    \t2148.01    \n",
            "37 \t48    \t2876.5 \t402.804    \t2967.75    \t857.814    \n",
            "38 \t43    \t2926.75\t256.003    \t2967.75    \t1328.01    \n",
            "39 \t51    \t2857.26\t482.55     \t2967.75    \t623.077    \n",
            "40 \t50    \t2969.75\t12.4966    \t3047.79    \t2967.75    \n",
            "41 \t44    \t2971.75\t17.4448    \t3047.79    \t2967.75    \n",
            "42 \t48    \t2977.75\t26.4716    \t3047.79    \t2967.75    \n",
            "43 \t55    \t2931.52\t273.837    \t3217.77    \t1707.95    \n",
            "44 \t50    \t2986.9 \t302.253    \t3422.83    \t1247.88    \n",
            "45 \t46    \t3082.78\t152.527    \t3422.83    \t2967.75    \n",
            "46 \t50    \t3155.17\t185.22     \t3697.63    \t2963.07    \n",
            "47 \t47    \t3169.93\t360.425    \t3697.63    \t1222.82    \n",
            "48 \t44    \t3228.07\t397.862    \t3422.83    \t937.977    \n",
            "49 \t40    \t3238.21\t564.054    \t3422.83    \t737.924    \n",
            "50 \t53    \t3355.69\t263.302    \t3657.51    \t1947.94    \n",
            "51 \t46    \t3409.39\t301.393    \t3657.51    \t1652.88    \n",
            "52 \t46    \t3498.73\t119.55     \t3657.51    \t3197.82    \n",
            "53 \t52    \t3478.98\t327.049    \t3932.64    \t1652.83    \n",
            "54 \t46    \t3530.98\t459.945    \t3777.76    \t732.995    \n",
            "55 \t41    \t3606.62\t447.33     \t4027.8     \t892.842    \n",
            "56 \t48    \t3706.57\t376.028    \t4027.8     \t1497.89    \n",
            "57 \t44    \t3775.69\t246.416    \t4027.8     \t2532.66    \n",
            "58 \t40    \t3938.04\t132.349    \t4027.8     \t3672.87    \n",
            "59 \t49    \t4014.28\t51.1669    \t4082.61    \t3742.74    \n",
            "60 \t48    \t3963.17\t421.339    \t4082.61    \t1332.96    \n",
            "61 \t49    \t4027.8 \t2.72848e-12\t4027.8     \t4027.8     \n",
            "62 \t46    \t4027.8 \t2.72848e-12\t4027.8     \t4027.8     \n",
            "63 \t46    \t4027.8 \t2.72848e-12\t4027.8     \t4027.8     \n",
            "64 \t41    \t4027.8 \t2.72848e-12\t4027.8     \t4027.8     \n",
            "65 \t47    \t4027.8 \t2.72848e-12\t4027.8     \t4027.8     \n",
            "66 \t47    \t4027.8 \t2.72848e-12\t4027.8     \t4027.8     \n",
            "67 \t50    \t4027.8 \t2.72848e-12\t4027.8     \t4027.8     \n",
            "68 \t44    \t4027.8 \t2.72848e-12\t4027.8     \t4027.8     \n",
            "69 \t43    \t4027.8 \t2.72848e-12\t4027.8     \t4027.8     \n",
            "70 \t46    \t3953.56\t334.295    \t4027.8     \t2167.99    \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected features: [False False False False False False False False  True False False False\n",
            " False False False  True False  True False False False False False False\n",
            " False False False False False False False False  True False False False\n",
            " False False False False False False False  True False False False False\n",
            " False]\n",
            "Accuracy with selected features: 0.511\n",
            "Selecting for params: {'population_size': 40, 'mutation_probability': 0.4, 'crossover_probability': 0.3} and scoring: functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200)\n",
            "gen\tnevals\tfitness\tfitness_std\tfitness_max\tfitness_min\n",
            "0  \t40    \t498.22 \t566.132    \t1527.96    \t-811.868   \n",
            "1  \t56    \t817.833\t372.667    \t1603.11    \t52.8694    \n",
            "2  \t58    \t1235.31\t386.984    \t2332.91    \t592.994    \n",
            "3  \t60    \t1580.41\t367.819    \t2492.94    \t877.902    \n",
            "4  \t53    \t1779.31\t363.305    \t2492.94    \t1052.81    \n",
            "5  \t60    \t2118.3 \t333.441    \t2882.97    \t1517.67    \n",
            "6  \t53    \t2309.82\t279.442    \t3103.01    \t1622.82    \n",
            "7  \t59    \t2410.44\t334.952    \t3103.01    \t1482.89    \n",
            "8  \t58    \t2528.33\t369.966    \t3103.01    \t1312.97    \n",
            "9  \t55    \t2691.57\t243.561    \t3103.01    \t2148.01    \n",
            "10 \t61    \t2793.2 \t445.317    \t3437.86    \t872.935    \n",
            "11 \t56    \t3016.44\t286.547    \t3437.86    \t2252.91    \n",
            "12 \t61    \t3085.46\t530.845    \t3788.11    \t1042.85    \n",
            "13 \t58    \t3394.23\t425.942    \t3857.88    \t1672.96    \n",
            "14 \t59    \t3619.1 \t190.679    \t3857.88    \t3122.98    \n",
            "15 \t56    \t3724.22\t183.859    \t4118.04    \t3242.87    \n",
            "16 \t58    \t3542.51\t798.524    \t4118.04    \t712.96     \n",
            "17 \t57    \t3822.28\t345.603    \t4173.12    \t1973.06    \n",
            "18 \t59    \t3855.9 \t487.279    \t4277.95    \t1032.85    \n",
            "19 \t64    \t3813.03\t726.43     \t4277.95    \t968.097    \n",
            "20 \t59    \t4064.17\t244.879    \t4277.95    \t2698       \n",
            "21 \t58    \t4122.55\t247.451    \t4293.01    \t2698       \n",
            "22 \t54    \t4177.54\t245.222    \t4338.08    \t2698       \n",
            "23 \t50    \t4192.37\t479.999    \t4462.95    \t1212.81    \n",
            "24 \t62    \t4113.23\t620.118    \t4632.95    \t1752.71    \n",
            "25 \t55    \t4302.25\t79.9115    \t4632.95    \t4123.01    \n",
            "26 \t55    \t4365.74\t108.704    \t4632.95    \t4173.12    \n",
            "27 \t56    \t4373.72\t457.808    \t4632.95    \t1602.87    \n",
            "28 \t57    \t4356.46\t687.693    \t4632.95    \t1122.98    \n",
            "29 \t49    \t4471.95\t575.165    \t4632.95    \t1907.95    \n",
            "30 \t52    \t4473.94\t615.852    \t4632.95    \t1327.91    \n",
            "31 \t45    \t4547.19\t535.533    \t4632.95    \t1202.79    \n",
            "32 \t56    \t4565.07\t423.882    \t4632.95    \t1917.93    \n",
            "33 \t56    \t4632.95\t3.63798e-12\t4632.95    \t4632.95    \n",
            "34 \t50    \t4565.45\t421.528    \t4632.95    \t1933.01    \n",
            "35 \t58    \t4521.7 \t503.143    \t4632.95    \t1807.85    \n",
            "36 \t58    \t4513.94\t524.103    \t4632.95    \t1917.93    \n",
            "37 \t51    \t4565.82\t419.188    \t4632.95    \t1947.99    \n",
            "38 \t55    \t4380.82\t885.803    \t4632.95    \t1183.06    \n",
            "39 \t59    \t4581.07\t323.949    \t4632.95    \t2558.01    \n",
            "40 \t61    \t4313.82\t862.288    \t4632.95    \t1517.96    \n",
            "41 \t56    \t4559.57\t458.219    \t4632.95    \t1698       \n",
            "42 \t57    \t4569.95\t393.435    \t4632.95    \t2112.94    \n",
            "43 \t58    \t4632.95\t3.63798e-12\t4632.95    \t4632.95    \n",
            "44 \t48    \t4599.7 \t207.64     \t4632.95    \t3302.99    \n",
            "45 \t64    \t4506.82\t549.839    \t4632.95    \t2067.95    \n",
            "46 \t58    \t4571.7 \t382.482    \t4632.95    \t2183.1     \n",
            "47 \t56    \t4549.95\t518.334    \t4632.95    \t1312.95    \n",
            "48 \t49    \t4552.69\t400.275    \t4632.95    \t2157.82    \n",
            "49 \t57    \t4614.57\t114.746    \t4632.95    \t3897.98    \n",
            "50 \t63    \t4632.95\t3.63798e-12\t4632.95    \t4632.95    \n",
            "51 \t59    \t4632.95\t3.63798e-12\t4632.95    \t4632.95    \n",
            "52 \t49    \t4555.57\t483.206    \t4632.95    \t1537.95    \n",
            "53 \t51    \t4632.95\t3.63798e-12\t4632.95    \t4632.95    \n",
            "54 \t53    \t4632.95\t3.63798e-12\t4632.95    \t4632.95    \n",
            "55 \t62    \t4632.95\t3.63798e-12\t4632.95    \t4632.95    \n",
            "56 \t61    \t4574.82\t362.991    \t4632.95    \t2307.95    \n",
            "57 \t63    \t4564.07\t430.14     \t4632.95    \t1877.85    \n",
            "58 \t59    \t4506.57\t551.427    \t4632.95    \t1993.05    \n",
            "59 \t54    \t4632.95\t3.63798e-12\t4632.95    \t4632.95    \n",
            "60 \t57    \t4533.94\t449.974    \t4632.95    \t2082.92    \n",
            "61 \t60    \t4632.95\t3.63798e-12\t4632.95    \t4632.95    \n",
            "62 \t48    \t4546.95\t537.054    \t4632.95    \t1193.05    \n",
            "63 \t56    \t4546.82\t537.836    \t4632.95    \t1188.04    \n",
            "64 \t62    \t4490.44\t621.184    \t4632.95    \t1782.76    \n",
            "65 \t52    \t4512.19\t526.375    \t4632.95    \t2217.77    \n",
            "66 \t51    \t4540.82\t575.354    \t4632.95    \t947.734    \n",
            "67 \t55    \t4480.83\t668.554    \t4632.95    \t1208.13    \n",
            "68 \t51    \t4533.07\t623.738    \t4632.95    \t637.824    \n",
            "69 \t60    \t4423.32\t736.266    \t4632.95    \t1802.77    \n",
            "70 \t57    \t4565.32\t341.1      \t4632.95    \t2512.91    \n",
            "Selected features: [False False False False False False False False False False False False\n",
            " False False False False False False False False False False False False\n",
            " False False  True False False  True False False False False  True False\n",
            " False False False False False False False False False False False False\n",
            " False]\n",
            "Accuracy with selected features: 0.509\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        }
      ],
      "source": [
        "from functools import partial\n",
        "\n",
        "params_list = []\n",
        "\n",
        "for population_size in [20, 30, 40]:\n",
        "  for mutation_probability in [0.6, 0.5, 0.4]:\n",
        "    for crossover_probability in [0.1, 0.2, 0.3]:\n",
        "      params_list.append({'population_size': population_size, 'mutation_probability': mutation_probability, 'crossover_probability': crossover_probability})\n",
        "\n",
        "scoring_list = [partial(calculate_score, reward=10, punishment=200)]\n",
        "\n",
        "results = []\n",
        "\n",
        "for params in params_list:\n",
        "  for scoring in scoring_list:\n",
        "    estimator = LogisticRegression()\n",
        "    selector = GAFeatureSelectionCV(\n",
        "        estimator=estimator,\n",
        "        cv=3,\n",
        "        scoring=scoring,\n",
        "        generations=70,\n",
        "        elitism=True,\n",
        "        n_jobs=-1,\n",
        "        verbose=2,\n",
        "        keep_top_k=8,\n",
        "        criteria=\"max\",\n",
        "        **params\n",
        "    )\n",
        "    print(f'Selecting for params: {params} and scoring: {scoring}')\n",
        "    # Fit the genetic algorithm feature selector\n",
        "    selector.fit(X_train, np.ravel(y_train))\n",
        "    best_features = selector.support_\n",
        "\n",
        "    # Transform the dataset to keep only the best features\n",
        "    X_train_selected = selector.transform(X_train)\n",
        "    X_test_selected = selector.transform(X_test)\n",
        "    estimator.fit(X_train_selected, y_train)\n",
        "    accuracy = estimator.score(X_test_selected, y_test)\n",
        "    results.append({'params': params, 'scoring': scoring, 'features': best_features, 'test_accuracy': accuracy,\n",
        "                    'test_score': calculate_score(estimator, X_test_selected, np.ravel(y_test))})\n",
        "\n",
        "    print(\"Selected features:\", best_features)\n",
        "    print(\"Accuracy with selected features:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TjA0xIgQn6be",
        "outputId": "c1f59c71-6f60-4da0-b143-2bb049bab867"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'params': {'population_size': 20,\n",
              "   'mutation_probability': 0.6,\n",
              "   'crossover_probability': 0.1},\n",
              "  'scoring': functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200),\n",
              "  'features': array([ True, False, False, False, False, False, False, False, False,\n",
              "          True, False, False,  True, False, False, False,  True, False,\n",
              "          True,  True, False, False, False,  True,  True, False, False,\n",
              "         False,  True, False, False,  True,  True, False,  True, False,\n",
              "         False, False, False, False, False, False, False, False, False,\n",
              "         False, False,  True, False]),\n",
              "  'test_accuracy': 0.524,\n",
              "  'test_score': 2650.501002004007},\n",
              " {'params': {'population_size': 20,\n",
              "   'mutation_probability': 0.6,\n",
              "   'crossover_probability': 0.2},\n",
              "  'scoring': functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200),\n",
              "  'features': array([False, False,  True, False, False, False, False, False, False,\n",
              "          True,  True, False,  True, False, False, False, False, False,\n",
              "          True, False, False,  True, False, False, False, False, False,\n",
              "         False, False,  True, False, False, False, False, False, False,\n",
              "         False, False, False,  True, False, False, False, False, False,\n",
              "         False, False, False, False]),\n",
              "  'test_accuracy': 0.51,\n",
              "  'test_score': 3470.1402805611215},\n",
              " {'params': {'population_size': 20,\n",
              "   'mutation_probability': 0.6,\n",
              "   'crossover_probability': 0.3},\n",
              "  'scoring': functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200),\n",
              "  'features': array([False, False, False, False, False, False, False,  True, False,\n",
              "         False,  True,  True, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False,\n",
              "         False,  True, False, False, False, False, False, False,  True,\n",
              "          True,  True, False, False,  True, False, False, False, False,\n",
              "         False, False, False, False]),\n",
              "  'test_accuracy': 0.522,\n",
              "  'test_score': 3590.3807615230453},\n",
              " {'params': {'population_size': 20,\n",
              "   'mutation_probability': 0.5,\n",
              "   'crossover_probability': 0.1},\n",
              "  'scoring': functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200),\n",
              "  'features': array([ True, False,  True,  True,  True, False, False,  True, False,\n",
              "          True, False, False, False, False, False, False,  True, False,\n",
              "         False, False, False,  True, False, False, False, False, False,\n",
              "         False, False, False,  True, False,  True, False,  True, False,\n",
              "         False, False, False, False, False,  True,  True, False, False,\n",
              "         False, False, False, False]),\n",
              "  'test_accuracy': 0.524,\n",
              "  'test_score': 2610.4208416833662},\n",
              " {'params': {'population_size': 20,\n",
              "   'mutation_probability': 0.5,\n",
              "   'crossover_probability': 0.2},\n",
              "  'scoring': functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200),\n",
              "  'features': array([ True, False, False, False,  True,  True, False,  True, False,\n",
              "         False,  True, False, False, False, False,  True, False, False,\n",
              "         False, False, False, False, False,  True, False, False,  True,\n",
              "          True, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False,  True]),\n",
              "  'test_accuracy': 0.533,\n",
              "  'test_score': 3350.701402805611},\n",
              " {'params': {'population_size': 20,\n",
              "   'mutation_probability': 0.5,\n",
              "   'crossover_probability': 0.3},\n",
              "  'scoring': functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200),\n",
              "  'features': array([False, False, False, False,  True, False, False, False, False,\n",
              "          True, False, False, False,  True, False, False, False, False,\n",
              "         False, False, False, False,  True, False, False, False, False,\n",
              "         False, False, False, False, False, False,  True, False, False,\n",
              "         False, False, False, False,  True, False, False, False, False,\n",
              "         False,  True, False, False]),\n",
              "  'test_accuracy': 0.541,\n",
              "  'test_score': 3930.66132264529},\n",
              " {'params': {'population_size': 20,\n",
              "   'mutation_probability': 0.4,\n",
              "   'crossover_probability': 0.1},\n",
              "  'scoring': functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200),\n",
              "  'features': array([ True, False, False, False, False, False, False, False, False,\n",
              "          True, False, False, False, False, False, False, False, False,\n",
              "         False, False, False,  True, False, False, False,  True,  True,\n",
              "         False,  True, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False,\n",
              "          True,  True, False, False]),\n",
              "  'test_accuracy': 0.489,\n",
              "  'test_score': 3329.8597194388776},\n",
              " {'params': {'population_size': 20,\n",
              "   'mutation_probability': 0.4,\n",
              "   'crossover_probability': 0.2},\n",
              "  'scoring': functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200),\n",
              "  'features': array([ True, False, False, False, False,  True,  True, False,  True,\n",
              "         False, False,  True,  True, False,  True, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False]),\n",
              "  'test_accuracy': 0.506,\n",
              "  'test_score': 3630.0601202404805},\n",
              " {'params': {'population_size': 20,\n",
              "   'mutation_probability': 0.4,\n",
              "   'crossover_probability': 0.3},\n",
              "  'scoring': functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200),\n",
              "  'features': array([ True,  True, False, False, False, False, False, False, False,\n",
              "         False, False,  True, False, False, False, False, False, False,\n",
              "         False, False, False, False, False,  True, False, False, False,\n",
              "         False, False,  True, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False]),\n",
              "  'test_accuracy': 0.511,\n",
              "  'test_score': 4110.220440881763},\n",
              " {'params': {'population_size': 30,\n",
              "   'mutation_probability': 0.6,\n",
              "   'crossover_probability': 0.1},\n",
              "  'scoring': functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200),\n",
              "  'features': array([False, False, False, False, False, False,  True, False, False,\n",
              "          True, False, False,  True,  True, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False,  True,\n",
              "         False, False,  True, False,  True,  True,  True,  True, False,\n",
              "         False, False, False, False, False,  True, False,  True, False,\n",
              "         False, False, False, False]),\n",
              "  'test_accuracy': 0.528,\n",
              "  'test_score': 2870.541082164328},\n",
              " {'params': {'population_size': 30,\n",
              "   'mutation_probability': 0.6,\n",
              "   'crossover_probability': 0.2},\n",
              "  'scoring': functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200),\n",
              "  'features': array([False, False, False, False, False, False,  True, False, False,\n",
              "         False, False,  True,  True,  True,  True, False, False, False,\n",
              "         False, False, False, False, False, False,  True, False, False,\n",
              "         False, False, False,  True, False, False, False, False, False,\n",
              "         False, False, False, False, False, False,  True, False, False,\n",
              "         False, False, False, False]),\n",
              "  'test_accuracy': 0.534,\n",
              "  'test_score': 3750.701402805611},\n",
              " {'params': {'population_size': 30,\n",
              "   'mutation_probability': 0.6,\n",
              "   'crossover_probability': 0.3},\n",
              "  'scoring': functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200),\n",
              "  'features': array([False, False, False, False, False, False,  True, False, False,\n",
              "         False, False,  True, False, False, False, False,  True, False,\n",
              "         False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False,  True, False,  True,\n",
              "         False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False]),\n",
              "  'test_accuracy': 0.53,\n",
              "  'test_score': 4290.581162324649},\n",
              " {'params': {'population_size': 30,\n",
              "   'mutation_probability': 0.5,\n",
              "   'crossover_probability': 0.1},\n",
              "  'scoring': functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200),\n",
              "  'features': array([ True, False, False, False, False, False,  True, False, False,\n",
              "          True,  True, False, False, False,  True, False, False,  True,\n",
              "         False, False,  True, False,  True, False, False, False, False,\n",
              "          True, False, False, False, False, False,  True, False, False,\n",
              "         False, False, False, False, False,  True, False, False, False,\n",
              "         False, False, False, False]),\n",
              "  'test_accuracy': 0.553,\n",
              "  'test_score': 3311.022044088176},\n",
              " {'params': {'population_size': 30,\n",
              "   'mutation_probability': 0.5,\n",
              "   'crossover_probability': 0.2},\n",
              "  'scoring': functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200),\n",
              "  'features': array([False, False, False, False, False,  True, False, False, False,\n",
              "         False,  True, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False,  True,\n",
              "          True, False,  True, False, False, False, False, False, False,\n",
              "         False,  True, False, False, False, False, False, False, False,\n",
              "         False, False, False, False]),\n",
              "  'test_accuracy': 0.512,\n",
              "  'test_score': 3930.2605210420834},\n",
              " {'params': {'population_size': 30,\n",
              "   'mutation_probability': 0.5,\n",
              "   'crossover_probability': 0.3},\n",
              "  'scoring': functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200),\n",
              "  'features': array([False, False, False, False, False, False, False, False, False,\n",
              "         False, False,  True, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False,  True,  True,\n",
              "         False, False, False, False, False, False, False,  True, False,\n",
              "         False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False]),\n",
              "  'test_accuracy': 0.493,\n",
              "  'test_score': 4169.939879759519},\n",
              " {'params': {'population_size': 30,\n",
              "   'mutation_probability': 0.4,\n",
              "   'crossover_probability': 0.1},\n",
              "  'scoring': functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200),\n",
              "  'features': array([False,  True, False, False, False, False, False,  True, False,\n",
              "         False,  True, False, False, False, False,  True,  True, False,\n",
              "         False,  True, False, False, False, False, False,  True, False,\n",
              "         False, False, False,  True, False, False, False, False,  True,\n",
              "         False, False, False, False,  True, False, False, False, False,\n",
              "         False, False, False, False]),\n",
              "  'test_accuracy': 0.52,\n",
              "  'test_score': 3150.3006012024043},\n",
              " {'params': {'population_size': 30,\n",
              "   'mutation_probability': 0.4,\n",
              "   'crossover_probability': 0.2},\n",
              "  'scoring': functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200),\n",
              "  'features': array([False, False, False, False, False,  True, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False,  True, False, False, False, False,\n",
              "         False,  True,  True, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False,\n",
              "          True, False,  True,  True]),\n",
              "  'test_accuracy': 0.547,\n",
              "  'test_score': 4030.861723446893},\n",
              " {'params': {'population_size': 30,\n",
              "   'mutation_probability': 0.4,\n",
              "   'crossover_probability': 0.3},\n",
              "  'scoring': functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200),\n",
              "  'features': array([False,  True, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False,\n",
              "         False,  True, False, False, False, False,  True, False,  True,\n",
              "         False, False, False, False, False, False, False, False, False,\n",
              "         False,  True, False, False, False, False, False, False, False,\n",
              "         False, False, False, False]),\n",
              "  'test_accuracy': 0.504,\n",
              "  'test_score': 4130.260521042083},\n",
              " {'params': {'population_size': 40,\n",
              "   'mutation_probability': 0.6,\n",
              "   'crossover_probability': 0.1},\n",
              "  'scoring': functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200),\n",
              "  'features': array([False, False, False, False, False, False, False, False, False,\n",
              "          True, False, False, False, False, False, False, False,  True,\n",
              "         False, False,  True, False, False, False, False, False, False,\n",
              "          True, False, False,  True, False, False, False, False, False,\n",
              "         False,  True, False,  True, False, False, False, False, False,\n",
              "         False, False, False, False]),\n",
              "  'test_accuracy': 0.539,\n",
              "  'test_score': 3890.581162324649},\n",
              " {'params': {'population_size': 40,\n",
              "   'mutation_probability': 0.6,\n",
              "   'crossover_probability': 0.2},\n",
              "  'scoring': functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200),\n",
              "  'features': array([False, False, False,  True, False, False, False, False, False,\n",
              "         False, False, False, False, False, False,  True, False, False,\n",
              "         False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False,  True, False, False,  True, False, False,\n",
              "         False, False, False, False, False, False,  True, False, False,\n",
              "         False, False, False, False]),\n",
              "  'test_accuracy': 0.523,\n",
              "  'test_score': 4210.420841683366},\n",
              " {'params': {'population_size': 40,\n",
              "   'mutation_probability': 0.6,\n",
              "   'crossover_probability': 0.3},\n",
              "  'scoring': functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200),\n",
              "  'features': array([False, False,  True, False, False, False, False, False, False,\n",
              "         False, False,  True, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False,  True, False,\n",
              "         False, False, False, False, False, False,  True, False, False,\n",
              "         False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False]),\n",
              "  'test_accuracy': 0.52,\n",
              "  'test_score': 4330.260521042083},\n",
              " {'params': {'population_size': 40,\n",
              "   'mutation_probability': 0.5,\n",
              "   'crossover_probability': 0.1},\n",
              "  'scoring': functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200),\n",
              "  'features': array([False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False,  True,\n",
              "         False, False, False,  True, False,  True, False, False, False,\n",
              "         False,  True, False, False, False, False, False, False, False,\n",
              "          True, False, False,  True, False, False,  True, False, False,\n",
              "         False, False, False,  True]),\n",
              "  'test_accuracy': 0.539,\n",
              "  'test_score': 3850.901803607214},\n",
              " {'params': {'population_size': 40,\n",
              "   'mutation_probability': 0.5,\n",
              "   'crossover_probability': 0.2},\n",
              "  'scoring': functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200),\n",
              "  'features': array([False, False,  True, False, False,  True, False, False, False,\n",
              "         False,  True, False, False, False, False, False, False,  True,\n",
              "         False, False,  True, False,  True, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False,  True,\n",
              "         False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False]),\n",
              "  'test_accuracy': 0.555,\n",
              "  'test_score': 4131.062124248497},\n",
              " {'params': {'population_size': 40,\n",
              "   'mutation_probability': 0.5,\n",
              "   'crossover_probability': 0.3},\n",
              "  'scoring': functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200),\n",
              "  'features': array([False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False,\n",
              "          True, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False]),\n",
              "  'test_accuracy': 0.517,\n",
              "  'test_score': 4950.300601202404},\n",
              " {'params': {'population_size': 40,\n",
              "   'mutation_probability': 0.4,\n",
              "   'crossover_probability': 0.1},\n",
              "  'scoring': functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200),\n",
              "  'features': array([False, False,  True, False,  True, False, False, False,  True,\n",
              "         False, False, False, False, False, False, False, False,  True,\n",
              "         False,  True,  True, False, False, False,  True, False, False,\n",
              "          True, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False]),\n",
              "  'test_accuracy': 0.545,\n",
              "  'test_score': 3830.861723446893},\n",
              " {'params': {'population_size': 40,\n",
              "   'mutation_probability': 0.4,\n",
              "   'crossover_probability': 0.2},\n",
              "  'scoring': functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200),\n",
              "  'features': array([False, False, False, False, False, False, False, False,  True,\n",
              "         False, False, False, False, False, False,  True, False,  True,\n",
              "         False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False,  True, False, False, False,\n",
              "         False, False, False, False, False, False, False,  True, False,\n",
              "         False, False, False, False]),\n",
              "  'test_accuracy': 0.511,\n",
              "  'test_score': 4150.300601202404},\n",
              " {'params': {'population_size': 40,\n",
              "   'mutation_probability': 0.4,\n",
              "   'crossover_probability': 0.3},\n",
              "  'scoring': functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200),\n",
              "  'features': array([False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False,  True,\n",
              "         False, False,  True, False, False, False, False,  True, False,\n",
              "         False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False]),\n",
              "  'test_accuracy': 0.509,\n",
              "  'test_score': 4470.1402805611215}]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'params': {'population_size': 20,\n",
              "   'mutation_probability': 0.6,\n",
              "   'crossover_probability': 0.1},\n",
              "  'scoring': functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200),\n",
              "  'features': array([ True, False, False, False, False, False, False, False, False,\n",
              "          True, False, False,  True, False, False, False,  True, False,\n",
              "          True,  True, False, False, False,  True,  True, False, False,\n",
              "         False,  True, False, False,  True,  True, False,  True, False,\n",
              "         False, False, False, False, False, False, False, False, False,\n",
              "         False, False,  True, False]),\n",
              "  'test_accuracy': 0.524,\n",
              "  'test_score': 2650.501002004007},\n",
              " {'params': {'population_size': 20,\n",
              "   'mutation_probability': 0.6,\n",
              "   'crossover_probability': 0.2},\n",
              "  'scoring': functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200),\n",
              "  'features': array([False, False,  True, False, False, False, False, False, False,\n",
              "          True,  True, False,  True, False, False, False, False, False,\n",
              "          True, False, False,  True, False, False, False, False, False,\n",
              "         False, False,  True, False, False, False, False, False, False,\n",
              "         False, False, False,  True, False, False, False, False, False,\n",
              "         False, False, False, False]),\n",
              "  'test_accuracy': 0.51,\n",
              "  'test_score': 3470.1402805611215},\n",
              " {'params': {'population_size': 20,\n",
              "   'mutation_probability': 0.6,\n",
              "   'crossover_probability': 0.3},\n",
              "  'scoring': functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200),\n",
              "  'features': array([False, False, False, False, False, False, False,  True, False,\n",
              "         False,  True,  True, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False,\n",
              "         False,  True, False, False, False, False, False, False,  True,\n",
              "          True,  True, False, False,  True, False, False, False, False,\n",
              "         False, False, False, False]),\n",
              "  'test_accuracy': 0.522,\n",
              "  'test_score': 3590.3807615230453},\n",
              " {'params': {'population_size': 20,\n",
              "   'mutation_probability': 0.5,\n",
              "   'crossover_probability': 0.1},\n",
              "  'scoring': functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200),\n",
              "  'features': array([ True, False,  True,  True,  True, False, False,  True, False,\n",
              "          True, False, False, False, False, False, False,  True, False,\n",
              "         False, False, False,  True, False, False, False, False, False,\n",
              "         False, False, False,  True, False,  True, False,  True, False,\n",
              "         False, False, False, False, False,  True,  True, False, False,\n",
              "         False, False, False, False]),\n",
              "  'test_accuracy': 0.524,\n",
              "  'test_score': 2610.4208416833662},\n",
              " {'params': {'population_size': 20,\n",
              "   'mutation_probability': 0.5,\n",
              "   'crossover_probability': 0.2},\n",
              "  'scoring': functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200),\n",
              "  'features': array([ True, False, False, False,  True,  True, False,  True, False,\n",
              "         False,  True, False, False, False, False,  True, False, False,\n",
              "         False, False, False, False, False,  True, False, False,  True,\n",
              "          True, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False,  True]),\n",
              "  'test_accuracy': 0.533,\n",
              "  'test_score': 3350.701402805611},\n",
              " {'params': {'population_size': 20,\n",
              "   'mutation_probability': 0.5,\n",
              "   'crossover_probability': 0.3},\n",
              "  'scoring': functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200),\n",
              "  'features': array([False, False, False, False,  True, False, False, False, False,\n",
              "          True, False, False, False,  True, False, False, False, False,\n",
              "         False, False, False, False,  True, False, False, False, False,\n",
              "         False, False, False, False, False, False,  True, False, False,\n",
              "         False, False, False, False,  True, False, False, False, False,\n",
              "         False,  True, False, False]),\n",
              "  'test_accuracy': 0.541,\n",
              "  'test_score': 3930.66132264529},\n",
              " {'params': {'population_size': 20,\n",
              "   'mutation_probability': 0.4,\n",
              "   'crossover_probability': 0.1},\n",
              "  'scoring': functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200),\n",
              "  'features': array([ True, False, False, False, False, False, False, False, False,\n",
              "          True, False, False, False, False, False, False, False, False,\n",
              "         False, False, False,  True, False, False, False,  True,  True,\n",
              "         False,  True, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False,\n",
              "          True,  True, False, False]),\n",
              "  'test_accuracy': 0.489,\n",
              "  'test_score': 3329.8597194388776},\n",
              " {'params': {'population_size': 20,\n",
              "   'mutation_probability': 0.4,\n",
              "   'crossover_probability': 0.2},\n",
              "  'scoring': functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200),\n",
              "  'features': array([ True, False, False, False, False,  True,  True, False,  True,\n",
              "         False, False,  True,  True, False,  True, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False]),\n",
              "  'test_accuracy': 0.506,\n",
              "  'test_score': 3630.0601202404805},\n",
              " {'params': {'population_size': 20,\n",
              "   'mutation_probability': 0.4,\n",
              "   'crossover_probability': 0.3},\n",
              "  'scoring': functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200),\n",
              "  'features': array([ True,  True, False, False, False, False, False, False, False,\n",
              "         False, False,  True, False, False, False, False, False, False,\n",
              "         False, False, False, False, False,  True, False, False, False,\n",
              "         False, False,  True, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False]),\n",
              "  'test_accuracy': 0.511,\n",
              "  'test_score': 4110.220440881763},\n",
              " {'params': {'population_size': 30,\n",
              "   'mutation_probability': 0.6,\n",
              "   'crossover_probability': 0.1},\n",
              "  'scoring': functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200),\n",
              "  'features': array([False, False, False, False, False, False,  True, False, False,\n",
              "          True, False, False,  True,  True, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False,  True,\n",
              "         False, False,  True, False,  True,  True,  True,  True, False,\n",
              "         False, False, False, False, False,  True, False,  True, False,\n",
              "         False, False, False, False]),\n",
              "  'test_accuracy': 0.528,\n",
              "  'test_score': 2870.541082164328},\n",
              " {'params': {'population_size': 30,\n",
              "   'mutation_probability': 0.6,\n",
              "   'crossover_probability': 0.2},\n",
              "  'scoring': functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200),\n",
              "  'features': array([False, False, False, False, False, False,  True, False, False,\n",
              "         False, False,  True,  True,  True,  True, False, False, False,\n",
              "         False, False, False, False, False, False,  True, False, False,\n",
              "         False, False, False,  True, False, False, False, False, False,\n",
              "         False, False, False, False, False, False,  True, False, False,\n",
              "         False, False, False, False]),\n",
              "  'test_accuracy': 0.534,\n",
              "  'test_score': 3750.701402805611},\n",
              " {'params': {'population_size': 30,\n",
              "   'mutation_probability': 0.6,\n",
              "   'crossover_probability': 0.3},\n",
              "  'scoring': functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200),\n",
              "  'features': array([False, False, False, False, False, False,  True, False, False,\n",
              "         False, False,  True, False, False, False, False,  True, False,\n",
              "         False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False,  True, False,  True,\n",
              "         False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False]),\n",
              "  'test_accuracy': 0.53,\n",
              "  'test_score': 4290.581162324649},\n",
              " {'params': {'population_size': 30,\n",
              "   'mutation_probability': 0.5,\n",
              "   'crossover_probability': 0.1},\n",
              "  'scoring': functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200),\n",
              "  'features': array([ True, False, False, False, False, False,  True, False, False,\n",
              "          True,  True, False, False, False,  True, False, False,  True,\n",
              "         False, False,  True, False,  True, False, False, False, False,\n",
              "          True, False, False, False, False, False,  True, False, False,\n",
              "         False, False, False, False, False,  True, False, False, False,\n",
              "         False, False, False, False]),\n",
              "  'test_accuracy': 0.553,\n",
              "  'test_score': 3311.022044088176},\n",
              " {'params': {'population_size': 30,\n",
              "   'mutation_probability': 0.5,\n",
              "   'crossover_probability': 0.2},\n",
              "  'scoring': functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200),\n",
              "  'features': array([False, False, False, False, False,  True, False, False, False,\n",
              "         False,  True, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False,  True,\n",
              "          True, False,  True, False, False, False, False, False, False,\n",
              "         False,  True, False, False, False, False, False, False, False,\n",
              "         False, False, False, False]),\n",
              "  'test_accuracy': 0.512,\n",
              "  'test_score': 3930.2605210420834},\n",
              " {'params': {'population_size': 30,\n",
              "   'mutation_probability': 0.5,\n",
              "   'crossover_probability': 0.3},\n",
              "  'scoring': functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200),\n",
              "  'features': array([False, False, False, False, False, False, False, False, False,\n",
              "         False, False,  True, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False,  True,  True,\n",
              "         False, False, False, False, False, False, False,  True, False,\n",
              "         False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False]),\n",
              "  'test_accuracy': 0.493,\n",
              "  'test_score': 4169.939879759519},\n",
              " {'params': {'population_size': 30,\n",
              "   'mutation_probability': 0.4,\n",
              "   'crossover_probability': 0.1},\n",
              "  'scoring': functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200),\n",
              "  'features': array([False,  True, False, False, False, False, False,  True, False,\n",
              "         False,  True, False, False, False, False,  True,  True, False,\n",
              "         False,  True, False, False, False, False, False,  True, False,\n",
              "         False, False, False,  True, False, False, False, False,  True,\n",
              "         False, False, False, False,  True, False, False, False, False,\n",
              "         False, False, False, False]),\n",
              "  'test_accuracy': 0.52,\n",
              "  'test_score': 3150.3006012024043},\n",
              " {'params': {'population_size': 30,\n",
              "   'mutation_probability': 0.4,\n",
              "   'crossover_probability': 0.2},\n",
              "  'scoring': functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200),\n",
              "  'features': array([False, False, False, False, False,  True, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False,  True, False, False, False, False,\n",
              "         False,  True,  True, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False,\n",
              "          True, False,  True,  True]),\n",
              "  'test_accuracy': 0.547,\n",
              "  'test_score': 4030.861723446893},\n",
              " {'params': {'population_size': 30,\n",
              "   'mutation_probability': 0.4,\n",
              "   'crossover_probability': 0.3},\n",
              "  'scoring': functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200),\n",
              "  'features': array([False,  True, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False,\n",
              "         False,  True, False, False, False, False,  True, False,  True,\n",
              "         False, False, False, False, False, False, False, False, False,\n",
              "         False,  True, False, False, False, False, False, False, False,\n",
              "         False, False, False, False]),\n",
              "  'test_accuracy': 0.504,\n",
              "  'test_score': 4130.260521042083},\n",
              " {'params': {'population_size': 40,\n",
              "   'mutation_probability': 0.6,\n",
              "   'crossover_probability': 0.1},\n",
              "  'scoring': functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200),\n",
              "  'features': array([False, False, False, False, False, False, False, False, False,\n",
              "          True, False, False, False, False, False, False, False,  True,\n",
              "         False, False,  True, False, False, False, False, False, False,\n",
              "          True, False, False,  True, False, False, False, False, False,\n",
              "         False,  True, False,  True, False, False, False, False, False,\n",
              "         False, False, False, False]),\n",
              "  'test_accuracy': 0.539,\n",
              "  'test_score': 3890.581162324649},\n",
              " {'params': {'population_size': 40,\n",
              "   'mutation_probability': 0.6,\n",
              "   'crossover_probability': 0.2},\n",
              "  'scoring': functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200),\n",
              "  'features': array([False, False, False,  True, False, False, False, False, False,\n",
              "         False, False, False, False, False, False,  True, False, False,\n",
              "         False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False,  True, False, False,  True, False, False,\n",
              "         False, False, False, False, False, False,  True, False, False,\n",
              "         False, False, False, False]),\n",
              "  'test_accuracy': 0.523,\n",
              "  'test_score': 4210.420841683366},\n",
              " {'params': {'population_size': 40,\n",
              "   'mutation_probability': 0.6,\n",
              "   'crossover_probability': 0.3},\n",
              "  'scoring': functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200),\n",
              "  'features': array([False, False,  True, False, False, False, False, False, False,\n",
              "         False, False,  True, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False,  True, False,\n",
              "         False, False, False, False, False, False,  True, False, False,\n",
              "         False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False]),\n",
              "  'test_accuracy': 0.52,\n",
              "  'test_score': 4330.260521042083},\n",
              " {'params': {'population_size': 40,\n",
              "   'mutation_probability': 0.5,\n",
              "   'crossover_probability': 0.1},\n",
              "  'scoring': functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200),\n",
              "  'features': array([False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False,  True,\n",
              "         False, False, False,  True, False,  True, False, False, False,\n",
              "         False,  True, False, False, False, False, False, False, False,\n",
              "          True, False, False,  True, False, False,  True, False, False,\n",
              "         False, False, False,  True]),\n",
              "  'test_accuracy': 0.539,\n",
              "  'test_score': 3850.901803607214},\n",
              " {'params': {'population_size': 40,\n",
              "   'mutation_probability': 0.5,\n",
              "   'crossover_probability': 0.2},\n",
              "  'scoring': functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200),\n",
              "  'features': array([False, False,  True, False, False,  True, False, False, False,\n",
              "         False,  True, False, False, False, False, False, False,  True,\n",
              "         False, False,  True, False,  True, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False,  True,\n",
              "         False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False]),\n",
              "  'test_accuracy': 0.555,\n",
              "  'test_score': 4131.062124248497},\n",
              " {'params': {'population_size': 40,\n",
              "   'mutation_probability': 0.5,\n",
              "   'crossover_probability': 0.3},\n",
              "  'scoring': functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200),\n",
              "  'features': array([False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False,\n",
              "          True, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False]),\n",
              "  'test_accuracy': 0.517,\n",
              "  'test_score': 4950.300601202404},\n",
              " {'params': {'population_size': 40,\n",
              "   'mutation_probability': 0.4,\n",
              "   'crossover_probability': 0.1},\n",
              "  'scoring': functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200),\n",
              "  'features': array([False, False,  True, False,  True, False, False, False,  True,\n",
              "         False, False, False, False, False, False, False, False,  True,\n",
              "         False,  True,  True, False, False, False,  True, False, False,\n",
              "          True, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False]),\n",
              "  'test_accuracy': 0.545,\n",
              "  'test_score': 3830.861723446893},\n",
              " {'params': {'population_size': 40,\n",
              "   'mutation_probability': 0.4,\n",
              "   'crossover_probability': 0.2},\n",
              "  'scoring': functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200),\n",
              "  'features': array([False, False, False, False, False, False, False, False,  True,\n",
              "         False, False, False, False, False, False,  True, False,  True,\n",
              "         False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False,  True, False, False, False,\n",
              "         False, False, False, False, False, False, False,  True, False,\n",
              "         False, False, False, False]),\n",
              "  'test_accuracy': 0.511,\n",
              "  'test_score': 4150.300601202404},\n",
              " {'params': {'population_size': 40,\n",
              "   'mutation_probability': 0.4,\n",
              "   'crossover_probability': 0.3},\n",
              "  'scoring': functools.partial(<function calculate_score at 0x7f256d9bcdc0>, reward=10, punishment=200),\n",
              "  'features': array([False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False,  True,\n",
              "         False, False,  True, False, False, False, False,  True, False,\n",
              "         False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False]),\n",
              "  'test_accuracy': 0.509,\n",
              "  'test_score': 4470.1402805611215}]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(results).to_csv(\"genetic_results_30052024.csv\")"
      ],
      "metadata": {
        "id": "fB6d4nC-xKGN"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.read_csv(\"genetic_results_29052024.csv\")\n",
        "df2 = pd.read_csv(\"genetic_results_30052024.csv\")\n",
        "\n",
        "df = pd.concat([df1, df2], axis=0)\n",
        "best_features = list(df.sort_values(by='test_score', ascending=False)['features'][:10])\n",
        "best_features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "sxBABzNXyR_2",
        "outputId": "5133defe-4781-4520-9731-f84937167f4a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[False False False False False False False False False False False False\\n False False False False False False False False False False False False\\n False False False False False False False False False False False  True\\n False False False False False False False False False False False False\\n False]',\n",
              " '[False False False False False False False False False False False False\\n False False False False False False False False False False False False\\n False False False False False False False False False False False False\\n  True False False False False False False False False False False False\\n False]',\n",
              " '[False False False False False False False False False False False False\\n False False False False False False False False False False False False\\n False False  True False False  True False False False False  True False\\n False False False False False False False False False False False False\\n False]',\n",
              " '[False False False  True False False False False False False False False\\n False False False False False False False  True False False False  True\\n False False False False False False False False False False False False\\n False False False False False False False  True False False False False\\n False]',\n",
              " '[False False  True False False False False False False False False  True\\n False False False False False False False False False False False False\\n False  True False False False False False False False  True False False\\n False False False False False False False False False False False False\\n False]',\n",
              " '[False False False False False False  True False False False False  True\\n False False False False  True False False False False False False False\\n False False False False False False False False False  True False  True\\n False False False False False False False False False False False False\\n False]',\n",
              " '[False False False  True False False False False False False False False\\n False False False  True False False False False False False False False\\n False False False False False False  True False False  True False False\\n False False False False False False  True False False False False False\\n False]',\n",
              " '[False False False False False False False False False False False  True\\n False False False False False False False False False False False False\\n False  True  True False False False False False False False  True False\\n False False False False False False False False False False False False\\n False]',\n",
              " '[False False False False False False False False  True False False False\\n False False False  True False  True False False False False False False\\n False False False False False False False False  True False False False\\n False False False False False False False  True False False False False\\n False]',\n",
              " '[False False  True False False  True False False False False  True False\\n False False False False False  True False False  True False  True False\\n False False False False False False False False False False False  True\\n False False False False False False False False False False False False\\n False]']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for features in best_features:\n",
        "  tmp = [x for x in features.replace(\"\\n\", \"\").replace(\"[\", \"\").replace(\"]\", \"\").split()]\n",
        "  tmp = [val == 'True' for val in tmp]\n",
        "  print(np.array(initial_features)[tmp])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jd-UK1wWzBmK",
        "outputId": "6bae2777-41fe-4393-b0fc-2ac6e0ac3b9f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['x423']\n",
            "['x459']\n",
            "['x329' 'x352' 'x413']\n",
            "['x102' 'x253' 'x322' 'x7']\n",
            "['x101' 'x153' 'x324' 'x404']\n",
            "['x105' 'x153' 'x22' 'x404' 'x423']\n",
            "['x102' 'x2' 'x36' 'x404' 'x65']\n",
            "['x153' 'x324' 'x329' 'x413']\n",
            "['x132' 'x2' 'x221' 'x40' 'x7']\n",
            "['x101' 'x104' 'x149' 'x221' 'x286' 'x304' 'x423']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nI0eU5FcyR0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDIc19MR2Fc0",
        "outputId": "bd34a9f4-6bc3-4c49-f652-acada24609cb",
        "collapsed": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gen\tnevals\tfitness\tfitness_std\tfitness_max\tfitness_min\n",
            "0  \t20    \t4204.2 \t179.091    \t4453.12    \t3878.15    \n",
            "1  \t36    \t4290.69\t129.387    \t4453.12    \t3997.91    \n",
            "2  \t35    \t4389.7 \t134.588    \t4642.84    \t4123       \n",
            "3  \t36    \t4391.65\t143.31     \t4563.04    \t4123       \n",
            "4  \t36    \t4392.12\t144.905    \t4563.04    \t4063.02    \n",
            "5  \t37    \t4420.69\t145.591    \t4587.73    \t4148.1     \n",
            "6  \t39    \t4415.69\t171.711    \t4627.65    \t4067.98    \n",
            "7  \t38    \t4437.96\t147.256    \t4627.65    \t4047.99    \n",
            "8  \t39    \t4498.94\t103.146    \t4627.65    \t4222.98    \n",
            "9  \t37    \t4456.35\t179.48     \t4627.65    \t4008.15    \n",
            "10 \t36    \t4520.79\t168.594    \t4627.65    \t4038.09    \n",
            "11 \t37    \t4561.5 \t92.7278    \t4627.65    \t4317.95    \n",
            "12 \t37    \t4510.56\t175.309    \t4667.96    \t3942.85    \n",
            "13 \t32    \t4499.83\t193.702    \t4667.96    \t4023.04    \n",
            "14 \t35    \t4566.01\t133.078    \t4667.96    \t4048.16    \n",
            "15 \t36    \t4587.74\t109.781    \t4637.75    \t4142.95    \n",
            "16 \t38    \t4605.23\t111.079    \t4697.7     \t4142.95    \n",
            "17 \t34    \t4585.03\t174.379    \t4737.67    \t4143.13    \n",
            "18 \t31    \t4608.03\t135.348    \t4742.75    \t4173.02    \n",
            "19 \t40    \t4543.55\t232.514    \t4737.67    \t3948.1     \n",
            "20 \t36    \t4626.02\t159.592    \t4767.78    \t4082.96    \n",
            "21 \t39    \t4603.26\t200.139    \t4767.78    \t4107.99    \n",
            "22 \t33    \t4690.24\t38.5249    \t4767.78    \t4617.89    \n",
            "23 \t36    \t4703.96\t33.9854    \t4807.69    \t4617.89    \n",
            "24 \t37    \t4672.96\t162.769    \t4807.69    \t3972.98    \n",
            "25 \t38    \t4689.71\t112.17     \t4807.69    \t4367.9     \n",
            "26 \t32    \t4685.72\t160.516    \t4807.69    \t4003.17    \n",
            "27 \t39    \t4701.72\t123.482    \t4807.69    \t4192.89    \n",
            "28 \t35    \t4658.23\t212.963    \t4807.69    \t3993.07    \n",
            "29 \t35    \t4731.96\t175.191    \t4807.69    \t3988.11    \n",
            "30 \t36    \t4736.74\t168.447    \t4807.69    \t4238.24    \n",
            "31 \t38    \t4606.03\t272.475    \t4807.69    \t4122.99    \n",
            "32 \t35    \t4695.24\t214.453    \t4807.69    \t4108.03    \n",
            "33 \t39    \t4719.47\t200.919    \t4807.69    \t4013.02    \n",
            "34 \t38    \t4529.05\t375.48     \t4807.69    \t3812.94    \n",
            "35 \t35    \t4571.03\t314.198    \t4807.69    \t3872.97    \n",
            "36 \t35    \t4715.97\t238.074    \t4807.69    \t4012.94    \n",
            "37 \t36    \t4767.95\t173.211    \t4807.69    \t4012.94    \n",
            "38 \t35    \t4737.74\t210.076    \t4807.69    \t4078.15    \n",
            "39 \t38    \t4711.49\t229.929    \t4807.69    \t4098.05    \n",
            "40 \t36    \t4807.69\t9.09495e-13\t4807.69    \t4807.69    \n",
            "41 \t38    \t4807.69\t9.09495e-13\t4807.69    \t4807.69    \n",
            "42 \t37    \t4753.49\t165.808    \t4807.69    \t4163.18    \n",
            "43 \t38    \t4695.5 \t235.996    \t4807.69    \t4033.13    \n",
            "44 \t34    \t4678.77\t207.309    \t4807.69    \t4208.09    \n",
            "45 \t36    \t4732.5 \t154.182    \t4807.69    \t4343.12    \n",
            "46 \t33    \t4796.77\t72.1034    \t4852.93    \t4487.98    \n",
            "47 \t35    \t4761.52\t201.306    \t4867.88    \t3948.1     \n",
            "48 \t37    \t4790.27\t168.596    \t4907.79    \t4157.92    \n",
            "49 \t39    \t4783.28\t239.325    \t4907.79    \t3982.76    \n",
            "50 \t36    \t4875.78\t54.2973    \t4907.79    \t4707.84    \n",
            "51 \t36    \t4808.07\t253.905    \t4907.79    \t4023.06    \n",
            "52 \t36    \t4791.34\t273.122    \t4907.79    \t4123.25    \n",
            "53 \t31    \t4907.79\t9.09495e-13\t4907.79    \t4907.79    \n",
            "54 \t37    \t4907.79\t9.09495e-13\t4907.79    \t4907.79    \n",
            "55 \t38    \t4738.09\t344.473    \t4907.79    \t3893.13    \n",
            "56 \t37    \t4766.07\t341.765    \t4907.79    \t3767.96    \n",
            "57 \t37    \t4732.58\t352.198    \t4907.79    \t3918.09    \n",
            "58 \t38    \t4766.59\t284.504    \t4907.79    \t4077.97    \n",
            "59 \t36    \t4827.07\t242.313    \t4907.79    \t4073.06    \n",
            "60 \t38    \t4780.57\t303.565    \t4907.79    \t3983.08    \n",
            "61 \t38    \t4698.59\t366.317    \t4907.79    \t3923.1     \n",
            "62 \t37    \t4735.1 \t279.683    \t4907.79    \t4083.15    \n",
            "63 \t37    \t4804.33\t181.398    \t4932.92    \t4227.84    \n",
            "64 \t37    \t4854.31\t97.2924    \t4907.79    \t4587.97    \n",
            "65 \t35    \t4862.55\t197.195    \t4907.79    \t4003       \n",
            "66 \t37    \t4776.82\t316.309    \t4907.79    \t3917.98    \n",
            "67 \t36    \t4795.33\t270.433    \t4907.79    \t4018.13    \n",
            "68 \t34    \t4843.32\t169.706    \t4907.79    \t4228.04    \n",
            "69 \t38    \t4816.84\t218.38     \t4907.79    \t4143.09    \n",
            "70 \t34    \t4907.06\t24.6211    \t4978.04    \t4822.94    \n",
            "71 \t33    \t4911.3 \t15.3102    \t4978.04    \t4907.79    \n",
            "72 \t39    \t4841.83\t210.14     \t4978.04    \t4213.08    \n",
            "73 \t38    \t4883.08\t141.531    \t4978.04    \t4272.99    \n",
            "74 \t37    \t4591.87\t409.164    \t4978.04    \t3922.9     \n",
            "75 \t35    \t4837.36\t193.775    \t4978.04    \t4302.8     \n",
            "76 \t37    \t4823.6 \t175.496    \t4978.04    \t4467.94    \n",
            "77 \t35    \t4697.42\t379.305    \t4978.04    \t3983.11    \n",
            "78 \t35    \t4864.18\t250.766    \t4978.04    \t4063.05    \n",
            "79 \t35    \t4897.96\t216.071    \t4978.04    \t3978.08    \n",
            "80 \t35    \t4930.02\t178.343    \t4978.04    \t4158.05    \n",
            "81 \t36    \t4931.29\t203.786    \t4978.04    \t4043       \n",
            "82 \t37    \t4978.04\t0          \t4978.04    \t4978.04    \n",
            "83 \t36    \t4897.28\t243.094    \t4978.04    \t4107.98    \n",
            "84 \t33    \t4900.27\t228.081    \t4978.04    \t3973.11    \n",
            "85 \t36    \t4777.04\t340.403    \t5008       \t3973.11    \n",
            "86 \t38    \t4796.76\t381.604    \t5008       \t4012.94    \n",
            "87 \t39    \t4859.75\t276.2      \t5008       \t4057.93    \n",
            "88 \t36    \t4664.98\t388.003    \t5008       \t3943.03    \n",
            "89 \t31    \t4723.21\t303.22     \t5008       \t4002.99    \n",
            "90 \t34    \t4800.48\t270.782    \t5008       \t4007.98    \n",
            "91 \t37    \t4748.5 \t321.32     \t5008       \t3952.99    \n",
            "92 \t35    \t4730.48\t290.681    \t5008       \t4127.98    \n",
            "93 \t37    \t4738.02\t286.463    \t5008       \t4077.81    \n",
            "94 \t35    \t4853.02\t217.706    \t5008       \t4108       \n",
            "95 \t39    \t4825   \t251.042    \t5008       \t4007.88    \n",
            "96 \t36    \t4750.75\t303.562    \t5008       \t4007.88    \n",
            "97 \t40    \t4701.25\t393.366    \t5008       \t3873.12    \n",
            "98 \t34    \t4869.52\t231.771    \t5008       \t4183.02    \n",
            "99 \t35    \t4850.51\t225.619    \t5008       \t4212.93    \n",
            "100\t36    \t4918.99\t122.416    \t5008       \t4547.92    \n",
            "Selected features: [False False False False False False False  True False False False False\n",
            " False  True False False  True False False  True False  True False False\n",
            " False False False  True False False False False False False False False\n",
            " False False False False False False False False False  True False False\n",
            " False]\n",
            "Accuracy with selected features: 0.518\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:432: UserWarning: X has feature names, but GAFeatureSelectionCV was fitted without feature names\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        }
      ],
      "source": [
        "from functools import partial\n",
        "\n",
        "# Define the estimator\n",
        "estimator = LogisticRegression()\n",
        "\n",
        "# Convert custom scoring function to a scorer\n",
        "# custom_scorer = make_scorer(penalized_score, greater_is_better=True)\n",
        "\n",
        "# Define the genetic algorithm feature selector\n",
        "selector = GAFeatureSelectionCV(\n",
        "    estimator=estimator,\n",
        "    cv=3,\n",
        "    scoring=partial(calculate_score, reward=10, punishment=50),\n",
        "    population_size=20,\n",
        "    generations=100,\n",
        "    mutation_probability=0.6,\n",
        "    crossover_probability=0.3,\n",
        "    elitism=True,\n",
        "    n_jobs=-1,\n",
        "    verbose=2,\n",
        "    keep_top_k=8,\n",
        "    criteria=\"max\"\n",
        ")\n",
        "\n",
        "# Fit the genetic algorithm feature selector\n",
        "selector.fit(X_train, np.ravel(y_train))\n",
        "\n",
        "# Plot the evolution of the fitness\n",
        "# plot_fitness_evolution(selector)\n",
        "\n",
        "# Get the best features\n",
        "best_features = selector.support_\n",
        "\n",
        "# Transform the dataset to keep only the best features\n",
        "X_train_selected = selector.transform(X_train)\n",
        "X_test_selected = selector.transform(X_test)\n",
        "\n",
        "# Train a model with the selected features\n",
        "estimator.fit(X_train_selected, y_train)\n",
        "accuracy = estimator.score(X_test_selected, y_test)\n",
        "\n",
        "print(\"Selected features:\", best_features)\n",
        "print(\"Accuracy with selected features:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8S_A6iwGU-7W",
        "outputId": "6973c1df-8b80-4073-8405-329319980e53"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([False, False, False, False, False, False, False,  True, False,\n",
              "       False, False, False, False,  True, False, False,  True, False,\n",
              "       False,  True, False,  True, False, False, False, False, False,\n",
              "        True, False, False, False, False, False, False, False, False,\n",
              "       False, False, False, False, False, False, False, False, False,\n",
              "        True, False, False, False])"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "selector.support_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgYKhTiENOIm"
      },
      "outputs": [],
      "source": [
        "features_selected = [\n",
        "    [False,  True, False, False, False, False, False,  True, False,\n",
        "       False, False, False, False, False, False, False, False, False,\n",
        "       False, False, False, False, False, False, False, False,  True,\n",
        "       False, False,  True, False, False, False, False, False, False,\n",
        "       False, False, False, False,  True, False,  True, False, False,\n",
        "       False, False, False, False],\n",
        "    [False, False, False, False, False, False, False, False, False,\n",
        "       False, False, False, False, False, False, False,  True, False,\n",
        "       False,  True, False, False, False, False, False, False, False,\n",
        "       False, False,  True, False, False, False, False, False, False,\n",
        "       False, False, False, False, False, False, False, False, False,\n",
        "       False, False, False, False],\n",
        "    [False, False, False, False, False, False, False,  True, False,\n",
        "       False, False, False, False,  True, False, False,  True, False,\n",
        "       False,  True, False,  True, False, False, False, False, False,\n",
        "        True, False, False, False, False, False, False, False, False,\n",
        "       False, False, False, False, False, False, False, False, False,\n",
        "        True, False, False, False]\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkzlnv5sXk2M"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOJ7YIeSZ9Op"
      },
      "source": [
        "# Random Forest Most Important Features Selector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4u63GDGocwD1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "d17d12fc-08a7-4e49-9ec1-9f27354eeb80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished - n_estimators: 50, max_depth: None, min_samples_split: 2\n",
            "Finished - n_estimators: 50, max_depth: None, min_samples_split: 5\n",
            "Finished - n_estimators: 50, max_depth: None, min_samples_split: 10\n",
            "Finished - n_estimators: 50, max_depth: 2, min_samples_split: 2\n",
            "Finished - n_estimators: 50, max_depth: 2, min_samples_split: 5\n",
            "Finished - n_estimators: 50, max_depth: 2, min_samples_split: 10\n",
            "Finished - n_estimators: 50, max_depth: 3, min_samples_split: 2\n",
            "Finished - n_estimators: 50, max_depth: 3, min_samples_split: 5\n",
            "Finished - n_estimators: 50, max_depth: 3, min_samples_split: 10\n",
            "Finished - n_estimators: 100, max_depth: None, min_samples_split: 2\n",
            "Finished - n_estimators: 100, max_depth: None, min_samples_split: 5\n",
            "Finished - n_estimators: 100, max_depth: None, min_samples_split: 10\n",
            "Finished - n_estimators: 100, max_depth: 2, min_samples_split: 2\n",
            "Finished - n_estimators: 100, max_depth: 2, min_samples_split: 5\n",
            "Finished - n_estimators: 100, max_depth: 2, min_samples_split: 10\n",
            "Finished - n_estimators: 100, max_depth: 3, min_samples_split: 2\n",
            "Finished - n_estimators: 100, max_depth: 3, min_samples_split: 5\n",
            "Finished - n_estimators: 100, max_depth: 3, min_samples_split: 10\n",
            "Finished - n_estimators: 200, max_depth: None, min_samples_split: 2\n",
            "Finished - n_estimators: 200, max_depth: None, min_samples_split: 5\n",
            "Finished - n_estimators: 200, max_depth: None, min_samples_split: 10\n",
            "Finished - n_estimators: 200, max_depth: 2, min_samples_split: 2\n",
            "Finished - n_estimators: 200, max_depth: 2, min_samples_split: 5\n",
            "Finished - n_estimators: 200, max_depth: 2, min_samples_split: 10\n",
            "Finished - n_estimators: 200, max_depth: 3, min_samples_split: 2\n",
            "Finished - n_estimators: 200, max_depth: 3, min_samples_split: 5\n",
            "Finished - n_estimators: 200, max_depth: 3, min_samples_split: 10\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[x1      0.015927\n",
              " x10     0.017882\n",
              " x101    0.042112\n",
              " x102    0.034067\n",
              " x103    0.041231\n",
              " x104    0.031301\n",
              " x105    0.031085\n",
              " x106    0.042342\n",
              " x132    0.019102\n",
              " x140    0.017967\n",
              " x149    0.017337\n",
              " x153    0.016563\n",
              " x156    0.016782\n",
              " x176    0.018234\n",
              " x191    0.017842\n",
              " x2      0.017476\n",
              " x22     0.018827\n",
              " x221    0.018617\n",
              " x229    0.019727\n",
              " x253    0.018052\n",
              " x286    0.018795\n",
              " x3      0.019098\n",
              " x304    0.017083\n",
              " x322    0.018566\n",
              " x323    0.017650\n",
              " x324    0.019173\n",
              " x329    0.017336\n",
              " x336    0.017705\n",
              " x35     0.016347\n",
              " x352    0.018720\n",
              " x36     0.018644\n",
              " x4      0.019787\n",
              " x40     0.019203\n",
              " x404    0.020049\n",
              " x413    0.016342\n",
              " x423    0.018630\n",
              " x459    0.016970\n",
              " x463    0.018928\n",
              " x499    0.018706\n",
              " x5      0.016791\n",
              " x58     0.017050\n",
              " x6      0.021857\n",
              " x65     0.016777\n",
              " x7      0.017816\n",
              " x74     0.017519\n",
              " x8      0.015848\n",
              " x81     0.016934\n",
              " x9      0.023360\n",
              " x99     0.015843\n",
              " dtype: float64,\n",
              " x1      0.015826\n",
              " x10     0.017720\n",
              " x101    0.044833\n",
              " x102    0.035061\n",
              " x103    0.041396\n",
              " x104    0.032084\n",
              " x105    0.034106\n",
              " x106    0.044202\n",
              " x132    0.017021\n",
              " x140    0.017695\n",
              " x149    0.017397\n",
              " x153    0.016545\n",
              " x156    0.017479\n",
              " x176    0.018358\n",
              " x191    0.019034\n",
              " x2      0.016068\n",
              " x22     0.019493\n",
              " x221    0.017661\n",
              " x229    0.017884\n",
              " x253    0.016616\n",
              " x286    0.016874\n",
              " x3      0.017839\n",
              " x304    0.018308\n",
              " x322    0.016406\n",
              " x323    0.016197\n",
              " x324    0.018140\n",
              " x329    0.015869\n",
              " x336    0.017317\n",
              " x35     0.017405\n",
              " x352    0.017404\n",
              " x36     0.017768\n",
              " x4      0.021229\n",
              " x40     0.017185\n",
              " x404    0.019500\n",
              " x413    0.017861\n",
              " x423    0.017161\n",
              " x459    0.020203\n",
              " x463    0.015946\n",
              " x499    0.018893\n",
              " x5      0.018309\n",
              " x58     0.017615\n",
              " x6      0.020938\n",
              " x65     0.018580\n",
              " x7      0.018713\n",
              " x74     0.018400\n",
              " x8      0.015981\n",
              " x81     0.017804\n",
              " x9      0.022945\n",
              " x99     0.016732\n",
              " dtype: float64,\n",
              " x1      0.015820\n",
              " x10     0.018435\n",
              " x101    0.045672\n",
              " x102    0.039555\n",
              " x103    0.043659\n",
              " x104    0.034135\n",
              " x105    0.033608\n",
              " x106    0.044241\n",
              " x132    0.016890\n",
              " x140    0.019156\n",
              " x149    0.018130\n",
              " x153    0.017992\n",
              " x156    0.013741\n",
              " x176    0.018379\n",
              " x191    0.017823\n",
              " x2      0.014942\n",
              " x22     0.020862\n",
              " x221    0.015630\n",
              " x229    0.018401\n",
              " x253    0.017452\n",
              " x286    0.016522\n",
              " x3      0.018982\n",
              " x304    0.016006\n",
              " x322    0.017704\n",
              " x323    0.016288\n",
              " x324    0.018925\n",
              " x329    0.018235\n",
              " x336    0.015971\n",
              " x35     0.017567\n",
              " x352    0.017634\n",
              " x36     0.016944\n",
              " x4      0.022083\n",
              " x40     0.017939\n",
              " x404    0.020041\n",
              " x413    0.018716\n",
              " x423    0.016531\n",
              " x459    0.016867\n",
              " x463    0.017703\n",
              " x499    0.019002\n",
              " x5      0.018187\n",
              " x58     0.016883\n",
              " x6      0.019579\n",
              " x65     0.018761\n",
              " x7      0.016281\n",
              " x74     0.016479\n",
              " x8      0.015230\n",
              " x81     0.014723\n",
              " x9      0.022876\n",
              " x99     0.016819\n",
              " dtype: float64,\n",
              " x1      0.002431\n",
              " x10     0.028874\n",
              " x101    0.082812\n",
              " x102    0.067120\n",
              " x103    0.177688\n",
              " x104    0.050398\n",
              " x105    0.081920\n",
              " x106    0.091679\n",
              " x132    0.000769\n",
              " x140    0.000000\n",
              " x149    0.006889\n",
              " x153    0.002963\n",
              " x156    0.000000\n",
              " x176    0.008790\n",
              " x191    0.002634\n",
              " x2      0.038534\n",
              " x22     0.003222\n",
              " x221    0.001481\n",
              " x229    0.003928\n",
              " x253    0.003313\n",
              " x286    0.000000\n",
              " x3      0.030096\n",
              " x304    0.002597\n",
              " x322    0.000000\n",
              " x323    0.003849\n",
              " x324    0.003082\n",
              " x329    0.002288\n",
              " x336    0.002714\n",
              " x35     0.000000\n",
              " x352    0.000000\n",
              " x36     0.002046\n",
              " x4      0.039059\n",
              " x40     0.000000\n",
              " x404    0.000000\n",
              " x413    0.003277\n",
              " x423    0.001673\n",
              " x459    0.002205\n",
              " x463    0.000000\n",
              " x499    0.005587\n",
              " x5      0.007023\n",
              " x58     0.005732\n",
              " x6      0.068350\n",
              " x65     0.000000\n",
              " x7      0.013826\n",
              " x74     0.005107\n",
              " x8      0.001941\n",
              " x81     0.008602\n",
              " x9      0.133034\n",
              " x99     0.002465\n",
              " dtype: float64,\n",
              " x1      0.002431\n",
              " x10     0.028874\n",
              " x101    0.082812\n",
              " x102    0.067120\n",
              " x103    0.177688\n",
              " x104    0.050398\n",
              " x105    0.081920\n",
              " x106    0.091679\n",
              " x132    0.000769\n",
              " x140    0.000000\n",
              " x149    0.006889\n",
              " x153    0.002963\n",
              " x156    0.000000\n",
              " x176    0.008790\n",
              " x191    0.002634\n",
              " x2      0.038534\n",
              " x22     0.003222\n",
              " x221    0.001481\n",
              " x229    0.003928\n",
              " x253    0.003313\n",
              " x286    0.000000\n",
              " x3      0.030096\n",
              " x304    0.002597\n",
              " x322    0.000000\n",
              " x323    0.003849\n",
              " x324    0.003082\n",
              " x329    0.002288\n",
              " x336    0.002714\n",
              " x35     0.000000\n",
              " x352    0.000000\n",
              " x36     0.002046\n",
              " x4      0.039059\n",
              " x40     0.000000\n",
              " x404    0.000000\n",
              " x413    0.003277\n",
              " x423    0.001673\n",
              " x459    0.002205\n",
              " x463    0.000000\n",
              " x499    0.005587\n",
              " x5      0.007023\n",
              " x58     0.005732\n",
              " x6      0.068350\n",
              " x65     0.000000\n",
              " x7      0.013826\n",
              " x74     0.005107\n",
              " x8      0.001941\n",
              " x81     0.008602\n",
              " x9      0.133034\n",
              " x99     0.002465\n",
              " dtype: float64,\n",
              " x1      0.002431\n",
              " x10     0.028874\n",
              " x101    0.082812\n",
              " x102    0.067120\n",
              " x103    0.177688\n",
              " x104    0.050398\n",
              " x105    0.081920\n",
              " x106    0.091679\n",
              " x132    0.000769\n",
              " x140    0.000000\n",
              " x149    0.006889\n",
              " x153    0.002963\n",
              " x156    0.000000\n",
              " x176    0.008790\n",
              " x191    0.002634\n",
              " x2      0.038534\n",
              " x22     0.003222\n",
              " x221    0.001481\n",
              " x229    0.003928\n",
              " x253    0.003313\n",
              " x286    0.000000\n",
              " x3      0.030096\n",
              " x304    0.002597\n",
              " x322    0.000000\n",
              " x323    0.003849\n",
              " x324    0.003082\n",
              " x329    0.002288\n",
              " x336    0.002714\n",
              " x35     0.000000\n",
              " x352    0.000000\n",
              " x36     0.002046\n",
              " x4      0.039059\n",
              " x40     0.000000\n",
              " x404    0.000000\n",
              " x413    0.003277\n",
              " x423    0.001673\n",
              " x459    0.002205\n",
              " x463    0.000000\n",
              " x499    0.005587\n",
              " x5      0.007023\n",
              " x58     0.005732\n",
              " x6      0.068350\n",
              " x65     0.000000\n",
              " x7      0.013826\n",
              " x74     0.005107\n",
              " x8      0.001941\n",
              " x81     0.008602\n",
              " x9      0.133034\n",
              " x99     0.002465\n",
              " dtype: float64,\n",
              " x1      0.015650\n",
              " x10     0.034356\n",
              " x101    0.065192\n",
              " x102    0.083261\n",
              " x103    0.119790\n",
              " x104    0.067349\n",
              " x105    0.070143\n",
              " x106    0.107839\n",
              " x132    0.006041\n",
              " x140    0.006212\n",
              " x149    0.002618\n",
              " x153    0.003297\n",
              " x156    0.010056\n",
              " x176    0.004306\n",
              " x191    0.006933\n",
              " x2      0.026391\n",
              " x22     0.004833\n",
              " x221    0.001011\n",
              " x229    0.005350\n",
              " x253    0.001590\n",
              " x286    0.002650\n",
              " x3      0.034096\n",
              " x304    0.004249\n",
              " x322    0.005775\n",
              " x323    0.004589\n",
              " x324    0.017528\n",
              " x329    0.006899\n",
              " x336    0.003685\n",
              " x35     0.001381\n",
              " x352    0.000054\n",
              " x36     0.006124\n",
              " x4      0.036793\n",
              " x40     0.004981\n",
              " x404    0.006693\n",
              " x413    0.006938\n",
              " x423    0.005435\n",
              " x459    0.002323\n",
              " x463    0.000000\n",
              " x499    0.009250\n",
              " x5      0.013304\n",
              " x58     0.013618\n",
              " x6      0.053445\n",
              " x65     0.007035\n",
              " x7      0.012687\n",
              " x74     0.004164\n",
              " x8      0.005369\n",
              " x81     0.007755\n",
              " x9      0.075276\n",
              " x99     0.005689\n",
              " dtype: float64,\n",
              " x1      0.015650\n",
              " x10     0.034356\n",
              " x101    0.065192\n",
              " x102    0.083261\n",
              " x103    0.119790\n",
              " x104    0.067349\n",
              " x105    0.070143\n",
              " x106    0.107839\n",
              " x132    0.006041\n",
              " x140    0.006212\n",
              " x149    0.002618\n",
              " x153    0.003297\n",
              " x156    0.010056\n",
              " x176    0.004306\n",
              " x191    0.006933\n",
              " x2      0.026391\n",
              " x22     0.004833\n",
              " x221    0.001011\n",
              " x229    0.005350\n",
              " x253    0.001590\n",
              " x286    0.002650\n",
              " x3      0.034096\n",
              " x304    0.004249\n",
              " x322    0.005775\n",
              " x323    0.004589\n",
              " x324    0.017528\n",
              " x329    0.006899\n",
              " x336    0.003685\n",
              " x35     0.001381\n",
              " x352    0.000054\n",
              " x36     0.006124\n",
              " x4      0.036793\n",
              " x40     0.004981\n",
              " x404    0.006693\n",
              " x413    0.006938\n",
              " x423    0.005435\n",
              " x459    0.002323\n",
              " x463    0.000000\n",
              " x499    0.009250\n",
              " x5      0.013304\n",
              " x58     0.013618\n",
              " x6      0.053445\n",
              " x65     0.007035\n",
              " x7      0.012687\n",
              " x74     0.004164\n",
              " x8      0.005369\n",
              " x81     0.007755\n",
              " x9      0.075276\n",
              " x99     0.005689\n",
              " dtype: float64,\n",
              " x1      0.015650\n",
              " x10     0.034356\n",
              " x101    0.065192\n",
              " x102    0.083261\n",
              " x103    0.119790\n",
              " x104    0.067349\n",
              " x105    0.070143\n",
              " x106    0.107839\n",
              " x132    0.006041\n",
              " x140    0.006212\n",
              " x149    0.002618\n",
              " x153    0.003297\n",
              " x156    0.010056\n",
              " x176    0.004306\n",
              " x191    0.006933\n",
              " x2      0.026391\n",
              " x22     0.004833\n",
              " x221    0.001011\n",
              " x229    0.005350\n",
              " x253    0.001590\n",
              " x286    0.002650\n",
              " x3      0.034096\n",
              " x304    0.004249\n",
              " x322    0.005775\n",
              " x323    0.004589\n",
              " x324    0.017528\n",
              " x329    0.006899\n",
              " x336    0.003685\n",
              " x35     0.001381\n",
              " x352    0.000054\n",
              " x36     0.006124\n",
              " x4      0.036793\n",
              " x40     0.004981\n",
              " x404    0.006693\n",
              " x413    0.006938\n",
              " x423    0.005435\n",
              " x459    0.002323\n",
              " x463    0.000000\n",
              " x499    0.009250\n",
              " x5      0.013304\n",
              " x58     0.013618\n",
              " x6      0.053445\n",
              " x65     0.007035\n",
              " x7      0.012687\n",
              " x74     0.004164\n",
              " x8      0.005369\n",
              " x81     0.007755\n",
              " x9      0.075276\n",
              " x99     0.005689\n",
              " dtype: float64,\n",
              " x1      0.016630\n",
              " x10     0.018336\n",
              " x101    0.042487\n",
              " x102    0.035086\n",
              " x103    0.039623\n",
              " x104    0.033748\n",
              " x105    0.031300\n",
              " x106    0.042125\n",
              " x132    0.018590\n",
              " x140    0.018860\n",
              " x149    0.017850\n",
              " x153    0.017317\n",
              " x156    0.016425\n",
              " x176    0.018497\n",
              " x191    0.019034\n",
              " x2      0.016960\n",
              " x22     0.018455\n",
              " x221    0.018047\n",
              " x229    0.019446\n",
              " x253    0.018697\n",
              " x286    0.018593\n",
              " x3      0.019292\n",
              " x304    0.017559\n",
              " x322    0.018048\n",
              " x323    0.017226\n",
              " x324    0.018958\n",
              " x329    0.017386\n",
              " x336    0.017783\n",
              " x35     0.016857\n",
              " x352    0.018276\n",
              " x36     0.017770\n",
              " x4      0.020386\n",
              " x40     0.018090\n",
              " x404    0.019137\n",
              " x413    0.016681\n",
              " x423    0.016844\n",
              " x459    0.017563\n",
              " x463    0.017581\n",
              " x499    0.019789\n",
              " x5      0.016310\n",
              " x58     0.016977\n",
              " x6      0.020411\n",
              " x65     0.017416\n",
              " x7      0.016878\n",
              " x74     0.016671\n",
              " x8      0.015619\n",
              " x81     0.017814\n",
              " x9      0.023463\n",
              " x99     0.017107\n",
              " dtype: float64,\n",
              " x1      0.016772\n",
              " x10     0.018144\n",
              " x101    0.044315\n",
              " x102    0.036386\n",
              " x103    0.039564\n",
              " x104    0.033828\n",
              " x105    0.032577\n",
              " x106    0.044848\n",
              " x132    0.017013\n",
              " x140    0.019170\n",
              " x149    0.016932\n",
              " x153    0.017144\n",
              " x156    0.016729\n",
              " x176    0.018141\n",
              " x191    0.018922\n",
              " x2      0.016229\n",
              " x22     0.019344\n",
              " x221    0.017283\n",
              " x229    0.019184\n",
              " x253    0.017684\n",
              " x286    0.016879\n",
              " x3      0.018275\n",
              " x304    0.017351\n",
              " x322    0.017031\n",
              " x323    0.016646\n",
              " x324    0.018345\n",
              " x329    0.016801\n",
              " x336    0.017464\n",
              " x35     0.016819\n",
              " x352    0.017926\n",
              " x36     0.018015\n",
              " x4      0.020882\n",
              " x40     0.016647\n",
              " x404    0.018699\n",
              " x413    0.017204\n",
              " x423    0.017082\n",
              " x459    0.019477\n",
              " x463    0.016373\n",
              " x499    0.018446\n",
              " x5      0.018056\n",
              " x58     0.017625\n",
              " x6      0.019195\n",
              " x65     0.017950\n",
              " x7      0.017913\n",
              " x74     0.017951\n",
              " x8      0.016457\n",
              " x81     0.017059\n",
              " x9      0.024433\n",
              " x99     0.016789\n",
              " dtype: float64,\n",
              " x1      0.016686\n",
              " x10     0.018139\n",
              " x101    0.046136\n",
              " x102    0.039823\n",
              " x103    0.041989\n",
              " x104    0.034899\n",
              " x105    0.033423\n",
              " x106    0.045107\n",
              " x132    0.016758\n",
              " x140    0.019920\n",
              " x149    0.017755\n",
              " x153    0.016830\n",
              " x156    0.015236\n",
              " x176    0.018640\n",
              " x191    0.018375\n",
              " x2      0.015743\n",
              " x22     0.018905\n",
              " x221    0.016310\n",
              " x229    0.017435\n",
              " x253    0.016456\n",
              " x286    0.017497\n",
              " x3      0.020243\n",
              " x304    0.014726\n",
              " x322    0.017441\n",
              " x323    0.015436\n",
              " x324    0.018404\n",
              " x329    0.018372\n",
              " x336    0.016212\n",
              " x35     0.017667\n",
              " x352    0.018044\n",
              " x36     0.016266\n",
              " x4      0.021103\n",
              " x40     0.017163\n",
              " x404    0.018224\n",
              " x413    0.017975\n",
              " x423    0.017960\n",
              " x459    0.017175\n",
              " x463    0.018018\n",
              " x499    0.019140\n",
              " x5      0.018531\n",
              " x58     0.016266\n",
              " x6      0.019162\n",
              " x65     0.019138\n",
              " x7      0.017493\n",
              " x74     0.017178\n",
              " x8      0.016246\n",
              " x81     0.015861\n",
              " x9      0.022401\n",
              " x99     0.016092\n",
              " dtype: float64,\n",
              " x1      0.017526\n",
              " x10     0.032093\n",
              " x101    0.111284\n",
              " x102    0.100504\n",
              " x103    0.141576\n",
              " x104    0.057273\n",
              " x105    0.058895\n",
              " x106    0.106877\n",
              " x132    0.002425\n",
              " x140    0.000880\n",
              " x149    0.006952\n",
              " x153    0.005173\n",
              " x156    0.001074\n",
              " x176    0.006854\n",
              " x191    0.001317\n",
              " x2      0.026127\n",
              " x22     0.002433\n",
              " x221    0.000741\n",
              " x229    0.001964\n",
              " x253    0.001657\n",
              " x286    0.002611\n",
              " x3      0.023072\n",
              " x304    0.002496\n",
              " x322    0.000000\n",
              " x323    0.003892\n",
              " x324    0.002724\n",
              " x329    0.004641\n",
              " x336    0.005104\n",
              " x35     0.000962\n",
              " x352    0.000000\n",
              " x36     0.005341\n",
              " x4      0.038125\n",
              " x40     0.000927\n",
              " x404    0.002014\n",
              " x413    0.001639\n",
              " x423    0.001651\n",
              " x459    0.001102\n",
              " x463    0.001565\n",
              " x499    0.007203\n",
              " x5      0.014934\n",
              " x58     0.003390\n",
              " x6      0.047230\n",
              " x65     0.002337\n",
              " x7      0.015803\n",
              " x74     0.002553\n",
              " x8      0.002900\n",
              " x81     0.005104\n",
              " x9      0.115056\n",
              " x99     0.001999\n",
              " dtype: float64,\n",
              " x1      0.017526\n",
              " x10     0.032093\n",
              " x101    0.111284\n",
              " x102    0.100504\n",
              " x103    0.141576\n",
              " x104    0.057273\n",
              " x105    0.058895\n",
              " x106    0.106877\n",
              " x132    0.002425\n",
              " x140    0.000880\n",
              " x149    0.006952\n",
              " x153    0.005173\n",
              " x156    0.001074\n",
              " x176    0.006854\n",
              " x191    0.001317\n",
              " x2      0.026127\n",
              " x22     0.002433\n",
              " x221    0.000741\n",
              " x229    0.001964\n",
              " x253    0.001657\n",
              " x286    0.002611\n",
              " x3      0.023072\n",
              " x304    0.002496\n",
              " x322    0.000000\n",
              " x323    0.003892\n",
              " x324    0.002724\n",
              " x329    0.004641\n",
              " x336    0.005104\n",
              " x35     0.000962\n",
              " x352    0.000000\n",
              " x36     0.005341\n",
              " x4      0.038125\n",
              " x40     0.000927\n",
              " x404    0.002014\n",
              " x413    0.001639\n",
              " x423    0.001651\n",
              " x459    0.001102\n",
              " x463    0.001565\n",
              " x499    0.007203\n",
              " x5      0.014934\n",
              " x58     0.003390\n",
              " x6      0.047230\n",
              " x65     0.002337\n",
              " x7      0.015803\n",
              " x74     0.002553\n",
              " x8      0.002900\n",
              " x81     0.005104\n",
              " x9      0.115056\n",
              " x99     0.001999\n",
              " dtype: float64,\n",
              " x1      0.017526\n",
              " x10     0.032093\n",
              " x101    0.111284\n",
              " x102    0.100504\n",
              " x103    0.141576\n",
              " x104    0.057273\n",
              " x105    0.058895\n",
              " x106    0.106877\n",
              " x132    0.002425\n",
              " x140    0.000880\n",
              " x149    0.006952\n",
              " x153    0.005173\n",
              " x156    0.001074\n",
              " x176    0.006854\n",
              " x191    0.001317\n",
              " x2      0.026127\n",
              " x22     0.002433\n",
              " x221    0.000741\n",
              " x229    0.001964\n",
              " x253    0.001657\n",
              " x286    0.002611\n",
              " x3      0.023072\n",
              " x304    0.002496\n",
              " x322    0.000000\n",
              " x323    0.003892\n",
              " x324    0.002724\n",
              " x329    0.004641\n",
              " x336    0.005104\n",
              " x35     0.000962\n",
              " x352    0.000000\n",
              " x36     0.005341\n",
              " x4      0.038125\n",
              " x40     0.000927\n",
              " x404    0.002014\n",
              " x413    0.001639\n",
              " x423    0.001651\n",
              " x459    0.001102\n",
              " x463    0.001565\n",
              " x499    0.007203\n",
              " x5      0.014934\n",
              " x58     0.003390\n",
              " x6      0.047230\n",
              " x65     0.002337\n",
              " x7      0.015803\n",
              " x74     0.002553\n",
              " x8      0.002900\n",
              " x81     0.005104\n",
              " x9      0.115056\n",
              " x99     0.001999\n",
              " dtype: float64,\n",
              " x1      0.014862\n",
              " x10     0.036377\n",
              " x101    0.087599\n",
              " x102    0.100325\n",
              " x103    0.092765\n",
              " x104    0.065761\n",
              " x105    0.066102\n",
              " x106    0.123041\n",
              " x132    0.005317\n",
              " x140    0.004554\n",
              " x149    0.003417\n",
              " x153    0.004577\n",
              " x156    0.007803\n",
              " x176    0.005577\n",
              " x191    0.004773\n",
              " x2      0.025105\n",
              " x22     0.007318\n",
              " x221    0.002792\n",
              " x229    0.005197\n",
              " x253    0.001563\n",
              " x286    0.004931\n",
              " x3      0.022310\n",
              " x304    0.003810\n",
              " x322    0.003766\n",
              " x323    0.004395\n",
              " x324    0.011271\n",
              " x329    0.009938\n",
              " x336    0.006273\n",
              " x35     0.003651\n",
              " x352    0.003479\n",
              " x36     0.007743\n",
              " x4      0.034965\n",
              " x40     0.004950\n",
              " x404    0.004350\n",
              " x413    0.006608\n",
              " x423    0.004128\n",
              " x459    0.003400\n",
              " x463    0.002495\n",
              " x499    0.007491\n",
              " x5      0.019766\n",
              " x58     0.008926\n",
              " x6      0.035302\n",
              " x65     0.004343\n",
              " x7      0.017000\n",
              " x74     0.002652\n",
              " x8      0.004875\n",
              " x81     0.005661\n",
              " x9      0.082570\n",
              " x99     0.004127\n",
              " dtype: float64,\n",
              " x1      0.014862\n",
              " x10     0.036377\n",
              " x101    0.087599\n",
              " x102    0.100325\n",
              " x103    0.092765\n",
              " x104    0.065761\n",
              " x105    0.066102\n",
              " x106    0.123041\n",
              " x132    0.005317\n",
              " x140    0.004554\n",
              " x149    0.003417\n",
              " x153    0.004577\n",
              " x156    0.007803\n",
              " x176    0.005577\n",
              " x191    0.004773\n",
              " x2      0.025105\n",
              " x22     0.007318\n",
              " x221    0.002792\n",
              " x229    0.005197\n",
              " x253    0.001563\n",
              " x286    0.004931\n",
              " x3      0.022310\n",
              " x304    0.003810\n",
              " x322    0.003766\n",
              " x323    0.004395\n",
              " x324    0.011271\n",
              " x329    0.009938\n",
              " x336    0.006273\n",
              " x35     0.003651\n",
              " x352    0.003479\n",
              " x36     0.007743\n",
              " x4      0.034965\n",
              " x40     0.004950\n",
              " x404    0.004350\n",
              " x413    0.006608\n",
              " x423    0.004128\n",
              " x459    0.003400\n",
              " x463    0.002495\n",
              " x499    0.007491\n",
              " x5      0.019766\n",
              " x58     0.008926\n",
              " x6      0.035302\n",
              " x65     0.004343\n",
              " x7      0.017000\n",
              " x74     0.002652\n",
              " x8      0.004875\n",
              " x81     0.005661\n",
              " x9      0.082570\n",
              " x99     0.004127\n",
              " dtype: float64,\n",
              " x1      0.014862\n",
              " x10     0.038221\n",
              " x101    0.088368\n",
              " x102    0.097726\n",
              " x103    0.093267\n",
              " x104    0.069003\n",
              " x105    0.063345\n",
              " x106    0.123716\n",
              " x132    0.005317\n",
              " x140    0.004554\n",
              " x149    0.003264\n",
              " x153    0.004577\n",
              " x156    0.009030\n",
              " x176    0.005361\n",
              " x191    0.004773\n",
              " x2      0.025105\n",
              " x22     0.007109\n",
              " x221    0.002792\n",
              " x229    0.005197\n",
              " x253    0.001563\n",
              " x286    0.004153\n",
              " x3      0.022413\n",
              " x304    0.003810\n",
              " x322    0.004323\n",
              " x323    0.003785\n",
              " x324    0.011045\n",
              " x329    0.009414\n",
              " x336    0.006273\n",
              " x35     0.003710\n",
              " x352    0.004118\n",
              " x36     0.007849\n",
              " x4      0.034664\n",
              " x40     0.004950\n",
              " x404    0.004350\n",
              " x413    0.006608\n",
              " x423    0.004128\n",
              " x459    0.003400\n",
              " x463    0.002539\n",
              " x499    0.007491\n",
              " x5      0.022356\n",
              " x58     0.008926\n",
              " x6      0.035707\n",
              " x65     0.004343\n",
              " x7      0.016203\n",
              " x74     0.002652\n",
              " x8      0.004875\n",
              " x81     0.005661\n",
              " x9      0.078977\n",
              " x99     0.004127\n",
              " dtype: float64,\n",
              " x1      0.016857\n",
              " x10     0.018889\n",
              " x101    0.041858\n",
              " x102    0.034718\n",
              " x103    0.039673\n",
              " x104    0.033769\n",
              " x105    0.031313\n",
              " x106    0.042111\n",
              " x132    0.018467\n",
              " x140    0.019064\n",
              " x149    0.018168\n",
              " x153    0.017680\n",
              " x156    0.016526\n",
              " x176    0.018424\n",
              " x191    0.018266\n",
              " x2      0.016538\n",
              " x22     0.018505\n",
              " x221    0.018315\n",
              " x229    0.018623\n",
              " x253    0.018638\n",
              " x286    0.018281\n",
              " x3      0.019056\n",
              " x304    0.017114\n",
              " x322    0.018652\n",
              " x323    0.016859\n",
              " x324    0.018822\n",
              " x329    0.017744\n",
              " x336    0.017209\n",
              " x35     0.016800\n",
              " x352    0.018125\n",
              " x36     0.017810\n",
              " x4      0.020218\n",
              " x40     0.017970\n",
              " x404    0.018558\n",
              " x413    0.016880\n",
              " x423    0.017854\n",
              " x459    0.017822\n",
              " x463    0.017568\n",
              " x499    0.019120\n",
              " x5      0.016883\n",
              " x58     0.017152\n",
              " x6      0.020400\n",
              " x65     0.017632\n",
              " x7      0.017355\n",
              " x74     0.016956\n",
              " x8      0.016268\n",
              " x81     0.017893\n",
              " x9      0.022863\n",
              " x99     0.017735\n",
              " dtype: float64,\n",
              " x1      0.016431\n",
              " x10     0.018273\n",
              " x101    0.043648\n",
              " x102    0.036919\n",
              " x103    0.038992\n",
              " x104    0.033883\n",
              " x105    0.032476\n",
              " x106    0.044569\n",
              " x132    0.017953\n",
              " x140    0.019348\n",
              " x149    0.017070\n",
              " x153    0.016906\n",
              " x156    0.016508\n",
              " x176    0.018507\n",
              " x191    0.018019\n",
              " x2      0.016627\n",
              " x22     0.018980\n",
              " x221    0.017831\n",
              " x229    0.018453\n",
              " x253    0.018111\n",
              " x286    0.017569\n",
              " x3      0.019397\n",
              " x304    0.017228\n",
              " x322    0.017616\n",
              " x323    0.016366\n",
              " x324    0.018036\n",
              " x329    0.017638\n",
              " x336    0.016933\n",
              " x35     0.017153\n",
              " x352    0.018169\n",
              " x36     0.017533\n",
              " x4      0.019911\n",
              " x40     0.017354\n",
              " x404    0.018088\n",
              " x413    0.017197\n",
              " x423    0.017500\n",
              " x459    0.019155\n",
              " x463    0.017067\n",
              " x499    0.018366\n",
              " x5      0.017855\n",
              " x58     0.017382\n",
              " x6      0.019217\n",
              " x65     0.018435\n",
              " x7      0.018296\n",
              " x74     0.017297\n",
              " x8      0.016717\n",
              " x81     0.016506\n",
              " x9      0.023493\n",
              " x99     0.017023\n",
              " dtype: float64,\n",
              " x1      0.017560\n",
              " x10     0.018795\n",
              " x101    0.046071\n",
              " x102    0.039651\n",
              " x103    0.042204\n",
              " x104    0.035649\n",
              " x105    0.034055\n",
              " x106    0.045930\n",
              " x132    0.017174\n",
              " x140    0.018728\n",
              " x149    0.017268\n",
              " x153    0.017546\n",
              " x156    0.015869\n",
              " x176    0.018201\n",
              " x191    0.017456\n",
              " x2      0.016581\n",
              " x22     0.018851\n",
              " x221    0.017069\n",
              " x229    0.017994\n",
              " x253    0.017088\n",
              " x286    0.017139\n",
              " x3      0.019858\n",
              " x304    0.015904\n",
              " x322    0.017761\n",
              " x323    0.015411\n",
              " x324    0.018042\n",
              " x329    0.017963\n",
              " x336    0.017655\n",
              " x35     0.016735\n",
              " x352    0.018125\n",
              " x36     0.016192\n",
              " x4      0.020763\n",
              " x40     0.016804\n",
              " x404    0.017527\n",
              " x413    0.017528\n",
              " x423    0.017538\n",
              " x459    0.017142\n",
              " x463    0.017233\n",
              " x499    0.018671\n",
              " x5      0.017326\n",
              " x58     0.015708\n",
              " x6      0.019139\n",
              " x65     0.018679\n",
              " x7      0.017590\n",
              " x74     0.016455\n",
              " x8      0.016390\n",
              " x81     0.015928\n",
              " x9      0.023106\n",
              " x99     0.015949\n",
              " dtype: float64,\n",
              " x1      0.021035\n",
              " x10     0.026349\n",
              " x101    0.130260\n",
              " x102    0.107884\n",
              " x103    0.131308\n",
              " x104    0.050844\n",
              " x105    0.080084\n",
              " x106    0.114776\n",
              " x132    0.001997\n",
              " x140    0.001466\n",
              " x149    0.005064\n",
              " x153    0.004625\n",
              " x156    0.000537\n",
              " x176    0.005399\n",
              " x191    0.003055\n",
              " x2      0.026312\n",
              " x22     0.005715\n",
              " x221    0.002697\n",
              " x229    0.004165\n",
              " x253    0.001579\n",
              " x286    0.004327\n",
              " x3      0.024816\n",
              " x304    0.003543\n",
              " x322    0.000441\n",
              " x323    0.001946\n",
              " x324    0.003615\n",
              " x329    0.006375\n",
              " x336    0.003524\n",
              " x35     0.000481\n",
              " x352    0.001464\n",
              " x36     0.003171\n",
              " x4      0.026277\n",
              " x40     0.001325\n",
              " x404    0.002183\n",
              " x413    0.000819\n",
              " x423    0.002092\n",
              " x459    0.001165\n",
              " x463    0.001872\n",
              " x499    0.006689\n",
              " x5      0.011128\n",
              " x58     0.002065\n",
              " x6      0.043188\n",
              " x65     0.002705\n",
              " x7      0.009995\n",
              " x74     0.006587\n",
              " x8      0.006444\n",
              " x81     0.003742\n",
              " x9      0.090551\n",
              " x99     0.002320\n",
              " dtype: float64,\n",
              " x1      0.021035\n",
              " x10     0.026349\n",
              " x101    0.130260\n",
              " x102    0.107884\n",
              " x103    0.131308\n",
              " x104    0.050844\n",
              " x105    0.080084\n",
              " x106    0.114776\n",
              " x132    0.001997\n",
              " x140    0.001466\n",
              " x149    0.005064\n",
              " x153    0.004625\n",
              " x156    0.000537\n",
              " x176    0.005399\n",
              " x191    0.003055\n",
              " x2      0.026312\n",
              " x22     0.005715\n",
              " x221    0.002697\n",
              " x229    0.004165\n",
              " x253    0.001579\n",
              " x286    0.004327\n",
              " x3      0.024816\n",
              " x304    0.003543\n",
              " x322    0.000441\n",
              " x323    0.001946\n",
              " x324    0.003615\n",
              " x329    0.006375\n",
              " x336    0.003524\n",
              " x35     0.000481\n",
              " x352    0.001464\n",
              " x36     0.003171\n",
              " x4      0.026277\n",
              " x40     0.001325\n",
              " x404    0.002183\n",
              " x413    0.000819\n",
              " x423    0.002092\n",
              " x459    0.001165\n",
              " x463    0.001872\n",
              " x499    0.006689\n",
              " x5      0.011128\n",
              " x58     0.002065\n",
              " x6      0.043188\n",
              " x65     0.002705\n",
              " x7      0.009995\n",
              " x74     0.006587\n",
              " x8      0.006444\n",
              " x81     0.003742\n",
              " x9      0.090551\n",
              " x99     0.002320\n",
              " dtype: float64,\n",
              " x1      0.021035\n",
              " x10     0.026349\n",
              " x101    0.130260\n",
              " x102    0.107884\n",
              " x103    0.131308\n",
              " x104    0.050844\n",
              " x105    0.080084\n",
              " x106    0.114776\n",
              " x132    0.001997\n",
              " x140    0.001466\n",
              " x149    0.005064\n",
              " x153    0.004625\n",
              " x156    0.000537\n",
              " x176    0.005399\n",
              " x191    0.003055\n",
              " x2      0.026312\n",
              " x22     0.005715\n",
              " x221    0.002697\n",
              " x229    0.004165\n",
              " x253    0.001579\n",
              " x286    0.004327\n",
              " x3      0.024816\n",
              " x304    0.003543\n",
              " x322    0.000441\n",
              " x323    0.001946\n",
              " x324    0.003615\n",
              " x329    0.006375\n",
              " x336    0.003524\n",
              " x35     0.000481\n",
              " x352    0.001464\n",
              " x36     0.003171\n",
              " x4      0.026277\n",
              " x40     0.001325\n",
              " x404    0.002183\n",
              " x413    0.000819\n",
              " x423    0.002092\n",
              " x459    0.001165\n",
              " x463    0.001872\n",
              " x499    0.006689\n",
              " x5      0.011128\n",
              " x58     0.002065\n",
              " x6      0.043188\n",
              " x65     0.002705\n",
              " x7      0.009995\n",
              " x74     0.006587\n",
              " x8      0.006444\n",
              " x81     0.003742\n",
              " x9      0.090551\n",
              " x99     0.002320\n",
              " dtype: float64,\n",
              " x1      0.018067\n",
              " x10     0.026272\n",
              " x101    0.102745\n",
              " x102    0.106492\n",
              " x103    0.088510\n",
              " x104    0.064401\n",
              " x105    0.066560\n",
              " x106    0.119637\n",
              " x132    0.006113\n",
              " x140    0.006578\n",
              " x149    0.004256\n",
              " x153    0.006278\n",
              " x156    0.005715\n",
              " x176    0.006655\n",
              " x191    0.004536\n",
              " x2      0.026253\n",
              " x22     0.007522\n",
              " x221    0.004841\n",
              " x229    0.008899\n",
              " x253    0.002875\n",
              " x286    0.004388\n",
              " x3      0.024276\n",
              " x304    0.005710\n",
              " x322    0.004722\n",
              " x323    0.004402\n",
              " x324    0.009047\n",
              " x329    0.008194\n",
              " x336    0.007299\n",
              " x35     0.002592\n",
              " x352    0.006349\n",
              " x36     0.006789\n",
              " x4      0.027095\n",
              " x40     0.004065\n",
              " x404    0.005295\n",
              " x413    0.005187\n",
              " x423    0.005269\n",
              " x459    0.003813\n",
              " x463    0.002696\n",
              " x499    0.009887\n",
              " x5      0.014742\n",
              " x58     0.004937\n",
              " x6      0.030296\n",
              " x65     0.005221\n",
              " x7      0.015716\n",
              " x74     0.005733\n",
              " x8      0.011751\n",
              " x81     0.003937\n",
              " x9      0.072937\n",
              " x99     0.004446\n",
              " dtype: float64,\n",
              " x1      0.018067\n",
              " x10     0.026272\n",
              " x101    0.102083\n",
              " x102    0.105265\n",
              " x103    0.088510\n",
              " x104    0.064401\n",
              " x105    0.068494\n",
              " x106    0.119637\n",
              " x132    0.006113\n",
              " x140    0.006578\n",
              " x149    0.004256\n",
              " x153    0.006278\n",
              " x156    0.005715\n",
              " x176    0.006655\n",
              " x191    0.004536\n",
              " x2      0.026253\n",
              " x22     0.007522\n",
              " x221    0.004841\n",
              " x229    0.008899\n",
              " x253    0.002875\n",
              " x286    0.004388\n",
              " x3      0.024276\n",
              " x304    0.005006\n",
              " x322    0.004722\n",
              " x323    0.004402\n",
              " x324    0.009047\n",
              " x329    0.008113\n",
              " x336    0.007794\n",
              " x35     0.002592\n",
              " x352    0.005909\n",
              " x36     0.006431\n",
              " x4      0.026068\n",
              " x40     0.004065\n",
              " x404    0.005295\n",
              " x413    0.006885\n",
              " x423    0.005622\n",
              " x459    0.003813\n",
              " x463    0.002696\n",
              " x499    0.009887\n",
              " x5      0.014742\n",
              " x58     0.004937\n",
              " x6      0.030296\n",
              " x65     0.005240\n",
              " x7      0.015716\n",
              " x74     0.005733\n",
              " x8      0.011751\n",
              " x81     0.003937\n",
              " x9      0.072937\n",
              " x99     0.004446\n",
              " dtype: float64,\n",
              " x1      0.018369\n",
              " x10     0.027680\n",
              " x101    0.103209\n",
              " x102    0.103589\n",
              " x103    0.088902\n",
              " x104    0.067215\n",
              " x105    0.065306\n",
              " x106    0.119982\n",
              " x132    0.006113\n",
              " x140    0.006578\n",
              " x149    0.004179\n",
              " x153    0.005572\n",
              " x156    0.006581\n",
              " x176    0.006547\n",
              " x191    0.004536\n",
              " x2      0.026231\n",
              " x22     0.007448\n",
              " x221    0.004841\n",
              " x229    0.008462\n",
              " x253    0.002875\n",
              " x286    0.003999\n",
              " x3      0.024321\n",
              " x304    0.004902\n",
              " x322    0.004755\n",
              " x323    0.004097\n",
              " x324    0.008800\n",
              " x329    0.007852\n",
              " x336    0.007611\n",
              " x35     0.002621\n",
              " x352    0.006308\n",
              " x36     0.006528\n",
              " x4      0.027839\n",
              " x40     0.004067\n",
              " x404    0.005295\n",
              " x413    0.006885\n",
              " x423    0.005622\n",
              " x459    0.004177\n",
              " x463    0.002604\n",
              " x499    0.009887\n",
              " x5      0.016037\n",
              " x58     0.004937\n",
              " x6      0.030723\n",
              " x65     0.005240\n",
              " x7      0.014270\n",
              " x74     0.005733\n",
              " x8      0.011751\n",
              " x81     0.003926\n",
              " x9      0.071176\n",
              " x99     0.003822\n",
              " dtype: float64]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "importances_dict = defaultdict(list)\n",
        "\n",
        "params = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 2, 3],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "all_importances = []\n",
        "for n_estimators in params['n_estimators']:\n",
        "  for max_depth in params['max_depth']:\n",
        "    for min_samples_split in params['min_samples_split']:\n",
        "      importances_dict = defaultdict(list)\n",
        "      for i in range(3):\n",
        "        model = RandomForestClassifier(random_state=i, n_estimators=n_estimators, max_depth=max_depth, min_samples_split=min_samples_split)\n",
        "        model.fit(X_train, np.ravel(y_train))\n",
        "      for j in range(len(model.feature_names_in_)):\n",
        "        importances_dict[model.feature_names_in_[j]] = model.feature_importances_[j]\n",
        "      mean_importances = pd.Series(importances_dict).groupby(level=0).mean()\n",
        "      all_importances.append(mean_importances)\n",
        "      print(f\"Finished - n_estimators: {n_estimators}, max_depth: {max_depth}, min_samples_split: {min_samples_split}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_features = [set(x.sort_values(ascending=False)[:7].keys()) for x in all_importances]\n",
        "best_features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyX03HYiDdYq",
        "outputId": "e3df88a2-bc28-4462-f821-cdfdd233f91c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'x101', 'x102', 'x103', 'x104', 'x105', 'x106', 'x9'},\n",
              " {'x101', 'x102', 'x103', 'x104', 'x105', 'x106', 'x9'},\n",
              " {'x101', 'x102', 'x103', 'x104', 'x105', 'x106', 'x9'},\n",
              " {'x101', 'x102', 'x103', 'x105', 'x106', 'x6', 'x9'},\n",
              " {'x101', 'x102', 'x103', 'x105', 'x106', 'x6', 'x9'},\n",
              " {'x101', 'x102', 'x103', 'x105', 'x106', 'x6', 'x9'},\n",
              " {'x101', 'x102', 'x103', 'x104', 'x105', 'x106', 'x9'},\n",
              " {'x101', 'x102', 'x103', 'x104', 'x105', 'x106', 'x9'},\n",
              " {'x101', 'x102', 'x103', 'x104', 'x105', 'x106', 'x9'},\n",
              " {'x101', 'x102', 'x103', 'x104', 'x105', 'x106', 'x9'},\n",
              " {'x101', 'x102', 'x103', 'x104', 'x105', 'x106', 'x9'},\n",
              " {'x101', 'x102', 'x103', 'x104', 'x105', 'x106', 'x9'},\n",
              " {'x101', 'x102', 'x103', 'x104', 'x105', 'x106', 'x9'},\n",
              " {'x101', 'x102', 'x103', 'x104', 'x105', 'x106', 'x9'},\n",
              " {'x101', 'x102', 'x103', 'x104', 'x105', 'x106', 'x9'},\n",
              " {'x101', 'x102', 'x103', 'x104', 'x105', 'x106', 'x9'},\n",
              " {'x101', 'x102', 'x103', 'x104', 'x105', 'x106', 'x9'},\n",
              " {'x101', 'x102', 'x103', 'x104', 'x105', 'x106', 'x9'},\n",
              " {'x101', 'x102', 'x103', 'x104', 'x105', 'x106', 'x9'},\n",
              " {'x101', 'x102', 'x103', 'x104', 'x105', 'x106', 'x9'},\n",
              " {'x101', 'x102', 'x103', 'x104', 'x105', 'x106', 'x9'},\n",
              " {'x101', 'x102', 'x103', 'x104', 'x105', 'x106', 'x9'},\n",
              " {'x101', 'x102', 'x103', 'x104', 'x105', 'x106', 'x9'},\n",
              " {'x101', 'x102', 'x103', 'x104', 'x105', 'x106', 'x9'},\n",
              " {'x101', 'x102', 'x103', 'x104', 'x105', 'x106', 'x9'},\n",
              " {'x101', 'x102', 'x103', 'x104', 'x105', 'x106', 'x9'},\n",
              " {'x101', 'x102', 'x103', 'x104', 'x105', 'x106', 'x9'}]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "whole_set = set()\n",
        "for x in best_features:\n",
        "  whole_set = whole_set.union(x)\n",
        "\n",
        "whole_set"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxrZQHsmun8A",
        "outputId": "71307ff6-47b4-4be3-abbc-b2f04b443cc0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'x101', 'x102', 'x103', 'x104', 'x105', 'x106', 'x6', 'x9'}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import combinations\n",
        "\n",
        "scores = []\n",
        "\n",
        "for i in range(len(whole_set), 2, -1):\n",
        "  for features in combinations(whole_set, i):\n",
        "    X_train_tmp = X_train[list(features)]\n",
        "    X_test_tmp = X_test[list(features)]\n",
        "    model = RandomForestClassifier()\n",
        "    model.fit(X_train_tmp, np.ravel(y_train))\n",
        "    score = calculate_score(model, X_test_tmp, np.ravel(y_test))\n",
        "    score_train = calculate_score(model, X_train_tmp, np.ravel(y_train))\n",
        "    scores.append({'features': features, 'score_test': score, 'score_train': score_train})\n",
        "    print(f\"Features: {features}, score test: {score}, score train: {score_train}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGCVWsCGnTVb",
        "outputId": "9497891b-f5d6-454d-90c6-ba69fc543e6c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features: ('x105', 'x9', 'x102', 'x6', 'x103', 'x106', 'x101', 'x104'), score test: 5133.466933867735, score train: 8400.0\n",
            "Features: ('x105', 'x9', 'x102', 'x6', 'x103', 'x106', 'x101'), score test: 5233.266533066131, score train: 8600.0\n",
            "Features: ('x105', 'x9', 'x102', 'x6', 'x103', 'x106', 'x104'), score test: 5253.306613226452, score train: 8600.0\n",
            "Features: ('x105', 'x9', 'x102', 'x6', 'x103', 'x101', 'x104'), score test: 5213.226452905811, score train: 8600.0\n",
            "Features: ('x105', 'x9', 'x102', 'x6', 'x106', 'x101', 'x104'), score test: 4912.625250501002, score train: 8600.0\n",
            "Features: ('x105', 'x9', 'x102', 'x103', 'x106', 'x101', 'x104'), score test: 5313.426853707414, score train: 8600.0\n",
            "Features: ('x105', 'x9', 'x6', 'x103', 'x106', 'x101', 'x104'), score test: 5333.466933867735, score train: 8600.0\n",
            "Features: ('x105', 'x102', 'x6', 'x103', 'x106', 'x101', 'x104'), score test: 5313.426853707414, score train: 8600.0\n",
            "Features: ('x9', 'x102', 'x6', 'x103', 'x106', 'x101', 'x104'), score test: 5313.426853707414, score train: 8600.0\n",
            "Features: ('x105', 'x9', 'x102', 'x6', 'x103', 'x106'), score test: 5152.7054108216425, score train: 8800.0\n",
            "Features: ('x105', 'x9', 'x102', 'x6', 'x103', 'x101'), score test: 4992.384769539078, score train: 8800.0\n",
            "Features: ('x105', 'x9', 'x102', 'x6', 'x103', 'x104'), score test: 5333.066132264528, score train: 8800.0\n",
            "Features: ('x105', 'x9', 'x102', 'x6', 'x106', 'x101'), score test: 5072.54509018036, score train: 8800.0\n",
            "Features: ('x105', 'x9', 'x102', 'x6', 'x106', 'x104'), score test: 5032.464929859719, score train: 8800.0\n",
            "Features: ('x105', 'x9', 'x102', 'x6', 'x101', 'x104'), score test: 4992.384769539078, score train: 8800.0\n",
            "Features: ('x105', 'x9', 'x102', 'x103', 'x106', 'x101'), score test: 5333.066132264528, score train: 8800.0\n",
            "Features: ('x105', 'x9', 'x102', 'x103', 'x106', 'x104'), score test: 5393.18637274549, score train: 8800.0\n",
            "Features: ('x105', 'x9', 'x102', 'x103', 'x101', 'x104'), score test: 5433.266533066131, score train: 8800.0\n",
            "Features: ('x105', 'x9', 'x102', 'x106', 'x101', 'x104'), score test: 5373.146292585169, score train: 8800.0\n",
            "Features: ('x105', 'x9', 'x6', 'x103', 'x106', 'x101'), score test: 5313.026052104207, score train: 8800.0\n",
            "Features: ('x105', 'x9', 'x6', 'x103', 'x106', 'x104'), score test: 5333.066132264528, score train: 8800.0\n",
            "Features: ('x105', 'x9', 'x6', 'x103', 'x101', 'x104'), score test: 5333.066132264528, score train: 8800.0\n",
            "Features: ('x105', 'x9', 'x6', 'x106', 'x101', 'x104'), score test: 5192.7855711422835, score train: 8800.0\n",
            "Features: ('x105', 'x9', 'x103', 'x106', 'x101', 'x104'), score test: 5453.306613226452, score train: 8800.0\n",
            "Features: ('x105', 'x102', 'x6', 'x103', 'x106', 'x101'), score test: 5353.106212424849, score train: 8800.0\n",
            "Features: ('x105', 'x102', 'x6', 'x103', 'x106', 'x104'), score test: 5453.306613226452, score train: 8800.0\n",
            "Features: ('x105', 'x102', 'x6', 'x103', 'x101', 'x104'), score test: 5393.18637274549, score train: 8800.0\n",
            "Features: ('x105', 'x102', 'x6', 'x106', 'x101', 'x104'), score test: 5252.905811623245, score train: 8800.0\n",
            "Features: ('x105', 'x102', 'x103', 'x106', 'x101', 'x104'), score test: 5493.386773547093, score train: 8800.0\n",
            "Features: ('x105', 'x6', 'x103', 'x106', 'x101', 'x104'), score test: 5333.066132264528, score train: 8800.0\n",
            "Features: ('x9', 'x102', 'x6', 'x103', 'x106', 'x101'), score test: 5272.945891783566, score train: 8800.0\n",
            "Features: ('x9', 'x102', 'x6', 'x103', 'x106', 'x104'), score test: 5353.106212424849, score train: 8800.0\n",
            "Features: ('x9', 'x102', 'x6', 'x103', 'x101', 'x104'), score test: 5313.026052104207, score train: 8800.0\n",
            "Features: ('x9', 'x102', 'x6', 'x106', 'x101', 'x104'), score test: 5052.50501002004, score train: 8800.0\n",
            "Features: ('x9', 'x102', 'x103', 'x106', 'x101', 'x104'), score test: 5493.386773547093, score train: 8800.0\n",
            "Features: ('x9', 'x6', 'x103', 'x106', 'x101', 'x104'), score test: 5333.066132264528, score train: 8800.0\n",
            "Features: ('x102', 'x6', 'x103', 'x106', 'x101', 'x104'), score test: 5513.426853707414, score train: 8800.0\n",
            "Features: ('x105', 'x9', 'x102', 'x6', 'x103'), score test: 5192.384769539078, score train: 9000.0\n",
            "Features: ('x105', 'x9', 'x102', 'x6', 'x106'), score test: 4971.94388777555, score train: 9000.0\n",
            "Features: ('x105', 'x9', 'x102', 'x6', 'x101'), score test: 5032.064128256512, score train: 9000.0\n",
            "Features: ('x105', 'x9', 'x102', 'x6', 'x104'), score test: 5052.104208416833, score train: 9000.0\n",
            "Features: ('x105', 'x9', 'x102', 'x103', 'x106'), score test: 5412.8256513026045, score train: 9000.0\n",
            "Features: ('x105', 'x9', 'x102', 'x103', 'x101'), score test: 5312.625250501002, score train: 9000.0\n",
            "Features: ('x105', 'x9', 'x102', 'x103', 'x104'), score test: 5593.18637274549, score train: 9000.0\n",
            "Features: ('x105', 'x9', 'x102', 'x106', 'x101'), score test: 5272.54509018036, score train: 9000.0\n",
            "Features: ('x105', 'x9', 'x102', 'x106', 'x104'), score test: 5272.54509018036, score train: 9000.0\n",
            "Features: ('x105', 'x9', 'x102', 'x101', 'x104'), score test: 5272.54509018036, score train: 9000.0\n",
            "Features: ('x105', 'x9', 'x6', 'x103', 'x106'), score test: 5432.865731462925, score train: 9000.0\n",
            "Features: ('x105', 'x9', 'x6', 'x103', 'x101'), score test: 5092.184368737474, score train: 9000.0\n",
            "Features: ('x105', 'x9', 'x6', 'x103', 'x104'), score test: 5252.50501002004, score train: 9000.0\n",
            "Features: ('x105', 'x9', 'x6', 'x106', 'x101'), score test: 5212.424849699398, score train: 9000.0\n",
            "Features: ('x105', 'x9', 'x6', 'x106', 'x104'), score test: 5172.344689378757, score train: 9000.0\n",
            "Features: ('x105', 'x9', 'x6', 'x101', 'x104'), score test: 5032.064128256512, score train: 9000.0\n",
            "Features: ('x105', 'x9', 'x103', 'x106', 'x101'), score test: 5392.7855711422835, score train: 9000.0\n",
            "Features: ('x105', 'x9', 'x103', 'x106', 'x104'), score test: 5533.066132264528, score train: 9000.0\n",
            "Features: ('x105', 'x9', 'x103', 'x101', 'x104'), score test: 5613.226452905811, score train: 9000.0\n",
            "Features: ('x105', 'x9', 'x106', 'x101', 'x104'), score test: 5292.585170340681, score train: 9000.0\n",
            "Features: ('x105', 'x102', 'x6', 'x103', 'x106'), score test: 5232.464929859719, score train: 9000.0\n",
            "Features: ('x105', 'x102', 'x6', 'x103', 'x101'), score test: 5272.54509018036, score train: 9000.0\n",
            "Features: ('x105', 'x102', 'x6', 'x103', 'x104'), score test: 5412.8256513026045, score train: 9000.0\n",
            "Features: ('x105', 'x102', 'x6', 'x106', 'x101'), score test: 5272.54509018036, score train: 9000.0\n",
            "Features: ('x105', 'x102', 'x6', 'x106', 'x104'), score test: 5192.384769539078, score train: 9000.0\n",
            "Features: ('x105', 'x102', 'x6', 'x101', 'x104'), score test: 5032.064128256512, score train: 9000.0\n",
            "Features: ('x105', 'x102', 'x103', 'x106', 'x101'), score test: 5733.466933867735, score train: 9000.0\n",
            "Features: ('x105', 'x102', 'x103', 'x106', 'x104'), score test: 5492.985971943887, score train: 9000.0\n",
            "Features: ('x105', 'x102', 'x103', 'x101', 'x104'), score test: 5553.106212424849, score train: 9000.0\n",
            "Features: ('x105', 'x102', 'x106', 'x101', 'x104'), score test: 5432.865731462925, score train: 9000.0\n",
            "Features: ('x105', 'x6', 'x103', 'x106', 'x101'), score test: 5513.026052104207, score train: 9000.0\n",
            "Features: ('x105', 'x6', 'x103', 'x106', 'x104'), score test: 5452.905811623245, score train: 9000.0\n",
            "Features: ('x105', 'x6', 'x103', 'x101', 'x104'), score test: 5492.985971943887, score train: 9000.0\n",
            "Features: ('x105', 'x6', 'x106', 'x101', 'x104'), score test: 5232.464929859719, score train: 9000.0\n",
            "Features: ('x105', 'x103', 'x106', 'x101', 'x104'), score test: 5593.18637274549, score train: 9000.0\n",
            "Features: ('x9', 'x102', 'x6', 'x103', 'x106'), score test: 5272.54509018036, score train: 9000.0\n",
            "Features: ('x9', 'x102', 'x6', 'x103', 'x101'), score test: 5152.304609218436, score train: 9000.0\n",
            "Features: ('x9', 'x102', 'x6', 'x103', 'x104'), score test: 5252.50501002004, score train: 9000.0\n",
            "Features: ('x9', 'x102', 'x6', 'x106', 'x101'), score test: 5252.50501002004, score train: 9000.0\n",
            "Features: ('x9', 'x102', 'x6', 'x106', 'x104'), score test: 5092.184368737474, score train: 9000.0\n",
            "Features: ('x9', 'x102', 'x6', 'x101', 'x104'), score test: 4991.983967935871, score train: 9000.0\n",
            "Features: ('x9', 'x102', 'x103', 'x106', 'x101'), score test: 5573.146292585169, score train: 9000.0\n",
            "Features: ('x9', 'x102', 'x103', 'x106', 'x104'), score test: 5553.106212424849, score train: 9000.0\n",
            "Features: ('x9', 'x102', 'x103', 'x101', 'x104'), score test: 5533.066132264528, score train: 9000.0\n",
            "Features: ('x9', 'x102', 'x106', 'x101', 'x104'), score test: 5513.026052104207, score train: 9000.0\n",
            "Features: ('x9', 'x6', 'x103', 'x106', 'x101'), score test: 5352.7054108216425, score train: 9000.0\n",
            "Features: ('x9', 'x6', 'x103', 'x106', 'x104'), score test: 5212.424849699398, score train: 9000.0\n",
            "Features: ('x9', 'x6', 'x103', 'x101', 'x104'), score test: 5172.344689378757, score train: 9000.0\n",
            "Features: ('x9', 'x6', 'x106', 'x101', 'x104'), score test: 5172.344689378757, score train: 9000.0\n",
            "Features: ('x9', 'x103', 'x106', 'x101', 'x104'), score test: 5392.7855711422835, score train: 9000.0\n",
            "Features: ('x102', 'x6', 'x103', 'x106', 'x101'), score test: 5432.865731462925, score train: 9000.0\n",
            "Features: ('x102', 'x6', 'x103', 'x106', 'x104'), score test: 5613.226452905811, score train: 9000.0\n",
            "Features: ('x102', 'x6', 'x103', 'x101', 'x104'), score test: 5392.7855711422835, score train: 9000.0\n",
            "Features: ('x102', 'x6', 'x106', 'x101', 'x104'), score test: 5513.026052104207, score train: 9000.0\n",
            "Features: ('x102', 'x103', 'x106', 'x101', 'x104'), score test: 5733.466933867735, score train: 9000.0\n",
            "Features: ('x6', 'x103', 'x106', 'x101', 'x104'), score test: 5292.585170340681, score train: 9000.0\n",
            "Features: ('x105', 'x9', 'x102', 'x6'), score test: 4811.222444889779, score train: 9200.0\n",
            "Features: ('x105', 'x9', 'x102', 'x103'), score test: 5272.144288577154, score train: 9200.0\n",
            "Features: ('x105', 'x9', 'x102', 'x106'), score test: 5111.823647294588, score train: 9200.0\n",
            "Features: ('x105', 'x9', 'x102', 'x101'), score test: 5011.623246492985, score train: 9200.0\n",
            "Features: ('x105', 'x9', 'x102', 'x104'), score test: 5212.024048096192, score train: 9200.0\n",
            "Features: ('x105', 'x9', 'x6', 'x103'), score test: 5191.983967935871, score train: 9200.0\n",
            "Features: ('x105', 'x9', 'x6', 'x106'), score test: 5011.623246492985, score train: 9200.0\n",
            "Features: ('x105', 'x9', 'x6', 'x101'), score test: 5011.623246492985, score train: 9200.0\n",
            "Features: ('x105', 'x9', 'x6', 'x104'), score test: 5031.663326653306, score train: 9200.0\n",
            "Features: ('x105', 'x9', 'x103', 'x106'), score test: 5532.665330661322, score train: 9200.0\n",
            "Features: ('x105', 'x9', 'x103', 'x101'), score test: 5392.384769539078, score train: 9200.0\n",
            "Features: ('x105', 'x9', 'x103', 'x104'), score test: 5532.665330661322, score train: 9200.0\n",
            "Features: ('x105', 'x9', 'x106', 'x101'), score test: 5232.064128256512, score train: 9200.0\n",
            "Features: ('x105', 'x9', 'x106', 'x104'), score test: 5232.064128256512, score train: 9200.0\n",
            "Features: ('x105', 'x9', 'x101', 'x104'), score test: 5292.184368737474, score train: 9200.0\n",
            "Features: ('x105', 'x102', 'x6', 'x103'), score test: 5332.264529058116, score train: 9200.0\n",
            "Features: ('x105', 'x102', 'x6', 'x106'), score test: 5071.743486973947, score train: 9200.0\n",
            "Features: ('x105', 'x102', 'x6', 'x101'), score test: 5131.863727454909, score train: 9200.0\n",
            "Features: ('x105', 'x102', 'x6', 'x104'), score test: 4991.583166332664, score train: 9200.0\n",
            "Features: ('x105', 'x102', 'x103', 'x106'), score test: 5272.144288577154, score train: 9200.0\n",
            "Features: ('x105', 'x102', 'x103', 'x101'), score test: 5252.104208416833, score train: 9200.0\n",
            "Features: ('x105', 'x102', 'x103', 'x104'), score test: 5612.8256513026045, score train: 9200.0\n",
            "Features: ('x105', 'x102', 'x106', 'x101'), score test: 5392.384769539078, score train: 9200.0\n",
            "Features: ('x105', 'x102', 'x106', 'x104'), score test: 5372.344689378757, score train: 9200.0\n",
            "Features: ('x105', 'x102', 'x101', 'x104'), score test: 5352.304609218436, score train: 9200.0\n",
            "Features: ('x105', 'x6', 'x103', 'x106'), score test: 5272.144288577154, score train: 9200.0\n",
            "Features: ('x105', 'x6', 'x103', 'x101'), score test: 5232.064128256512, score train: 9200.0\n",
            "Features: ('x105', 'x6', 'x103', 'x104'), score test: 5272.144288577154, score train: 9200.0\n",
            "Features: ('x105', 'x6', 'x106', 'x101'), score test: 5432.464929859719, score train: 9200.0\n",
            "Features: ('x105', 'x6', 'x106', 'x104'), score test: 5272.144288577154, score train: 9200.0\n",
            "Features: ('x105', 'x6', 'x101', 'x104'), score test: 5131.863727454909, score train: 9200.0\n",
            "Features: ('x105', 'x103', 'x106', 'x101'), score test: 5552.7054108216425, score train: 9200.0\n",
            "Features: ('x105', 'x103', 'x106', 'x104'), score test: 5352.304609218436, score train: 9200.0\n",
            "Features: ('x105', 'x103', 'x101', 'x104'), score test: 5592.7855711422835, score train: 9200.0\n",
            "Features: ('x105', 'x106', 'x101', 'x104'), score test: 5472.54509018036, score train: 9200.0\n",
            "Features: ('x9', 'x102', 'x6', 'x103'), score test: 5091.783567134268, score train: 9200.0\n",
            "Features: ('x9', 'x102', 'x6', 'x106'), score test: 5091.783567134268, score train: 9200.0\n",
            "Features: ('x9', 'x102', 'x6', 'x101'), score test: 4971.543086172344, score train: 9200.0\n",
            "Features: ('x9', 'x102', 'x6', 'x104'), score test: 4971.543086172344, score train: 9200.0\n",
            "Features: ('x9', 'x102', 'x103', 'x106'), score test: 5372.344689378757, score train: 9200.0\n",
            "Features: ('x9', 'x102', 'x103', 'x101'), score test: 5312.224448897795, score train: 9200.0\n",
            "Features: ('x9', 'x102', 'x103', 'x104'), score test: 5532.665330661322, score train: 9200.0\n",
            "Features: ('x9', 'x102', 'x106', 'x101'), score test: 5492.585170340681, score train: 9200.0\n",
            "Features: ('x9', 'x102', 'x106', 'x104'), score test: 5472.54509018036, score train: 9200.0\n",
            "Features: ('x9', 'x102', 'x101', 'x104'), score test: 5191.983967935871, score train: 9200.0\n",
            "Features: ('x9', 'x6', 'x103', 'x106'), score test: 5292.184368737474, score train: 9200.0\n",
            "Features: ('x9', 'x6', 'x103', 'x101'), score test: 5071.743486973947, score train: 9200.0\n",
            "Features: ('x9', 'x6', 'x103', 'x104'), score test: 5272.144288577154, score train: 9200.0\n",
            "Features: ('x9', 'x6', 'x106', 'x101'), score test: 5292.184368737474, score train: 9200.0\n",
            "Features: ('x9', 'x6', 'x106', 'x104'), score test: 5272.144288577154, score train: 9200.0\n",
            "Features: ('x9', 'x6', 'x101', 'x104'), score test: 5011.623246492985, score train: 9200.0\n",
            "Features: ('x9', 'x103', 'x106', 'x101'), score test: 5592.7855711422835, score train: 9200.0\n",
            "Features: ('x9', 'x103', 'x106', 'x104'), score test: 5532.665330661322, score train: 9200.0\n",
            "Features: ('x9', 'x103', 'x101', 'x104'), score test: 5432.464929859719, score train: 9200.0\n",
            "Features: ('x9', 'x106', 'x101', 'x104'), score test: 5392.384769539078, score train: 9200.0\n",
            "Features: ('x102', 'x6', 'x103', 'x106'), score test: 5091.783567134268, score train: 9200.0\n",
            "Features: ('x102', 'x6', 'x103', 'x101'), score test: 5191.983967935871, score train: 9200.0\n",
            "Features: ('x102', 'x6', 'x103', 'x104'), score test: 5452.50501002004, score train: 9200.0\n",
            "Features: ('x102', 'x6', 'x106', 'x101'), score test: 5492.585170340681, score train: 9200.0\n",
            "Features: ('x102', 'x6', 'x106', 'x104'), score test: 5452.50501002004, score train: 9200.0\n",
            "Features: ('x102', 'x6', 'x101', 'x104'), score test: 5051.703406813626, score train: 9200.0\n",
            "Features: ('x102', 'x103', 'x106', 'x101'), score test: 5692.985971943887, score train: 9200.0\n",
            "Features: ('x102', 'x103', 'x106', 'x104'), score test: 5532.665330661322, score train: 9200.0\n",
            "Features: ('x102', 'x103', 'x101', 'x104'), score test: 5632.865731462925, score train: 9200.0\n",
            "Features: ('x102', 'x106', 'x101', 'x104'), score test: 5713.026052104207, score train: 9200.0\n",
            "Features: ('x6', 'x103', 'x106', 'x101'), score test: 5512.625250501002, score train: 9200.0\n",
            "Features: ('x6', 'x103', 'x106', 'x104'), score test: 5171.94388777555, score train: 9200.0\n",
            "Features: ('x6', 'x103', 'x101', 'x104'), score test: 5212.024048096192, score train: 9200.0\n",
            "Features: ('x6', 'x106', 'x101', 'x104'), score test: 5171.94388777555, score train: 9200.0\n",
            "Features: ('x103', 'x106', 'x101', 'x104'), score test: 5332.264529058116, score train: 9200.0\n",
            "Features: ('x105', 'x9', 'x102'), score test: 5051.302605210421, score train: 9400.0\n",
            "Features: ('x105', 'x9', 'x6'), score test: 5011.222444889779, score train: 9400.0\n",
            "Features: ('x105', 'x9', 'x103'), score test: 5452.104208416833, score train: 9400.0\n",
            "Features: ('x105', 'x9', 'x106'), score test: 5452.104208416833, score train: 9400.0\n",
            "Features: ('x105', 'x9', 'x101'), score test: 5271.743486973947, score train: 9400.0\n",
            "Features: ('x105', 'x9', 'x104'), score test: 5371.94388777555, score train: 9400.0\n",
            "Features: ('x105', 'x102', 'x6'), score test: 4890.981963927855, score train: 9400.0\n",
            "Features: ('x105', 'x102', 'x103'), score test: 5171.543086172344, score train: 9400.0\n",
            "Features: ('x105', 'x102', 'x106'), score test: 5031.2625250501, score train: 9400.0\n",
            "Features: ('x105', 'x102', 'x101'), score test: 5151.503006012023, score train: 9400.0\n",
            "Features: ('x105', 'x102', 'x104'), score test: 5171.543086172344, score train: 9400.0\n",
            "Features: ('x105', 'x6', 'x103'), score test: 5091.3827655310615, score train: 9400.0\n",
            "Features: ('x105', 'x6', 'x106'), score test: 5071.342685370741, score train: 9400.0\n",
            "Features: ('x105', 'x6', 'x101'), score test: 5191.583166332664, score train: 9400.0\n",
            "Features: ('x105', 'x6', 'x104'), score test: 4951.102204408817, score train: 9400.0\n",
            "Features: ('x105', 'x103', 'x106'), score test: 5271.743486973947, score train: 9400.0\n",
            "Features: ('x105', 'x103', 'x101'), score test: 5231.663326653306, score train: 9400.0\n",
            "Features: ('x105', 'x103', 'x104'), score test: 5311.823647294588, score train: 9400.0\n",
            "Features: ('x105', 'x106', 'x101'), score test: 5331.863727454909, score train: 9400.0\n",
            "Features: ('x105', 'x106', 'x104'), score test: 5291.783567134268, score train: 9400.0\n",
            "Features: ('x105', 'x101', 'x104'), score test: 5271.743486973947, score train: 9400.0\n",
            "Features: ('x9', 'x102', 'x6'), score test: 4931.062124248497, score train: 9400.0\n",
            "Features: ('x9', 'x102', 'x103'), score test: 5131.4629258517025, score train: 9400.0\n",
            "Features: ('x9', 'x102', 'x106'), score test: 5271.743486973947, score train: 9400.0\n",
            "Features: ('x9', 'x102', 'x101'), score test: 5131.4629258517025, score train: 9400.0\n",
            "Features: ('x9', 'x102', 'x104'), score test: 5331.863727454909, score train: 9400.0\n",
            "Features: ('x9', 'x6', 'x103'), score test: 5091.3827655310615, score train: 9400.0\n",
            "Features: ('x9', 'x6', 'x106'), score test: 4991.182364729459, score train: 9400.0\n",
            "Features: ('x9', 'x6', 'x101'), score test: 5091.3827655310615, score train: 9400.0\n",
            "Features: ('x9', 'x6', 'x104'), score test: 5031.2625250501, score train: 9400.0\n",
            "Features: ('x9', 'x103', 'x106'), score test: 5472.144288577154, score train: 9400.0\n",
            "Features: ('x9', 'x103', 'x101'), score test: 5271.743486973947, score train: 9400.0\n",
            "Features: ('x9', 'x103', 'x104'), score test: 5572.344689378757, score train: 9400.0\n",
            "Features: ('x9', 'x106', 'x101'), score test: 5051.302605210421, score train: 9400.0\n",
            "Features: ('x9', 'x106', 'x104'), score test: 5111.4228456913825, score train: 9400.0\n",
            "Features: ('x9', 'x101', 'x104'), score test: 5391.983967935871, score train: 9400.0\n",
            "Features: ('x102', 'x6', 'x103'), score test: 4971.142284569138, score train: 9400.0\n",
            "Features: ('x102', 'x6', 'x106'), score test: 5071.342685370741, score train: 9400.0\n",
            "Features: ('x102', 'x6', 'x101'), score test: 5211.623246492985, score train: 9400.0\n",
            "Features: ('x102', 'x6', 'x104'), score test: 5171.543086172344, score train: 9400.0\n",
            "Features: ('x102', 'x103', 'x106'), score test: 5291.783567134268, score train: 9400.0\n",
            "Features: ('x102', 'x103', 'x101'), score test: 5351.90380761523, score train: 9400.0\n",
            "Features: ('x102', 'x103', 'x104'), score test: 5492.184368737474, score train: 9400.0\n",
            "Features: ('x102', 'x106', 'x101'), score test: 5391.983967935871, score train: 9400.0\n",
            "Features: ('x102', 'x106', 'x104'), score test: 5672.54509018036, score train: 9400.0\n",
            "Features: ('x102', 'x101', 'x104'), score test: 5311.823647294588, score train: 9400.0\n",
            "Features: ('x6', 'x103', 'x106'), score test: 5211.623246492985, score train: 9400.0\n",
            "Features: ('x6', 'x103', 'x101'), score test: 5291.783567134268, score train: 9400.0\n",
            "Features: ('x6', 'x103', 'x104'), score test: 5351.90380761523, score train: 9400.0\n",
            "Features: ('x6', 'x106', 'x101'), score test: 5271.743486973947, score train: 9400.0\n",
            "Features: ('x6', 'x106', 'x104'), score test: 5031.2625250501, score train: 9400.0\n",
            "Features: ('x6', 'x101', 'x104'), score test: 5111.4228456913825, score train: 9400.0\n",
            "Features: ('x103', 'x106', 'x101'), score test: 5552.304609218436, score train: 9400.0\n",
            "Features: ('x103', 'x106', 'x104'), score test: 5331.863727454909, score train: 9400.0\n",
            "Features: ('x103', 'x101', 'x104'), score test: 5391.983967935871, score train: 9400.0\n",
            "Features: ('x106', 'x101', 'x104'), score test: 5311.823647294588, score train: 9400.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(scores).sort_values(by='score_test', ascending=False).head(10)['features'].to_list()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Awbwj9udrppl",
        "outputId": "6df0d9ad-76dd-4876-84a6-34f7a61a820f"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('x105', 'x102', 'x103', 'x106', 'x101'),\n",
              " ('x102', 'x103', 'x106', 'x101', 'x104'),\n",
              " ('x102', 'x106', 'x101', 'x104'),\n",
              " ('x102', 'x103', 'x106', 'x101'),\n",
              " ('x102', 'x106', 'x104'),\n",
              " ('x102', 'x103', 'x101', 'x104'),\n",
              " ('x105', 'x9', 'x103', 'x101', 'x104'),\n",
              " ('x102', 'x6', 'x103', 'x106', 'x104'),\n",
              " ('x105', 'x102', 'x103', 'x104'),\n",
              " ('x105', 'x9', 'x102', 'x103', 'x104')]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q4FgEfvRKrOz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}