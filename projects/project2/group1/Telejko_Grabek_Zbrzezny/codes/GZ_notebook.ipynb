{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from src.feature_selector import BaseFeatureSelector\n",
    "from src.settings import DATA_DIR\n",
    "from src.experiment_utils import perform_experiments, find_best_experiments\n",
    "from src.experiment import Experiment\n",
    "from src.custom_feature_selectors.boruta import Boruta\n",
    "from src.custom_feature_selectors.after_boruta_selectors import PermutationImportance, Impurity\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from skfeature.function.similarity_based.fisher_score import fisher_score\n",
    "from src.custom_feature_selectors.manual_feature_selector import ManualFeatureSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the text file into a dataframe\n",
    "X = pd.read_csv(os.path.join(DATA_DIR, 'x_train.txt'), sep=' ', header=None).to_numpy()\n",
    "y = pd.read_csv(os.path.join(DATA_DIR, 'y_train.txt'), header=None).to_numpy().T[0]\n",
    "X_test = pd.read_csv(os.path.join(DATA_DIR, 'x_test.txt'), sep=' ', header=None).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# from src.train import calculate_score\n",
    "# from boruta import BorutaPy\n",
    "\n",
    "\n",
    "# df = pd.DataFrame(X)\n",
    "# corr_matrix = df.corr().abs()\n",
    "# upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "# to_drop = [column for column in upper.columns if any(upper[column] > 0.9)]\n",
    "# df.drop(to_drop, axis=1, inplace=True)\n",
    "\n",
    "# X = df.to_numpy()\n",
    "\n",
    "# model = RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=5)\n",
    "\n",
    "# feat_selector = BorutaPy(\n",
    "#     verbose=2,\n",
    "#     estimator=model,\n",
    "#     n_estimators='auto',\n",
    "#     max_iter=10\n",
    "# )\n",
    "\n",
    "# feat_selector.fit(X_reduced, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_reduced = feat_selector.transform(X_reduced)\n",
    "# chosen_columns = []\n",
    "# for i in range(X.shape[1]):\n",
    "#     for j in range(X_reduced.shape[1]):\n",
    "#         if np.equal(X[:, i], X_reduced[:, j]).all():\n",
    "#             chosen_columns.append(i)\n",
    "#             break\n",
    "# chosen_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def discretize_dataset(X, bins=10):\n",
    "#     X_discr = np.copy(X)\n",
    "#     for i in range(X.shape[1]):\n",
    "#         X_discr[:, i] = pd.cut(X[:, i], bins=bins, labels=False)\n",
    "\n",
    "#     return X_discr\n",
    "\n",
    "# class ChiSquared(BaseFeatureSelector):\n",
    "\n",
    "#     def __init__(self, n_feats=3) -> None:\n",
    "#         super().__init__()\n",
    "#         self.n_feats = n_feats\n",
    "#         self.chi_chosen = None\n",
    "\n",
    "#     def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "#         X_cat = discretize_dataset(X)\n",
    "#         chi2_features = SelectKBest(chi2, k=3)\n",
    "#         X_best = chi2_features.fit_transform(X_cat, y)\n",
    "\n",
    "#         self.chi_chosen = []\n",
    "#         for i in range(X_cat.shape[1]):\n",
    "#             for j in range(X_best.shape[1]):\n",
    "#                 if np.equal(X_cat[:, i], X_best[:, j]).all():\n",
    "#                     self.chi_chosen.append(i)\n",
    "#                     break\n",
    "\n",
    "#     def get_support(self, indices: bool = True) -> np.ndarray:\n",
    "#         return self.chi_chosen\n",
    "    \n",
    "# class Fisher(BaseFeatureSelector):\n",
    "\n",
    "#     def __init__(self, n_feats=3) -> None:\n",
    "#         super().__init__()\n",
    "#         self.n_feats = n_feats\n",
    "#         self.fisher_chosen = None\n",
    "\n",
    "#     def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "#         self.fisher_results = np.argsort(-fisher_score(X, y))\n",
    "\n",
    "#     def get_support(self, indices: bool = True) -> np.ndarray:\n",
    "#         return self.fisher_results[:self.n_feats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_config = [\n",
    "    # Experiment(\n",
    "    #     classifier=RandomForestClassifier,\n",
    "    #     classifier_config={\n",
    "    #         \"max_depth\": 3\n",
    "    #     },\n",
    "    #     feature_selector=SelectFromModel,\n",
    "    #     feature_selector_config={\n",
    "    #         \"estimator\": LogisticRegression(max_iter=1000)\n",
    "    #     }\n",
    "    # ),\n",
    "    # Experiment(\n",
    "    #     classifier=GaussianNB,\n",
    "    #     # classifier_config={\n",
    "    #     #     \"learning_rate_init\": 0.01,\n",
    "    #     # },\n",
    "    #     feature_selector=ManualFeatureSelector,\n",
    "    #     feature_selector_config={\n",
    "    #         \"indices\": [100, 102, 105],\n",
    "    #     }\n",
    "    # ),\n",
    "    # Experiment(\n",
    "    #     classifier=GaussianNB,\n",
    "    #     # classifier_config={\n",
    "    #     #     \"max_depth\": 5,\n",
    "    #     #     \"n_estimators\": 100\n",
    "    #     # },\n",
    "    #     feature_selector=PermutationImportance,\n",
    "    #     feature_selector_config={\n",
    "    #         \"n_feats\": 3,\n",
    "    #     }\n",
    "    # ),\n",
    "    Experiment(\n",
    "        classifier=QDA,\n",
    "        classifier_config={\n",
    "            \"reg_param\": 0.5\n",
    "        },\n",
    "        feature_selector=Boruta,\n",
    "        feature_selector_config={\n",
    "            \"additional_feat_selector\": PermutationImportance(n_feats=3),\n",
    "            \"model_n_estimators\": 100,\n",
    "            \"model_max_depth\": 5,\n",
    "            \"boruta_n_estimators\": \"auto\",\n",
    "            \"boruta_max_iter\": 10,\n",
    "        }\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices for best none: \n",
    "# np.array(chosen_columns)[[6, 8, 9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, indices = perform_experiments(X, y, experiment_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment exp_qda_bor_pi_056032 in progress...\n",
      "Using 3 features, we properly classified 151/200 clients.\n",
      "Using 3 features, we properly classified 149/200 clients.\n",
      "Using 3 features, we properly classified 148/200 clients.\n",
      "Using 3 features, we properly classified 150/200 clients.\n",
      "Using 3 features, we properly classified 145/200 clients.\n"
     ]
    }
   ],
   "source": [
    "scores, indices = perform_experiments(X, y, experiment_config, split_indices=[np.array([102, 100, 104]),\n",
    "  np.array([102, 105, 100]),\n",
    "  np.array([100, 102, 105]),\n",
    "  np.array([102, 100, 105]),\n",
    "  np.array([100, 105, 102])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exp_gnb_bor_pi_8902e4': [array([102, 100, 104]),\n",
       "  array([102, 105, 100]),\n",
       "  array([100, 102, 105]),\n",
       "  array([102, 100, 105]),\n",
       "  array([100, 105, 102])]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_experiments = find_best_experiments()\n",
    "best_experiment = best_experiments[1]\n",
    "\n",
    "model = best_experiment.classifier(**best_experiment.classifier_config)\n",
    "feature_selector = best_experiment.feature_selector(\n",
    "    **best_experiment.feature_selector_config\n",
    ")\n",
    "\n",
    "# feature_selector.fit(X, y)\n",
    "# X_train_reduced = feature_selector.transform(X)\n",
    "# model.fit(X_train_reduced, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selector.fit(X_reduced, y)\n",
    "X_final = feature_selector.transform(X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.699\n",
      "Using 6 features, we properly classified 162/200 clients.\n",
      "6900.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reduced[:, 6:], y, test_size=0.2)\n",
    "# print(calculate_score(model, X_train, X_test, y_train, y_test))\n",
    "# model = xgb.XGBClassifier(tree_method=\"hist\")\n",
    "model.fit(X_train, y_train)\n",
    "preds_proba = model.predict_proba(X_test)\n",
    "preds = model.predict(X_test)\n",
    "print(accuracy_score(y_test, preds))\n",
    "print(calculate_score(model, X_train, X_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y_test[np.argsort(-preds_proba[:, 1])[:200]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_reduced = feature_selector.transform(X_test)\n",
    "proba_preds = model.predict_proba(X_test_reduced)\n",
    "np.argsort(proba_preds[:, 1])[-1000:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
